\documentclass[11pt,a4paper]{article}

\usepackage[utf8]{inputenc}
\usepackage[english]{babel}
\usepackage{csquotes}

\usepackage{amsmath}
\usepackage{mathtools}
\usepackage{nicefrac}
\usepackage{IEEEtrantools}

\usepackage{enumitem}

\usepackage[parfill]{parskip}

\usepackage[backend=biber,style=authoryear]{biblatex}
\addbibresource{langmodels.bib}

\usepackage{color}

% Should be included last, to redefine commands
\usepackage{hyperref}

% ------------------------------------------------------------------------------

\newcommand{\mytitle}{Thesis Proposal: \\
  Questioning Entropy to measure Language Models using efficient Argmax Queries}
  %Measuring the Quality of Language Models: Efficiently computing Argmax Queries}
\newcommand{\myname}{Lukas Schmelzeisen}
\newcommand{\mymail}{lukas@uni-koblenz.de}
\newcommand{\myaddress}{University of Koblenz-Landau, Germany}

% -- Math ----------------------------------------------------------------------

% given
% see http://tex.stackexchange.com/questions/141570/sizing-for-given-that-symbol-vertical-bar
\newcommand\givenbase[1][]{\,#1\lvert\,}
\let\given\givenbase
\newcommand\sgiven{\givenbase[\delimsize]}
\DeclarePairedDelimiterX\Basics[1](){\let\given\sgiven #1}

% argmax
\DeclareMathOperator*{\argmax}{arg\,max}

% cardinality
\newcommand{\cardinality}[1]{|#1|}

% probability
\newcommand{\probSymbol}[1][]{P_{#1}}
\newcommand{\prob}[2][]{\probSymbol[#1](#2)}
\newcommand{\probCond}[3][]{\prob[#1]{#2 \given #3}}

\newcommand{\probSymbolLower}[1][]{\hat{P}_{#1}}
\newcommand{\probLower}[2][]{\probSymbolLower[#1](#2)}
\newcommand{\probCondLower}[3][]{\probLower[#1]{#2 \given #3}}

% next word prediction
\newcommand{\nwp}[1]{\operatorname{NWP}(#1)}

% -- Editorial commands --------------------------------------------------------

\newcommand{\todo}[1]{\textcolor{red}{\textbf{TODO}: #1}}
\newcommand{\lukas}[1]{\textcolor{cyan}{Lukas: #1}}
\newcommand{\rene}[1]{\textcolor{blue}{Rene: #1}}

\newcommand{\noref}{\textcolor{red}{(NoRef)}}
\newcommand{\mbref}[1]{\textcolor{magenta}{(#1)}}

% ------------------------------------------------------------------------------

\title{\mytitle}
\author{
  \myname \\
  \small{\myaddress} \\
  \small{\texttt{\href{mailto:\mymail}{\nolinkurl{\mymail}}}}
}
\date{\today}

\hypersetup{
  unicode = true,
  pdfauthor = \myname,
  pdftitle = \mytitle,
}

\begin{document}
\maketitle

% ==============================================================================
\begin{abstract}
  The quality of a language model is commonly measured with the entropy of its
  probability distribution.
  However it is not clear whether language models that score better entropy
  actually perform better in a given application.
  Applications' usage of language models can usually be represented as an
  argmax query.
  However argmax computation is a very time intensive operation, and therefore
  no practicable measure of language model quality.
  This thesis will research how to efficiently compute argmax queries and
  thus be able to present a measure of quality that is closer to actual
  applications.
  It will then explore how this measure compares to entropy.
\end{abstract}

% ==============================================================================
\section{Introduction}

\todo{Check distance, metric and measure.}

Statistical language modeling is the task of assigning probabilities $\prob{s}$
to sequences of tokens ${s = w_1 \ldots w_n}$ by means of a
probability distribution.
%This can be thought of as predicting the occurrence of words.
Many natural language processing applications use language models, like
part-of-speech tagging or machine translation \parencite{Church1988,Brown1990}.
\todo{More examples, maybe from: Hull, 1992; Kernighan et al., 1990;
Srihari and Baltus, 1992.}

The quality of these applications usually depends on the quality of the used
language model.
Therefore comparison between the qualities of different language models is
necessary.
The quality of a language model is defined as its closeness to the ``true''
model of a language \noref.
However, since true language models are not available, other measures have to be
used.
%Applications typically have domain-based metrics available to measure their
%performance, and thus the quality of the employed language model.
%But in order to calculate these metrics, domain-based knowledge has to be
%applied to the language model.
%So these metrics can only be used to measure quality of a language model in the
%context of the application.
%\todo{More Examples and sources.}

% ------------------------------------------------------------------------------
\subsection{Entropy as a Measure of Quality for \mbox{Language Models}}

Most commonly \emph{entropy} is used to measure the quality of a language
model\footnote{Another measure often encountered is \emph{perplexity}, which is
just two to the power of entropy $2^H$.} \parencite{Goodman2001}.
Entropy is the average amount of uncertainty stored in a probability
distribution and defined as \parencite{Shannon1948}:
\begin{equation}
  H \coloneqq - \sum_{s} \prob{s} \log \prob{s}
\end{equation}
%Entropy has the nice property that it is the average number of bits per token
%that would be necessary to encode the test data using an optimal encoding.

But in order to calculate entropy one would have to sum over all sequences
$s$, the very large set of all sequences $\probSymbol$ assigns a probability to.
Thus in practice we usually instead measure cross entropy over test data using
the language model learned on training data \parencite{Goodman2001}.
Lower entropy is interpreted as representing a better language model.

\todo{Why is entropy actually used? Because it has a background in information
theory? Because it is fast and easy to calculate?}

However, to the knowledge of the author, it has not been researched whether
lower entropy actually results in better application quality.
Clearly it would be nice to have a measure of quality that is more directly
related to applications.
\todo{Is this a good claim to make?}
\todo{Because smoothing has to increase entropy (has shown in Shannon 1948)?}
\todo{What are Maximum-Entropy-Language-Models? Would they support my claim?}

% ------------------------------------------------------------------------------
\subsection{Application-oriented Measures of Quality}

Applications typically compute a term of the form:
\begin{equation}\label{eq:argmaxprod}
  \argmax_{s \in W^*} \, \probCond[domain]{k}{s} \cdot \prob[LM]{s}
\end{equation}
With $W$ being the set of all tokens, and $W^*$ being the set of all possible
sequences\footnote{$W^*$ is the very large set of all sequences $\probSymbol$
assigns a probability to. $\cardinality{W^*}$ is at least the
number of tokens to the power of the longest possible sequence.};
$\probCond[domain]{k}{s}$ being a domain-specific probability distribution
taking domain knowledge $k$ as input, and $\prob[LM]{s}$ being the
probability distribution of a language model \parencite{Church1988,Brown1990}.
\todo{More sources.} \todo{Noisy Channel?}

For example, to translate French into English, \cite{Brown1990} devised a
probability distribution $\probCond[translate]{e}{f}$, that would give the
probability for each English sentence $e$ to be the result of translating
the French sentence $f$ into English.
To obtain the most likely translation, the $e$ that maximizes that probability
has to be found.
Using Bayes' Theorem one can write
$\probCond{e}{f} = {\probCond{f}{e}\cdot\prob{e} \mathbin{/} \prob{f}}$.
Since the denominator does not depend on $e$ it suffice to find the $e$ that
optimizes ${\probCond{f}{e} \cdot \prob{e}}$ which is exactly equation
\eqref{eq:argmaxprod}.

Equation \eqref{eq:argmaxprod} yields better results the higher the quality of our
used language model $\probSymbol[LM]$.
Thus a good metric of quality, that is not coupled to any application domain
knowledge, should be one that only depends on:
\begin{equation}\label{eq:argmaxlm}
  \argmax_{s \in W^*} \prob[LM]{s}
\end{equation}

% ------------------------------------------------------------------------------
\subsection{Next Word Prediction}

One such metric is \emph{Next Word Prediction} \parencite{Shannon1951}.
Using the chain rule of conditional probabilities we can write:
\begin{equation}\label{eq:chainrule}
  \prob{w_1 \ldots w_n} = \probCond{w_n}{w_{n-1} \ldots w_1} \cdot \ldots
                          \cdot \probCond{w_2}{w_1} \cdot \prob{w_1}
\end{equation}
And interpret $\probCond{w_n}{w_{n-1} \dots w_1}$ as the probability of $w_n$
following the sequence $w_{n-1} \ldots w_1$. Next Word Prediction is defined as
the task of finding the most likely token following a sequence of tokens:
\begin{equation}\label{eq:nwp}
  \nwp{w_1 \dots w_n} \coloneqq \argmax_{w \in W} \probCond{w}{w_1 \ldots w_n}
\end{equation}
Using the chain rule \eqref{eq:chainrule} one can see that next word
prediction \eqref{eq:nwp} is similar in form to equation \eqref{eq:argmaxlm}.
\todo{Is this correct?}

Albeit being a good metric \noref, next word prediction is not used in practice
because one has to calculate the probability for all $w \in W$, the set of all
tokens.
This is a very time intensive operation, and as such faster and easier metrics,
like entropy, are used instead.
\todo{Add time to calculate argmax with tirvial solution.}
\todo{Is this the actually reason why entropy is used and NWP is not?}

% ==============================================================================
\section{Research Goals}

This thesis' research goals are to:
\begin{enumerate}[label=(\alph*)]
  \item
    Devise an algorithm, that makes it possible to efficiently compute
    next word prediction \eqref{eq:nwp} and ideally equation
    \eqref{eq:argmaxlm}, as opposed to the trivial solution of enumerating
    all possible arguments.

    This algorithm should be applicable to the state-of-the-art language model
    \emph{Modified Kneser-Ney Smoothing} \parencite{ChenGoodman1999} and its
    generalization, the \emph{Generalized Language Model}
    \parencite{Pickhardt2014}.

  \item
    Research if and how entropy correlates with next word prediction, as a case
    of application quality, and whether entropy actually is a good measure of
    quality of language models.
\end{enumerate}

% ==============================================================================
\section{Related work}

There is related work for word prediction, but it is all application related
(using POS, etc. ...).

I haven't found anything that questions entropy.

% ==============================================================================
\section{Preliminary work}

\todo{Include this section?}

As part of his work as research assistant at University of Koblenz-Landau,
the author developed the \emph{Generalized Language Modeling Toolkit}
(GLMTK)\footnote{\url{https://github.com/renepickhardt/generalized-language-modeling-toolkit/}}.
A program to learn and evaluate language models, among other things capable of
calculating \emph{Modified Kneser Ney Smoothing} \parencite{ChenGoodman1999} and
the \emph{Generalized Language Model} \parencite{Pickhardt2014}.

Research findings of this thesis will be implemented in it, and it will be used
during evaluation or just lower $n$-grams.

% ==============================================================================
\section{Description of Work and Expected Outcomes}

The aim of this thesis is to create a dynamic programming algorithm to calculate
next word prediction.

\todo{Stuff I still have to look at:\\
\\
-- Beam search \\
-- Stack search \\
-- Vertibi algorithm \\
-- Top $k$-Join \\
-- Zipf Distribution and Zipf's Law
}

% ==============================================================================
\section{Methodology and Evaluation}

Experiments on Brown corpus or NAB (North American Business news).
\todo{Why those Corpora? Goodman, 2001 uses them.}

Most $k$-likely following instead of just NWP to have more continuous metric.

Evaluation will compare next word prediction with entropy, cross-entropy,
conditional entropy and perplexity.

Will plot NWP-Entropy diagram. Since we currently can only measure two language
models (MKN and GLM) we might have to also use MLE or use some variations like
abs-MKN or skipdel-GLM.

% ==============================================================================
\section{Conclusion}

% ==============================================================================
\section{Timeline}

\todo{Is this section necessary?}

\begin{enumerate}
  \item Implementing trivial solution for NWP.
\end{enumerate}

\todo{Is any important part of a proposal missing from this document. Another
structure for proposal I found online was this :\\
\\
-- Introduction \\
-- Problem Statement \\
-- Background \\
-- Purpose \\
-- Significance \\
-- Methodology \\
-- Literature Review \\
-- Hypotheses \\
-- Definition of Terms \\
-- Assumptions \\
-- Scope and Limitations \\
-- Long-Range Consequence
}

% ==============================================================================
\printbibliography

% ==============================================================================
\clearpage
\section{Notes}

\subsection{Modified Kneser-Ney}

MKN is:
\begin{align}
  \begin{split}
    \probCond[mkn]{w_n}{w_1^{n-1}} =        &\frac{\max{\{c(w_1^n) - D(c(w_1^n)), 0\}}}
                                                  {c(w_1^{n-1} \bullet)} \\
                                          + &\frac{  D_1    N_1   (w_1^{n-1} \bullet)
                                                   + D_2    N_2   (w_1^{n-1} \bullet)
                                                   + D_{3+} N_{3+}(w_1^{n-1} \bullet)}
                                             {c(w_1^{n-1} \bullet)}
                                          \cdot \probCondLower[mkn]{w_n}{w_1^{n-2}}
  \end{split} \\
  \begin{split}
    \probCondLower[mkn]{w_n}{w_1^{n-1}} =   &\frac{\max{\{N_{1+}(w_1^n) - D(N_{1+}(w_1^n)), 0\}}}
                                                  {N_{1+}(w_1^{n-1} \bullet)} \\
                                          + &\frac{  D_1    N_1   (w_1^{n-1} \bullet)
                                                   + D_2    N_2   (w_1^{n-1} \bullet)
                                                   + D_{3+} N_{3+}(w_1^{n-1} \bullet)}
                                             {N_{1+}(w_1^{n-1} \bullet)}
                                          \cdot \probCondLower[mkn]{w_n}{w_1^{n-2}}
  \end{split}
\end{align}

We define:
\begin{align}
  \alpha(w_1^n)     =& \frac{\max{\{c(w_1^n) - D(c(w_1^n)), 0\}}}{c(w_1^{n-1} \bullet)} \\
  \gamma(w_1^{n-1}) =& \frac{  D_1    N_1   (w_1^{n-1} \bullet)
                           + D_2    N_2   (w_1^{n-1} \bullet)
                           + D_{3+} N_{3+}(w_1^{n-1} \bullet)}
                          {c(w_1^{n-1} \bullet)} \\
  \hat\alpha(w_1^n)     =& \frac{\max{\{N_{1+}(w_1^n) - D(N_{1+}(w_1^n)), 0\}}}{N_{1+}(w_1^{n-1} \bullet)} \\
  \hat\gamma(w_1^{n-1}) =& \frac{  D_1    N_1   (w_1^{n-1} \bullet)
                           + D_2    N_2   (w_1^{n-1} \bullet)
                           + D_{3+} N_{3+}(w_1^{n-1} \bullet)}
                          {N_{1+}(w_1^{n-1} \bullet)}
\end{align}
We should be able to calculate these for all $n$-grams beforehand and store
them permanently.

Using that we can rewrite MKN:
\begin{align}
  \probCond[mkn]{w_n}{w_1^{n-1}}      =& \alpha(w_1^n) + \gamma(w_1^{n-1}) \cdot \probCondLower[mkn]{w_n}{w_1^{n-2}} \\
  \probCondLower[mkn]{w_n}{w_1^{n-1}} =& \hat\alpha(w_1^n) + \hat\gamma(w_1^{n-1}) \cdot \probCondLower[mkn]{w_n}{w_1^{n-2}}
\end{align}

We define:
\begin{align}
  \alpha_i(w_1^n) =&
    \begin{cases}
      \alpha(w_1^n)     & \text{if } i = 0 \\
      \hat\alpha(w_1^n) & \text{otherwise}
    \end{cases} \\
  \gamma_i(w_1^n) =&
    \begin{cases}
      1                                                                & \text{if } i = 0 \\
      \gamma(w_1^n)                                                    & \text{if } i = 1 \\
      \gamma_{i-1}(w_1^n) \cdot \hat\gamma(\partial^b_{i-1} w_1^n) & \text{otherwise}
    \end{cases} \\
  \partial^b_i(w_1^n) =& \; w_i \ldots w_n
\end{align}
Because the argument of $\gamma_i$ is usually given by context, it is omitted
most of the time.

MKN can no be rewritten to avoid recursion:
\begin{equation}
  \probCond[mkn]{w_n}{w_1^{n-1}} = \sum_{i=0}^n \gamma_i \cdot \alpha_i(\partial^b_i w_1^n)
\end{equation}

Inequalities of $\gamma_i$:
\begin{align}
  \gamma_{i>0} <& \, 1 \\
  \gamma_i     >& \, \gamma_{i+1}
\end{align}

\subsection{Generalized Language Model}

We define:
\begin{align}
  \partial^s_i(w_1^n) =& \; w_1 \ldots w_{i-1} \bullet w_{i+1} \ldots w_n \\
  \delta(w_1^n)             =& \bigcup_{i = 1}^n \,\{ \, \partial^s_i(w_1^n) \given w_i \neq \bullet \, \}
\end{align}

GLM is:
\begin{align}
  \probCond[glm]{w_n}{w_1^{n-1}}      =& \alpha(w_1^n) + \gamma(w_1^{n-1}) \cdot \frac{\sum_{\hat h \in \delta(w_1^{n-1})} \probCondLower[glm]{w_n}{\hat h}}{\cardinality{\delta(w_1^{n-1})}} \\
  \probCondLower[glm]{w_n}{w_1^{n-1}} =& \hat\alpha(w_1^n) + \hat\gamma(w_1^{n-1}) \cdot \frac{\sum_{\hat h \in \delta(w_1^{n-1})} \probCondLower[glm]{w_n}{\hat h}}{\cardinality{\delta(w_1^{n-1})}}
\end{align}

\subsection{Expansion}

% Yeah fuck me for thinking this might be a good Idea...

\begin{equation}
  \begin{split}
    &\hspace{-2em}\probCond[mkn]{\text{Br\"ucke}}{\text{ich gehe \"uber die}} = \\
      &\alpha(\text{ich gehe \"uber die Br\"ucke}) + \gamma(\text{ich gehe \"uber die}) \cdot (\\
        &\quad \alpha(\text{gehe \"uber die Br\"ucke}) + \gamma(\text{gehe \"uber die}) \cdot (\\
          &\quad\quad \alpha(\text{\"uber die Br\"ucke}) + \gamma(\text{\"uber die}) \cdot (\\
            &\quad\quad\quad \alpha(\text{die Br\"ucke}) + \gamma(\text{die}) \cdot (\\
              &\quad\quad\quad\quad \alpha(\text{Br\"ucke})))))
  \end{split}
\end{equation}

\begin{equation}
  \begin{split}
    &\hspace{-2em}\probCond[glm]{\text{Br\"ucke}}{\text{gehe \"uber die}} = \\
      &\alpha(\text{gehe \"uber die Br\"ucke}) + \gamma(\text{gehe \"uber die}) \cdot \nicefrac{1}{3} \cdot (\\
        &\quad\alpha(\bullet\text{\"uber die Br\"ucke}) + \gamma(\bullet\text{\"uber die}) \cdot \nicefrac{1}{2} \cdot (\\
          &\quad\quad\alpha(\bullet\bullet\text{die Br\"ucke}) + \gamma(\bullet\bullet\text{die}) \cdot \nicefrac{1}{1} \cdot (\\
            &\quad\quad\quad\alpha(\bullet\bullet\bullet\text{Br\"ucke})) \, + \\
          &\quad\quad\alpha(\bullet\text{\"uber}\bullet\text{Br\"ucke}) + \gamma(\bullet\text{\"uber}\bullet) \cdot \nicefrac{1}{1} \cdot (\\
            &\quad\quad\quad\alpha(\bullet\bullet\bullet\text{Br\"ucke}))) \, + \\
        &\quad\alpha(\text{gehe}\bullet\text{die Br\"ucke}) + \gamma(\text{gehe}\bullet\text{die}) \cdot \nicefrac{1}{2} \cdot ( \\
          &\quad\quad\alpha(\bullet\bullet\text{die Br\"ucke}) + \gamma(\bullet\bullet\text{die}) \cdot \nicefrac{1}{1} \cdot (\\
            &\quad\quad\quad\alpha(\bullet\bullet\bullet\text{Br\"ucke})) \, + \\
          &\quad\quad\alpha(\text{gehe}\bullet\bullet\text{Br\"ucke}) + \gamma(\text{gehe}\bullet\bullet) \cdot \nicefrac{1}{1} \cdot (\\
            &\quad\quad\quad\alpha(\bullet\bullet\bullet\text{Br\"ucke}))) \, + \\
        &\quad\alpha(\text{gehe \"uber}\bullet\text{Br\"ucke}) + \gamma(\text{gehe \"uber}\bullet) \cdot \nicefrac{1}{2} \cdot (\\
          &\quad\quad\alpha(\bullet\text{\"uber}\bullet\text{Br\"ucke}) + \gamma(\bullet\text{\"uber}\bullet) \cdot \nicefrac{1}{1} \cdot (\\
            &\quad\quad\quad\alpha(\bullet\bullet\bullet\text{Br\"ucke})) \, + \\
          &\quad\quad\alpha(\text{gehe}\bullet\bullet\text{Br\"ucke}) + \gamma(\text{gehe}\bullet\bullet) \cdot \nicefrac{1}{1} \cdot (\\
            &\quad\quad\quad\alpha(\bullet\bullet\bullet\text{Br\"ucke}))))
  \end{split}
\end{equation}

\begin{IEEEeqnarray*}{llll}
  \probCond[glm]{\text{Br\"ucke}}{\text{gehe \"uber die}} = \hspace{-10em}& \hspace{-10em}& \hspace{-10em}& \\
    \qquad\alpha(\text{gehe \"uber die Br\"ucke}) \, + \hspace{-10em}&                                                                     &                                                                       & \\
    \qquad\nicefrac{1}{3}\cdot\gamma(\text{gehe \"uber die})\cdot(   & \alpha(\bullet\text{\"uber die Br\"ucke}) \, + \hspace{-10em}       &                                                                       & \\
                                                                     & \nicefrac{1}{2}\cdot\gamma(\bullet\text{\"uber die})\cdot(          & \alpha(\bullet\bullet\text{die Br\"ucke}) \, + \hspace{-10em}         & \\
                                                                     &                                                                     & \nicefrac{1}{1}\cdot\gamma(\bullet\bullet\text{die})\cdot(            & \alpha(\bullet\bullet\bullet\text{Br\"ucke})) \, + \\
                                                                     &                                                                     & \alpha(\bullet\text{\"uber}\bullet\text{Br\"ucke}) \, + \hspace{-10em}& \\
                                                                     &                                                                     & \nicefrac{1}{1}\cdot\gamma(\bullet\text{\"uber}\bullet)\cdot(         & \alpha(\bullet\bullet\bullet\text{Br\"ucke}))) \, + \\
                                                                     & \alpha(\text{gehe}\bullet\text{die Br\"ucke}) \, + \hspace{-10em}   &                                                                       & \\
                                                                     & \nicefrac{1}{2}\cdot\gamma(\text{gehe}\bullet\text{die})\cdot(      & \alpha(\bullet\bullet\text{die Br\"ucke}) \, + \hspace{-10em}         & \\
                                                                     &                                                                     & \nicefrac{1}{1}\cdot\gamma(\bullet\bullet\text{die})\cdot(            & \alpha(\bullet\bullet\bullet\text{Br\"ucke})) \, + \\
                                                                     &                                                                     & \alpha(\text{gehe}\bullet\bullet\text{Br\"ucke}) \, + \hspace{-10em}  & \\
                                                                     &                                                                     & \nicefrac{1}{1}\cdot\gamma(\text{gehe}\bullet\bullet)\cdot(           & \alpha(\bullet\bullet\bullet\text{Br\"ucke}))) \, + \\
                                                                     & \alpha(\text{gehe \"uber}\bullet\text{Br\"ucke}) \, + \hspace{-10em}&                                                                       & \\
                                                                     & \nicefrac{1}{2}\cdot\gamma(\text{gehe \"uber}\bullet)\cdot(         & \alpha(\bullet\text{\"uber}\bullet\text{Br\"ucke}) \, + \hspace{-10em}& \\
                                                                     &                                                                     & \nicefrac{1}{1}\cdot\gamma(\bullet\text{\"uber}\bullet)\cdot(         & \alpha(\bullet\bullet\bullet\text{Br\"ucke})) \, + \\
                                                                     &                                                                     & \alpha(\text{gehe}\bullet\bullet\text{Br\"ucke}) \, + \hspace{-10em}  & \\
                                                                     &                                                                     & \nicefrac{1}{1}\cdot\gamma(\text{gehe}\bullet\bullet)\cdot(           & \alpha(\bullet\bullet\bullet\text{Br\"ucke}))))
\end{IEEEeqnarray*}

\begin{equation}
  \begin{split}
    &\hspace{-2em}\probCond[glm]{\text{Br\"ucke}}{\text{gehe \"uber die}} = \\
    &\nicefrac{1}{1} \cdot \alpha(\text{gehe \"uber die Br\"ucke}) \, + \\
    &\nicefrac{1}{3} \cdot \gamma(\text{gehe \"uber die}) \cdot \alpha(\bullet\text{\"uber die Br\"ucke}) \, + \\
    &\nicefrac{1}{6} \cdot \gamma(\text{gehe \"uber die}) \cdot \gamma(\bullet\text{\"uber die}) \cdot \alpha(\bullet\bullet\text{die Br\"ucke}) \, + \\
    &\nicefrac{1}{6} \cdot \gamma(\text{gehe \"uber die}) \cdot \gamma(\bullet\text{\"uber die}) \cdot \gamma(\bullet\bullet\text{die}) \cdot \alpha(\bullet\bullet\bullet\text{Br\"ucke}) \, + \\
    &\nicefrac{1}{6} \cdot \gamma(\text{gehe \"uber die}) \cdot \gamma(\bullet\text{\"uber die}) \cdot \alpha(\bullet\text{\"uber}\bullet\text{Br\"ucke}) \, + \\
    &\nicefrac{1}{6} \cdot \gamma(\text{gehe \"uber die}) \cdot \gamma(\bullet\text{\"uber die}) \cdot \gamma(\bullet\text{\"uber}\bullet) \cdot \alpha(\bullet\bullet\bullet\text{Br\"ucke}) \, + \\
    &\nicefrac{1}{3} \cdot \gamma(\text{gehe \"uber die}) \cdot \alpha(\text{gehe}\bullet\text{die Br\"ucke}) \, + \\
    &\nicefrac{1}{6} \cdot \gamma(\text{gehe \"uber die}) \cdot \gamma(\text{gehe}\bullet\text{die}) \cdot \alpha(\bullet\bullet\text{die Br\"ucke}) \, + \\
    &\nicefrac{1}{6} \cdot \gamma(\text{gehe \"uber die}) \cdot \gamma(\text{gehe}\bullet\text{die}) \cdot \gamma(\bullet\bullet\text{die}) \cdot \alpha(\bullet\bullet\bullet\text{Br\"ucke}) \, + \\
    &\nicefrac{1}{6} \cdot \gamma(\text{gehe \"uber die}) \cdot \gamma(\text{gehe}\bullet\text{die}) \cdot \alpha(\text{gehe}\bullet\bullet\text{Br\"ucke}) \, + \\
    &\nicefrac{1}{6} \cdot \gamma(\text{gehe \"uber die}) \cdot \gamma(\text{gehe}\bullet\text{die}) \cdot \gamma(\text{gehe}\bullet\bullet) \cdot \alpha(\bullet\bullet\bullet\text{Br\"ucke}) \, + \\
    &\nicefrac{1}{3} \cdot \gamma(\text{gehe \"uber die}) \cdot \alpha(\text{gehe \"uber}\bullet\text{Br\"ucke}) \, + \\
    &\nicefrac{1}{6} \cdot \gamma(\text{gehe \"uber die}) \cdot \gamma(\text{gehe \"uber}\bullet) \cdot \alpha(\bullet\text{\"uber}\bullet\text{Br\"ucke}) \, + \\
    &\nicefrac{1}{6} \cdot \gamma(\text{gehe \"uber die}) \cdot \gamma(\text{gehe \"uber}\bullet) \cdot \gamma(\bullet\text{\"uber}\bullet) \cdot \alpha(\bullet\bullet\bullet\text{Br\"ucke}) \, + \\
    &\nicefrac{1}{6} \cdot \gamma(\text{gehe \"uber die}) \cdot \gamma(\text{gehe \"uber}\bullet) \cdot \alpha(\text{gehe}\bullet\bullet\text{Br\"ucke}) \, + \\
    &\nicefrac{1}{6} \cdot \gamma(\text{gehe \"uber die}) \cdot \gamma(\text{gehe \"uber}\bullet) \cdot \gamma(\text{gehe}\bullet\bullet) \cdot \alpha(\bullet\bullet\bullet\text{Br\"ucke})
  \end{split}
\end{equation}

\begin{equation}
  \begin{split}
    &\hspace{-2em}\probCond[glm]{\text{Br\"ucke}}{\text{gehe \"uber die}} = \\
    &\nicefrac{1}{1} \cdot \alpha(\text{gehe \"uber die Bru\"cke}) \, + \\
    &\nicefrac{1}{3} \cdot \gamma(\text{gehe \"uber die}) \cdot \alpha(\bullet\text{\"uber die Br\"ucke}) \, + \\
    &\nicefrac{1}{3} \cdot \gamma(\text{gehe \"uber die}) \cdot \alpha(\text{gehe}\bullet\text{die Br\"ucke}) \, + \\
    &\nicefrac{1}{3} \cdot \gamma(\text{gehe \"uber die}) \cdot \alpha(\text{gehe \"uber}\bullet\text{Br\"ucke}) \, + \\
    &\nicefrac{1}{6} \cdot \gamma(\text{gehe \"uber die}) \cdot \gamma(\bullet\text{\"uber die}) \cdot \alpha(\bullet\bullet\text{die Br\"ucke}) \, + \\
    &\nicefrac{1}{6} \cdot \gamma(\text{gehe \"uber die}) \cdot \gamma(\bullet\text{\"uber die}) \cdot \alpha(\bullet\text{\"uber}\bullet\text{Br\"ucke}) \, + \\
    &\nicefrac{1}{6} \cdot \gamma(\text{gehe \"uber die}) \cdot \gamma(\text{gehe}\bullet\text{die}) \cdot \alpha(\bullet\bullet\text{die Br\"ucke}) \, + \\
    &\nicefrac{1}{6} \cdot \gamma(\text{gehe \"uber die}) \cdot \gamma(\text{gehe}\bullet\text{die}) \cdot \alpha(\text{gehe}\bullet\bullet\text{Br\"ucke}) \, + \\
    &\nicefrac{1}{6} \cdot \gamma(\text{gehe \"uber die}) \cdot \gamma(\text{gehe \"uber}\bullet) \cdot \alpha(\bullet\text{\"uber}\bullet\text{Br\"ucke}) \, + \\
    &\nicefrac{1}{6} \cdot \gamma(\text{gehe \"uber die}) \cdot \gamma(\text{gehe \"uber}\bullet) \cdot \alpha(\text{gehe}\bullet\bullet\text{Br\"ucke}) \, + \\
    &\nicefrac{1}{6} \cdot \gamma(\text{gehe \"uber die}) \cdot \gamma(\bullet\text{\"uber die}) \cdot \gamma(\bullet\bullet\text{die}) \cdot \alpha(\bullet\bullet\bullet\text{Br\"ucke}) \, + \\
    &\nicefrac{1}{6} \cdot \gamma(\text{gehe \"uber die}) \cdot \gamma(\bullet\text{\"uber die}) \cdot \gamma(\bullet\text{\"uber}\bullet) \cdot \alpha(\bullet\bullet\bullet\text{Br\"ucke}) \, + \\
    &\nicefrac{1}{6} \cdot \gamma(\text{gehe \"uber die}) \cdot \gamma(\text{gehe}\bullet\text{die}) \cdot \gamma(\bullet\bullet\text{die}) \cdot \alpha(\bullet\bullet\bullet\text{Br\"ucke}) \, + \\
    &\nicefrac{1}{6} \cdot \gamma(\text{gehe \"uber die}) \cdot \gamma(\text{gehe}\bullet\text{die}) \cdot \gamma(\text{gehe}\bullet\bullet) \cdot \alpha(\bullet\bullet\bullet\text{Br\"ucke}) \, + \\
    &\nicefrac{1}{6} \cdot \gamma(\text{gehe \"uber die}) \cdot \gamma(\text{gehe \"uber}\bullet) \cdot \gamma(\bullet\text{\"uber}\bullet) \cdot \alpha(\bullet\bullet\bullet\text{Br\"ucke}) \, + \\
    &\nicefrac{1}{6} \cdot \gamma(\text{gehe \"uber die}) \cdot \gamma(\text{gehe \"uber}\bullet) \cdot \gamma(\text{gehe}\bullet\bullet) \cdot \alpha(\bullet\bullet\bullet\text{Br\"ucke})
  \end{split}
\end{equation}

\begin{IEEEeqnarray*}{lll}
    &\hspace{-2em}\probCond[glm]{\text{Br\"ucke}}{\text{gehe \"uber die}} = &\\
    &\alpha(\text{gehe \"uber die Br\"ucke})                  &(\nicefrac{1}{1})\, + \\
    &\alpha(\bullet\text{\"uber die Br\"ucke}) \cdot          &(\nicefrac{1}{3} \cdot \gamma(\text{gehe \"uber die})) \, + \\
    &\alpha(\text{gehe}\bullet\text{die Br\"ucke}) \cdot      &(\nicefrac{1}{3} \cdot \gamma(\text{gehe \"uber die})) \, + \\
    &\alpha(\text{gehe \"uber}\bullet\text{Br\"ucke}) \cdot   &(\nicefrac{1}{3} \cdot \gamma(\text{gehe \"uber die})) \, + \\
    &\alpha(\bullet\bullet\text{die Br\"ucke}) \cdot          &(\nicefrac{1}{6} \cdot \gamma(\text{gehe \"uber die}) \cdot \gamma(\bullet\text{\"uber die}) \, +\\
    &                                                         &\phantom(\nicefrac{1}{6} \cdot \gamma(\text{gehe \"uber die}) \cdot \gamma(\text{gehe}\bullet\text{die})) \, + \\
    &\alpha(\bullet\text{\"uber}\bullet\text{Br\"ucke}) \cdot &(\nicefrac{1}{6} \cdot \gamma(\text{gehe \"uber die}) \cdot \gamma(\bullet\text{\"uber die}) \, + \\
    &                                                         &\phantom(\nicefrac{1}{6} \cdot \gamma(\text{gehe \"uber die}) \cdot \gamma(\text{gehe \"uber}\bullet)) \, + \\
    &\alpha(\text{gehe}\bullet\bullet\text{Br\"ucke}) \cdot   &(\nicefrac{1}{6} \cdot \gamma(\text{gehe \"uber die}) \cdot \gamma(\text{gehe}\bullet\text{die}) \, + \\
    &                                                         &\phantom(\nicefrac{1}{6} \cdot \gamma(\text{gehe \"uber die}) \cdot \gamma(\text{gehe \"uber}\bullet)) \, + \\
    &\alpha(\bullet\bullet\bullet\text{Br\"ucke}) \cdot       &(\nicefrac{1}{6} \cdot \gamma(\text{gehe \"uber die}) \cdot \gamma(\bullet\text{\"uber die}) \cdot \gamma(\bullet\text{\"uber}\bullet) \, + \\
    &                                                         &\phantom(\nicefrac{1}{6} \cdot \gamma(\text{gehe \"uber die}) \cdot \gamma(\bullet\text{\"uber die}) \cdot \gamma(\bullet\bullet\text{die}) \, + \\
    &                                                         &\phantom(\nicefrac{1}{6} \cdot \gamma(\text{gehe \"uber die}) \cdot \gamma(\text{gehe}\bullet\text{die}) \cdot \gamma(\text{gehe}\bullet\bullet) \, + \\
    &                                                         &\phantom(\nicefrac{1}{6} \cdot \gamma(\text{gehe \"uber die}) \cdot \gamma(\text{gehe}\bullet\text{die}) \cdot \gamma(\bullet\bullet\text{die}) \, + \\
    &                                                         &\phantom(\nicefrac{1}{6} \cdot \gamma(\text{gehe \"uber die}) \cdot \gamma(\text{gehe \"uber}\bullet) \cdot \gamma(\bullet\text{\"uber}\bullet) \, + \\
    &                                                         &\phantom(\nicefrac{1}{6} \cdot \gamma(\text{gehe \"uber die}) \cdot \gamma(\text{gehe \"uber}\bullet) \cdot \gamma(\text{gehe}\bullet\bullet))
\end{IEEEeqnarray*}

\end{document}
