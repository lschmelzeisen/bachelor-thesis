\documentclass[11pt,a4paper]{article}

\usepackage{amsmath}
\usepackage{mathtools}

\usepackage{color}

\usepackage{hyperref}

\usepackage[parfill]{parskip}

\newcommand{\todo}[1]{\textcolor{red}{\textbf{TODO}: #1}}
\newcommand{\lukas}[1]{\textcolor{cyan}{Lukas: #1}}
\newcommand{\rene}[1]{\textcolor{blue}{Rene: #1}}

\newcommand{\noref}{\textcolor{red}{\small \textsuperscript{[NoRef]}}}
\newcommand{\mbref}[1]{\textcolor{magenta}{\small \textsuperscript{[#1]}}}

% Math -------------------------------------------------------------------------

% given
% see http://tex.stackexchange.com/questions/141570/sizing-for-given-that-symbol-vertical-bar
\newcommand\givenbase[1][]{\:#1\lvert\:}
\let\given\givenbase
\newcommand\sgiven{\givenbase[\delimsize]}
\DeclarePairedDelimiterX\Basics[1](){\let\given\sgiven #1}

% argmax
\DeclareMathOperator*{\argmax}{arg\,max}

% cardinality
\newcommand{\cardinality}[1]{|#1|}

% probability
\newcommand{\prob}[1]{P(#1)}
\newcommand{\probCond}[2]{P(#1 \given #2)}

\title{\todo{Title}}
\author{Lukas Schmelzeisen \\ \texttt{lukas@uni-koblenz.de}}
\date{\today}

\hypersetup{
  pdfauthor = Lukas Schmelzeisen
  pdftitle = Title,
}

\begin{document}http://tex.stackexchange.com/questions/141570/sizing-for-given-that-symbol-vertical-bar
\maketitle

\section{Abstract}

The quality of a language model is traditionally measured by calculating
cross-entropy over testing data \noref.
However it is not clear if language models that score better entropy actually
perform better in a given application \noref.
Applications are usually interested in an argmax query \noref.
However argmax computation is a very time intensive operation.
This bachelor thesis will research how to efficiently compute argmax queries and
thus be able to truly measure the quality of a language model.
It will then research whether lower entropy actually correlates with better
application.

\section{Introduction}

Statistical Language Modeling has many applications.
\mbref{Church, 1988; Brown et al, 1990; Hull, 1992; Kernighan et al., 1990;
Srihari and Baltus, 1992}

The quality of many natural language processing applications depends on the
quality of language models.

Applications typically have their own metrics to measure quality, so we need a
method to measure quality of the underlying language models.
For example word-error-rate.\noref
\todo{More examples.}

The most commonly used method for measuring the quality of a language model is
perplexity.\mbref{Gooman, 2001}

Perplexity of a language model is equal to the geometric average of the inverse
probability of words measured on test data.
\lukas{Goodman, 2001: A language model that assigned equal probability to 100
words would have perplexity 100.}

\begin{equation}
  \sqrt[n]{\prod_{i=1}^n\frac{1}{\probCond{w_i}{w_1 ... w_{i-1}}}}
\end{equation}

Entropy is $\log_2$ of perplexity. \lukas{Goodman, 2001: What we actually
measure is the cross entropy of the test data given the model. Since this is by
far the most common type of entropy measured, we abuse the term by simply saying
entropy, when what we really mean is a particular cross entropy.}

To get actual entropy / perplexity values we would have to calculate over all
possible sequences $W^n$, maybe $W^*$.

However it is not clear how improved entropy / perplexity scores correlate with
better application measure, say word error rate, or if such a correlation even
exists.\noref

Experiments on Brown corpus or NAB (North American Business news)
\lukas{(like Goodman, 2001)}.

% ------------------------------------------------------------------------------
\vspace{1em}
\hrule

Language models assign a probability $P(w_1^n)$ to a sequence of $n$
words $w_1 \ldots w_n$\noref.
To compute these probabilities the chain rule of probability is applied\noref:
\begin{equation}
  P(w_1^n) = P(w_n | w_1^{n-1}) \cdot P(w_{n-1} | w_1^{n-2}) \cdots P(w_1)
\end{equation}
Language models are typically specified by giving a formula to calculate
$P(w_n | w_1^{n-1})$ to compute the conditional probability for a
\emph{sequence} $w_n$ given a \emph{history} $w_1^{n-1}$.

The quality of a language model is traditionally measured by calculating
cross-entropy over testing data.
\begin{equation}
  H(Q,P) = - \sum_{w_1^n} Q(w_1^n) \log P(w_1^n)
\end{equation}
Where $Q$ is the Maximum-Likelihood distribution over testing data and $P$ is
language model learned on training data\noref.
Lower entropy is interpreted as a better language model.

However it is not clear if a lower entropy actually correlates with better
application.

\todo{Why I'm not using existing applications.}

Applications are usually not interested in the probability of a sequence, but
rather in the most probable sequence $w'$ following a given history
$w_1^{n-1}$\noref.
\begin{equation}
  w' = \argmax_{w \in W} P(w | w_1^{n-1})
\end{equation}
\todo{I believe this is not fully correct. Typically the noisy channel model is
referenced here}.

The trivial solution, calculating the probability for all $w \in W$, is very
time expensive, and thus not practical.
\todo{Add time to calculate argmax with trivial solution here.}

This bachelor thesis research will focus on how to calculate the argmax query
on state-of-the-art language models \footnote{\emph{Modified Kneser-Ney}\noref
and the \emph{Generalized Language Model}\noref.} in a reasonable amount of
time.

\section{Research Goals}

\section{Related work}

\section{Preliminary work}

\section{Description of Work and Expected Outcomes}

\section{Methodology and Evaluation}

\section{Conclusion}

\section{References}

\section{Timeline?}

%\section{Introduction}
%\section{Problem statement}
%\section{Background}
%\section{Purpose}
%\section{Significance}
%\section{Methodology}
%\section{Literature Review}
%\section{Hypotheses}
%\section{Definition of Terms}
%\section{Assumptions}
%\section{Scope and Limitations}
%\section{Long-Range Consequences}
\end{document}
