\chapter{Review of Considered Language Models}
\label{ch:review-lm}

\todo[inline]{Try to find improvements to $\NumSkpOp$ notation, maybe by
establishing it as current order?}

This chapter will recall language modeling techniques considered in this work,
and introduce the used notation (\cref{sec:review-notation}).
Namely, \emph{Modified Kneser-Ney Smoothing} (\cref{sec:review-lm-mkn})
and the \emph{Generalized Language Model} (\cref{sec:review-lm-glm})
are described.

\todo[inline, caption = {Citations in Review}]{
  How do I declare citations for this section?
  \begin{itemize}
    \item Do I cite Rene or ChenGoodman?
    \item Do I cite once or on every definition?
    \item Some stuff is not published anywhere else yet. (For example new skip
      notation or new glm notation)
  \end{itemize}
}
\Rene[inline]{In a section like this you would have a sentence like: we follow
[...] and review basics. for unpublished stuff that is work in progress I think
marking it and having a statement in the foreword that states that some of the
stuff in here is new and not your IP seems reasonable.}

% ------------------------------------------------------------------------------
\section{Notation: Counts and Skips}
\label{sec:review-notation}

Let $\Count(w_1^n)$ denote the frequency count of a sequence's
$w_1^n = w_1 \ldots w_n$ occurence in training data.

Let $\Skp$ be a wildcard (called \emph{``skip''}), that can take the place
of any word at its location.
Thus, for example $\Count(\Skp w_1^n)$ is the frequency count of sequences
in training data that start with any word $x \in \Sigma$ and where the remaining
words are $w_1^n$:
\begin{equation}
  \Count(\Skp w_1^n) = \sum_{x \in \Sigma} \Count(x \: w_1^n)
\end{equation}
Where $\Sigma$ is the set of all words in a language
$\Language \subseteq \Sigma^{*}$.
It is easy to extend this definition to sequences with multiple skips:
\begin{equation}
  \Count(w_1 \Skp \Skp w_2) = \sum_{x, y \in \Sigma} \Count(w_1 \: x \: y \: w_2)
\end{equation}

\todo[inline]{Do we mention the difference of $\Count(\Skp w_1^n)$ to $\Count(w_1^n)$
in absence of Beginning of Sentence tags?}

Additionally we define \emph{continuation} counts $\ContCount_k(\DummyArg)$ as
the number of sequences from the training data that occur exactly $k$ times and
can be constructed by replacing $\WSkp$ (called \emph{``continuation skip''})
with any word $x \in \Sigma$.
For example $\ContCountII(\WSkp w_1^n)$ denotes the number of words that can
precede $w_1^n$ in training data, and occur exactly $2$ times:
\begin{equation}
  \ContCountII(\WSkp w_1^n) =
    \Cardinality{\left\{ x \in \Sigma \,\middle|\, \Count(x \: w_1^n) = 2 \right\}}
\end{equation}
Further let $\ContCount_{k+}(\DummyArg)$ count sequences that occur $k$ or more
times:
\begin{equation}
  \ContCountIp(\WSkp w_1^n) =
    \Cardinality{\left\{ x \in \Sigma \,\middle|\, \Count(x \: w_1^n) \geq 1 \right\}}
\end{equation}
\Rene{$k+$ and $1+$ intermixed.}

In continuation counts regular skips and continuation skips can be mixed, for
example:
\todo{Check if equation actually corresponds with implementation.}
\begin{equation}
  \ContCountIp(\WSkp w \Skp \WSkp) =
    \Cardinality{\left\{ x, y \in \Sigma \,\middle|\, c(x \: w \Skp y) \geq 1 \right\}}
\end{equation}

To make a clear distinction between ``normal'' counts $\Count(\DummyArg)$ and
continuation counts $\ContCount_{\DummyIndex}(\DummyArg)$, we
sometimes call the former \emph{absolute} counts.

\todo[inline]{Examples of counting with small text corpus.}

% ------------------------------------------------------------------------------
\section{Modified Kneser-Ney Smoothing}
\label{sec:review-lm-mkn}

\begin{draft}
Modified Kneser-Ney smoothed language models are computed by interpolating
between higher and lower order $n$-gram language models.
The highest order distribution is interpolated with lower order distirbutions
as follows:
\todo{(Copied from Rene)}
\end{draft}
\begin{equation}
  \label{eq:mkn-high}
  \ProbMKN{w_n}{w_1^{n-1}} =
    \frac{\DiscountedCount(w_1^n) + \gamma(w_1^{n-1}) \ProbMKN*{w_n}{w_2^{n-1}}}
         {\Count(w_1^{n-1} \Skp)} \\
\end{equation}
With the \emph{discounted} count
${\DiscountedCount(w_1^n) = \max\Set{\Count(w_1^n) - \Discount(\Count(w_1^n)), 0}}$.

The exact definitions of $D(\DummyArg)$ and $\gamma(w_1^{n-1})$ are not
important in our context, however for the sake of completeness they are given
in \cref{sec:discounts-interpolation-weights}.

Naturally \cref{eq:mkn-high} is only defined if the denominator
${\Count(w_1^{n-1} \Skp) > 0}$, that is if the history $w_1^{n-1} \Skp$ was seen
in training.
If $w_1^{n-1} \Skp$ was not seen, we perform a so called \emph{``back-off''}
and set:
\begin{align}
  \label{eq:mkn-backoff}
  \ProbMKN{w_n}{w_1^{n-1}} = \ProbMKN{w_n}{w_2^{n-1}}
      & & \Condition{\Count(w_1^{n-1} \Skp) = 0}
\end{align}
It is common to perform multiple back-offs until the history was actually seen.

\begin{draft}
Essentially, interpolation with a lower order model corresponds to leaving out
the first word in the considered sequence.
\todo{(Copied from Rene)}
\end{draft}
Lower order models are computed differently, and in turn interpolated with
further lower orders:
\begin{equation}
  \label{eq:mkn-low}
  \ProbMKN*{w_n}{w_1^{n-1}} =
    \frac{\DiscountedCount*(\WSkp w_1^n) + \gamma(w_1^{n-1}) \ProbMKN*{w_n}{w_2^{n-1}}}
         {\ContCountIp(\WSkp w_1^{n-1} \WSkp)} \\
\end{equation}
With the \emph{discounted} continuation count
${\DiscountedCount*(\WSkp w_1^n) = \max\Set{ \ContCountIp(\WSkp w_1^n) - \Discount(\Count(w_1^n)), 0 }}$.
Backing-off to compute lower orders is never necessary, because
if history $w_1^{n-1}$ was seen, $w_2^{n-1}$ was seen as well.
\todo{This should actually not be true, because we increase n-gram length for
first lower order. How do we handle this in current implementation?}

The lowest order (the probability of a single word) can not be further
interpolated and we use the relative continuation frequency of that word.
Similarly if we only want to compute the probability of a single word on the
highest order, we compute it as the relative frequency of that word:
\Rene[noline]{Let us discuss this.}
\begin{subequations}
  \label{eq:mkn-lowest}
  \begin{align}
    \label{eq:mkn-lowest-abs}
    \ProbMKN {w} &= \frac{\Count(w)}{\Count(\Skp)} \\
    \label{eq:mkn-lowest-cont}
    \ProbMKN*{w} &= \frac{\ContCountIp(\WSkp w)}{\ContCountIp(\WSkp \WSkp)}
  \end{align}
\end{subequations}

% ------------------------------------------------------------------------------
\section{Generalized Language Model}
\label{sec:review-lm-glm}

The Generalized Language Model is a natural extension to Modified Kneser-Ney
Smoothing.
It is again based on interpolation, though it differs in the way lower order
models are interpolated.
The highest order is computed as:
\Rene[noline]{I'm unsure whether I understand the semantics of $\NumSkpOp$
correctly.}
\begin{equation}
  \label{eq:glm-high}
  \ProbGLM{w_n}{w_1^{n-1}} =
    \frac{\DiscountedCount(w_1^n) + \frac{\gamma(w_1^{n-1})}{\NumSkpOp(w_1^{n-1})}
                                    \sum_{j=1}^{\NumSkpOp(w_1^{n-1})} \ProbGLM*{w_n}{\SkpOp_j w_1^{n-1}}}
         {\Count(w_1^{n-1} \Skp)}
\end{equation}
Auxiliary definitions $\DiscountedCount, \DiscountedCount*, \gamma$ are the same
as for Modified Kneser-Ney Smoothing and given in \cref{sec:review-lm-mkn}.

The skip operator $\SkpOp_j w_1^{n-1}$ can be any mapping that relates
$w_1^{n-1}$ to exactly $\NumSkpOp(w_1^{n-1})$ many derived sequences.
It is easy to see that for $\NumSkpOp(w_1^{n-1}) = 1$ and
$\SkpOp_1 w_1^{n-1} = w_2^{n-1}$ \cref{eq:glm-high} is an
instantiation of \cref{eq:mkn-high} (Modified Kneser-Ney Smoothing), and thus
a true generalization.

In practice only one definition for $\SkpOp$ is used though.
That is $\NumSkpOp(w_1^{n-1})$ is set to the number of non-skip words in
$w_1^{n-1}$ and $\SkpOp_j(w_1^{n-1})$ replaces the $j$th non-skip word with
a skip.
For instance: $\SkpOp_2(w_1^3) = w_1 \Skp w_3$.

Again as with Modified Kneser-Ney Smoothing, \cref{eq:glm-high} is not
defined if $w_1^{n-1}$ is unseen and thus our denominator would be zero.
The back-off step is then performed as:
\begin{equation}
  \label{eq:glm-backoff}
  \begin{split}
    \ProbGLM{w_n}{w_1^{n-1}} = \frac{1}{\NumSkpOp(w_1^{n-1})}
                               \sum_{j=1}^{\NumSkpOp(w_1^{n-1})} \ProbGLM{w_n}{\SkpOp_j w_1^{n-1}} \\
      \hfill \Condition{\Count(w_1^{n-1} \Skp) = 0}
  \end{split}
\end{equation}

Subsequently lower order models are computed as:
\begin{equation}
  \label{eq:glm-low}
  \ProbGLM*{w_n}{w_1^{n-1}} =
    \frac{\DiscountedCount*(\WSkp w_1^n) + \frac{\gamma(w_1^{n-1})}{\NumSkpOp(w_1^{n-1})}
                                     \sum_{j=1}^{\NumSkpOp(w_1^{n-1})} \ProbGLM*{w_n}{\SkpOp_j w_1^{n-1}}}
         {\ContCountIp(\WSkp w_1^{n-1} \WSkp)}
\end{equation}

\todo[inline, caption={Switch of \Skp and \WSkp in GLM $\ContCountIp$}]{
\begin{itemize}
  \item Replace \Skp with \WSkp in $\ContCountIp$ in GLM?
  \item \WSkp at end of $\ContCountIp$ correct?
\end{itemize}
}

The case of the lowest order occurs when $\NumSkpOp(w_1^{n-1}) = 0$ and is
handled similarly to Modified Kneser-Ney Smoothing.
In practice this is the case when $w_1^{n-1}$ only consists of skips.
\begin{subequations}
  \label{eq:glm-lowest}
  \begin{align}
    \ProbGLM {w_n}{w_1^{n-1}} &= \frac{\Count(w_1^n)}{\Count(w_1^{n-1} \Skp)}
      & \Condition{\NumSkpOp(w_1^{n-1}) = 0} \\
    \ProbMKN*{w_n}{w_1^{n-1}} &= \frac{\ContCountIp(\WSkp w_1^n)}{\ContCountIp(\WSkp w_1^{n-1} \WSkp)}
      & \Condition{\NumSkpOp(w_1^{n-1}) = 0}
  \end{align}
\end{subequations}

% ------------------------------------------------------------------------------
\section{Discounting and Interpolation Weights}
\label{sec:discounts-interpolation-weights}

\begin{draft}
Explain Discounting and Smoothing.
\end{draft}

\begin{draft}
\begin{equation}
  \Discount(c) =
    \begin{dcases*}
      0      & if $c = 0$ \\
      D_1    & if $c = 1$ \\
      D_2    & if $c = 2$ \\
      D_{3+} & if $c \geq 3$
    \end{dcases*}
\end{equation}

\begin{subequations}
  \begin{align}
    D_1    &= 1 - 2 Y \frac{n_2}{n_1} \\
    D_2    &= 2 - 3 Y \frac{n_3}{n_2} \\
    D_{3+} &= 3 - 4 Y \frac{n_4}{n_3}
  \end{align}
\end{subequations}

\begin{equation}
  Y = \frac{n_1}{n_1 + n_2}
\end{equation}

\begin{equation}
  \gamma(w_1^{n-1}) =   D_1    \ContCountI    (w_1^{n-1} \WSkp)
                      + D_2    \ContCountII   (w_1^{n-1} \WSkp)
                      + D_{3+} \ContCountIIIp (w_1^{n-1} \WSkp)
\end{equation}
\end{draft}
