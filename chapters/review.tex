\chapter{Review of Considered Language Models}
\label{ch:review}

The currently most commonly used \parencite{JurafskyMartin2009,Chelba2013}
technique for estimating language models is \emph{Modified Kneser-Ney Smoothing}
by \textcite{ChenGoodman1996,ChenGoodman1998,ChenGoodman1999}, based on
\emph{Kneser-Ney Smoothing} by \textcite{KneserNey1995}.
\textcite{BilmesKirchhoff2003} introduced the concept of a \emph{generalized
parallel backoff}, which very recently \textcite{Pickhardt2014} used to provide
a generalization of Modified Kneser-Ney Smoothing, the \emph{Generalized
Language Model}.
\todo{Justify selection of MKN and GLM over recently popular LMs from
\textcite{Chelba2013}.}

This chapter will introduce the used notation (\cref{sec:review-notation})
and recall the language modeling techniques considered in this work,
namely \emph{Modified Kneser-Ney Smoothing} (\cref{sec:review-lm-mkn}) and the
\emph{Generalized Language Model} (\cref{sec:review-lm-glm}).
However we will only give and explain definitions for these language models.
No motivation or justification of their genesis will be performed, as these are
not necessary to understand the contribution of this thesis.
The interested reader is instead hereby referred to the primary sources.


% ------------------------------------------------------------------------------
\section{Notation: Counts and Skips}
\label{sec:review-notation}

Let $\Count(w_1^n)$ denote the frequency count of a sequence's
$w_1^n = w_1 \ldots w_n$ occurrence in training data.
$w_1^n$ may be any contiguous sequence of length $n$ in a text.
So $w_1$ does not have to be the first word of a sentence or the like,
it just designates the first word of the sequence.

By writing two words directly next each other, as in $w_1 w_2$, we mean to
\emph{concatenate} them to a sequence of words $w_1^2$.
Sometimes for clarity concatenation is made explicit by writing
$w_1 \NGramConcat w_2$. We also extend this notation to concatenations of
sequences.

Let $\Skp$ be a wildcard (called \emph{``skip''}), that can take the place
of any word at its location.
Thus, for example $\Count(\Skp w_1^n)$ is the frequency count of sequences
in training data that start with any word $x \in \Sigma$ and where the remaining
words are $w_1^n$:
\begin{equation}
  \Count(\Skp w_1^n) = \sum_{x \in \Sigma} \Count(x \: w_1^n)
\end{equation}
Where $\Sigma$ is the \emph{vocabulary}, the set of all words in a language
$\Language \subseteq \Sigma^{*}$.
It is easy to extend this definition to sequences with multiple skips:
\begin{equation}
  \Count(w_1 \Skp \Skp w_2) = \sum_{x, y \in \Sigma} \Count(w_1 \: x \: y \: w_2)
\end{equation}

\todo[inline]{Do we mention the difference of $\Count(\Skp w_1^n)$ to $\Count(w_1^n)$
in absence of Beginning of Sentence tags?}

Additionally we define \emph{continuation} counts $\ContCount_k(\DummyArg)$ as
the number of sequences from the training data that occur exactly $k$ times and
can be constructed by replacing $\WSkp$ (called \emph{``continuation skip''})
with any word $x \in \Sigma$.
For example $\ContCountII(\WSkp w_1^n)$ denotes the number of words that can
precede $w_1^n$ in training data, and occur exactly $2$ times:
\begin{equation}
  \ContCountII(\WSkp w_1^n) =
    \Cardinality{\left\{ x \in \Sigma \,\middle|\, \Count(x \: w_1^n) = 2 \right\}}
\end{equation}
Further let $\ContCount_{k+}(\DummyArg)$ count sequences that occur $k$ or more
times:
\begin{equation}
  \ContCount_{k+}(\WSkp w_1^n) =
    \Cardinality{\left\{ x \in \Sigma \,\middle|\, \Count(x \: w_1^n) \geq k \right\}}
\end{equation}

In continuation counts regular skips and continuation skips can be mixed, for
example:
\todo{Check if equation actually corresponds with implementation.}
\begin{equation}
  \ContCountIp(\WSkp w \Skp \WSkp) =
    \Cardinality{\left\{ x, y \in \Sigma \,\middle|\, c(x \: w \Skp y) \geq 1 \right\}}
\end{equation}

To make a clear distinction between ``normal'' counts $\Count(\DummyArg)$ and
continuation counts $\ContCount_{\DummyIndex}(\DummyArg)$, we
sometimes call the former \emph{absolute} counts.

This notation mirrors that of previous publications in the field by
\textcite{ChenGoodman1996,ChenGoodman1998,ChenGoodman1999,Goodman2001,Pickhardt2014}.
With the exception of the new skip $\Skp$ wildcard, which is primarily based
on yet unpublished work by \textsc{Ren{\'e} Pickhardt}.
\todo{Examples of counting with small text corpus?}

% ------------------------------------------------------------------------------
\section{Modified Kneser-Ney Smoothing}
\label{sec:review-lm-mkn}

Our definition of Modified Kneser-Ney Smoothing is that of an
\emph{interpolated} language model.
It directly follows \textcite{ChenGoodman1999}, who report interpolated
techniques to be superior to the previous work of \emph{backoff} language
models by \textcite{KneserNey1995}.

Therefore Modified Kneser-Ney smoothed language models are calculated as an
interpolation of higher and lower order $n$-gram language models
\parencite{Pickhardt2014}.
The highest order model $\ProbMKN$ is interpolated with lower orders $\ProbMKN*$
as follows:
\begin{equation}
  \label{eq:mkn-high}
  \ProbMKN{w_n}{w_1^{n-1}} =
    \frac{\DiscountedCount(w_1^n) + \gamma(w_1^{n-1}) \ProbMKN*{w_n}{w_2^{n-1}}}
         {\Count(w_1^{n-1} \Skp)} \\
\end{equation}
With the \emph{discounted} count
${\DiscountedCount(w_1^n) = \max\Set{\Count(w_1^n) - \Discount(\Count(w_1^n)), 0}}$.

The exact definitions of $D(\DummyArg)$ and $\gamma(w_1^{n-1})$ are not
important in our context, however for the sake of completeness they are given
in \cref{sec:discounts-interpolation-weights}.

We call the word $w_n$ for which we calculate the probability the
\emph{argument}, and the conditional sequence $w_1^{n-1}$ the \emph{history}.

Naturally \cref{eq:mkn-high} is only defined if the denominator
${\Count(w_1^{n-1} \Skp) > 0}$, that is if the history $w_1^{n-1} \Skp$ was seen
in training.
If $w_1^{n-1} \Skp$ was not seen, we perform a so called \emph{``back-off''}
and set:
\begin{align}
  \label{eq:mkn-backoff}
  \ProbMKN{w_n}{w_1^{n-1}} = \ProbMKN{w_n}{w_2^{n-1}}
      & & \Condition{\Count(w_1^{n-1} \Skp) = 0}
\end{align}
It is common to perform multiple back-offs until the history was actually seen.

Thus, interpolation with lower order models correspond to leaving out the first
word of the history, on which our probabilities are conditioned.
Lower order models are computed differently, and in turn interpolated with
further lower orders:
\begin{equation}
  \label{eq:mkn-low}
  \ProbMKN*{w_n}{w_1^{n-1}} =
    \frac{\DiscountedCount*(\WSkp w_1^n) + \gamma(w_1^{n-1}) \ProbMKN*{w_n}{w_2^{n-1}}}
         {\ContCountIp(\WSkp w_1^{n-1} \WSkp)} \\
\end{equation}
With the \emph{discounted} continuation count
${\DiscountedCount*(\WSkp w_1^n) = \max\Set{ \ContCountIp(\WSkp w_1^n) - \Discount(\Count(w_1^n)), 0 }}$.
Backing-off to compute lower orders is never necessary, because
if history $w_1^{n-1}$ was seen, $w_2^{n-1}$ was seen as well.
\todo{This should actually not be true, because we increase n-gram length for
first lower order. How do we handle this in current implementation?}

The lowest order (the probability of a single word) can not be further
interpolated and we use the relative continuation frequency of that word.
Similarly if we only want to compute the probability of a single word on the
highest order, we compute it as the relative frequency of that word:
\begin{subequations}
  \label{eq:mkn-lowest}
  \begin{align}
    \label{eq:mkn-lowest-abs}
    \ProbMKN {w} &= \frac{\Count(w)}{\Count(\Skp)} \\
    \label{eq:mkn-lowest-cont}
    \ProbMKN*{w} &= \frac{\ContCountIp(\WSkp w)}{\ContCountIp(\WSkp \WSkp)}
  \end{align}
\end{subequations}

Note that this definition of lowest order probabilities deviates from
\textcite{ChenGoodman1999}.
They instead smooth the lowest order with the uniform distribution of
$\frac{1}{\Cardinality{\Vocab}}$.
\todo{Why don't we do this?}


% ------------------------------------------------------------------------------
\section{Generalized Language Model}
\label{sec:review-lm-glm}

The equations of this section match the ones from \textcite{Pickhardt2014},
but are given in a slightly modified own notation that enables easier proceeding
in \cref{ch:weightedsum}.

The Generalized Language Model is a natural extension to Modified Kneser-Ney
Smoothing.
It is again based on interpolation, though it differs in the way lower order
models are interpolated.
The highest order is computed as:
\begin{equation}
  \label{eq:glm-high}
  \ProbGLM{w_n}{w_1^{n-1}} =
    \frac{\DiscountedCount(w_1^n) + \frac{\gamma(w_1^{n-1})}{\NumSkpOp{w_1^{n-1}}}
                                    \sum_{j=1}^{\NumSkpOp{w_1^{n-1}}} \ProbGLM*{w_n}{\SkpOp[j]{w_1^{n-1}}}}
         {\Count(w_1^{n-1} \Skp)}
\end{equation}
Auxiliary definitions $\DiscountedCount$, $\DiscountedCount*$, $\gamma$ are the
same as for Modified Kneser-Ney Smoothing and given in \cref{sec:review-lm-mkn}.

The skip operator $\SkpOp[j]{w_1^{n-1}}$ can be any mapping that relates
$w_1^{n-1}$ to exactly $\NumSkpOp{w_1^{n-1}}$ many derived sequences.
It is easy to see that for $\NumSkpOp{w_1^{n-1}} = 1$ and
$\SkpOp[1]{w_1^{n-1}} = w_2^{n-1}$ \cref{eq:glm-high} is an
instantiation of \cref{eq:mkn-high} (Modified Kneser-Ney Smoothing), and thus
a true generalization.

In practice only one definition for $\SkpOp$ is used though.
That is $\NumSkpOp{w_1^{n-1}}$ is set to the number of non-skip words in
$w_1^{n-1}$ and $\SkpOp[j]{w_1^{n-1}}$ replaces the $j$th non-skip word with
a skip.
For instance: $\SkpOp[2]{w_1^3} = w_1 \Skp w_3$.

Again as with Modified Kneser-Ney Smoothing, \cref{eq:glm-high} is not
defined if $w_1^{n-1}$ is unseen and thus our denominator would be zero.
The back-off step is then performed as:
\begin{equation}
  \label{eq:glm-backoff}
  \begin{split}
    \ProbGLM{w_n}{w_1^{n-1}} = \frac{1}{\NumSkpOp{w_1^{n-1}}}
                               \sum_{j=1}^{\NumSkpOp{w_1^{n-1}}} \ProbGLM{w_n}{\SkpOp[j]{w_1^{n-1}}} \\
      \hfill \Condition{\Count(w_1^{n-1} \Skp) = 0}
  \end{split}
\end{equation}

Subsequently lower order models are computed as:
\todo{Replace \Skp with \WSkp in $\ContCountIp$ in GLM?}
\todo{\WSkp at end of $\ContCountIp$ correct?}
\begin{equation}
  \label{eq:glm-low}
  \ProbGLM*{w_n}{w_1^{n-1}} =
    \frac{\DiscountedCount*(\WSkp w_1^n) + \frac{\gamma(w_1^{n-1})}{\NumSkpOp{w_1^{n-1}}}
                                     \sum_{j=1}^{\NumSkpOp{w_1^{n-1}}} \ProbGLM*{w_n}{\SkpOp[j]{w_1^{n-1}}}}
         {\ContCountIp(\WSkp w_1^{n-1} \WSkp)}
\end{equation}

The case of the lowest order occurs when $\NumSkpOp{w_1^{n-1}} = 0$ and is
handled similarly to Modified Kneser-Ney Smoothing.
In practice this is the case when $w_1^{n-1}$ only consists of skips.
\begin{subequations}
  \label{eq:glm-lowest}
  \begin{align}
    \ProbGLM {w_n}{w_1^{n-1}} &= \frac{\Count(w_1^n)}{\Count(w_1^{n-1} \Skp)}
      & \Condition{\NumSkpOp{w_1^{n-1}} = 0} \\
    \ProbMKN*{w_n}{w_1^{n-1}} &= \frac{\ContCountIp(\WSkp w_1^n)}{\ContCountIp(\WSkp w_1^{n-1} \WSkp)}
      & \Condition{\NumSkpOp{w_1^{n-1}} = 0}
  \end{align}
\end{subequations}

% ------------------------------------------------------------------------------
\section{Discounting and Interpolation Weights}
\label{sec:discounts-interpolation-weights}

\begin{draft}
Explain Discounting and Smoothing.
\end{draft}

\begin{draft}
\begin{equation}
  \Discount(c) =
    \begin{dcases*}
      0      & if $c = 0$ \\
      D_1    & if $c = 1$ \\
      D_2    & if $c = 2$ \\
      D_{3+} & if $c \geq 3$
    \end{dcases*}
\end{equation}

\begin{subequations}
  \begin{align}
    D_1    &= 1 - 2 Y \frac{n_2}{n_1} \\
    D_2    &= 2 - 3 Y \frac{n_3}{n_2} \\
    D_{3+} &= 3 - 4 Y \frac{n_4}{n_3}
  \end{align}
\end{subequations}

\begin{equation}
  Y = \frac{n_1}{n_1 + n_2}
\end{equation}

\begin{equation}
  \gamma(w_1^{n-1}) =   D_1    \ContCountI    (w_1^{n-1} \WSkp)
                      + D_2    \ContCountII   (w_1^{n-1} \WSkp)
                      + D_{3+} \ContCountIIIp (w_1^{n-1} \WSkp)
\end{equation}
\end{draft}
