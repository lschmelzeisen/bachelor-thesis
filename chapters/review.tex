\chapter{Review of Considered Language Models}
\label{ch:review}

The currently most commonly used \parencite{JurafskyMartin2009,Chelba2013}
technique for estimating language models is \emph{Modified Kneser-Ney Smoothing}
by \textcite{ChenGoodman1996,ChenGoodman1998,ChenGoodman1999}, based on
\emph{Kneser-Ney Smoothing} by \textcite{KneserNey1995}.
\textcite{BilmesKirchhoff2003} introduced the concept of a \emph{generalized
parallel backoff}, which very recently \textcite{Pickhardt2014} used to provide
a generalization of Modified Kneser-Ney Smoothing, the \emph{Generalized
Language Model}.
\todo{Justify selection of MKN and GLM over recently popular LMs from
\textcite{Chelba2013}.}

\begin{draft}
Mention \emph{Nerual Network Language Models} \parencite{Bengio2003,Mikolov2012}
(do we not use them because they can not be represented as weighted sums?), as
they seem to be in favor of academic research recently.
\end{draft}

This chapter will introduce the used notation (\cref{sec:review-notation})
and recall the language modeling techniques considered in this work,
namely \emph{Modified Kneser-Ney Smoothing} (\cref{sec:review-lm-mkn}) and the
\emph{Generalized Language Model} (\cref{sec:review-lm-glm}).
However we will only give and explain definitions for these language models.
No motivation or justification of their genesis will be performed, as these are
not necessary to understand the contribution of this thesis.
Instead the interested reader is hereby referred to the primary sources.


% ------------------------------------------------------------------------------
\section{Notation: Counts and Skips}
\label{sec:review-notation}

Let $\Count(w_1^n)$ denote the frequency count of a sequence's
$w_1^n = w_1 \ldots w_n$ occurrence in training data.
$w_1^n$ may be any contiguous sequence of length $n$ in a text.
So $w_1$ does not have to be the first word of a sentence or the like,
it just designates the first word of the sequence.

By writing two words directly next each other, as in $w_1 w_2$, we mean to
\emph{concatenate} them to a sequence of words $w_1^2$.
Sometimes for clarity concatenation is made explicit by writing
$w_1 \NGramConcat w_2$. We also extend this notation to concatenations of
sequences.

Let $\Skp$ be a wildcard (called \emph{``skip''}), that can take the place
of any word at its location.
Thus, for example $\Count(\Skp w_1^n)$ is the frequency count of sequences
in training data that start with any word $x \in \Sigma$ and where the remaining
words are $w_1^n$:
\begin{equation}
  \Count(\Skp w_1^n) = \sum_{x \in \Sigma} \Count(x \: w_1^n)
\end{equation}
Where $\Sigma$ is the \emph{vocabulary}, the set of all words in a language
$\Language \subseteq \Sigma^{*}$.
It is easy to extend this definition to sequences with multiple skips:
\todo{Do we mention the difference of $\Count(\Skp w_1^n)$ to $\Count(w_1^n)$
in absence of Beginning of Sentence tags?}
\begin{equation}
  \Count(w_1 \Skp \Skp w_2) = \sum_{x, y \in \Sigma} \Count(w_1 \: x \: y \: w_2)
\end{equation}

Additionally we define \emph{continuation} counts $\ContCount_k(\DummyArg)$ as
the number of sequences from the training data that occur exactly $k$ times and
can be constructed by replacing $\WSkp$ (called \emph{``continuation skip''})
with any word $x \in \Sigma$.
For example $\ContCountII(\WSkp w_1^n)$ denotes the number of words that can
precede $w_1^n$ in training data, and occur exactly $2$ times:
\begin{equation}
  \ContCountII(\WSkp w_1^n) =
    \Cardinality{\left\{ x \in \Sigma \,\middle|\, \Count(x \: w_1^n) = 2 \right\}}
\end{equation}
Further let $\ContCount_{k+}(\DummyArg)$ count sequences that occur $k$ or more
times:
\begin{equation}
  \ContCount_{k+}(\WSkp w_1^n) =
    \Cardinality{\left\{ x \in \Sigma \,\middle|\, \Count(x \: w_1^n) \geq k \right\}}
\end{equation}

In continuation counts regular skips and continuation skips can be mixed, for
example:
\todo{Check if equation actually corresponds with implementation.}
\begin{equation}
  \ContCountIp(\WSkp w \Skp \WSkp) =
    \Cardinality{\left\{ x, y \in \Sigma \,\middle|\, c(x \: w \Skp y) \geq 1 \right\}}
\end{equation}

To make a clear distinction between ``normal'' counts $\Count(\DummyArg)$ and
continuation counts $\ContCount_{\DummyIndex}(\DummyArg)$, we
sometimes call the former \emph{absolute} counts.

This notation mirrors that of previous publications in the field by
\textcite{ChenGoodman1996,ChenGoodman1998,ChenGoodman1999,Goodman2001,Pickhardt2014}.
With the exception of the new skip $\Skp$ wildcard, which is primarily based
on yet unpublished work by \textsc{Ren{\'e} Pickhardt}.
\todo{Examples of counting with small text corpus?}

% ------------------------------------------------------------------------------
\section{Modified Kneser-Ney Smoothing}
\label{sec:review-lm-mkn}

Our definition of Modified Kneser-Ney Smoothing is that of an
\emph{interpolated} language model.
It directly follows \textcite{ChenGoodman1999}, who report interpolated
techniques to be superior to the previous work of \emph{backoff} language
models by \textcite{KneserNey1995}.

Therefore Modified Kneser-Ney smoothed language models are calculated as an
interpolation of higher and lower order $n$-gram language models
\parencite{Pickhardt2014}.
The highest order model $\ProbMKN$ is interpolated with lower orders $\ProbMKN*$
as follows:
\begin{equation}
  \label{eq:mkn-high}
  \ProbMKN{w_n}{w_1^{n-1}} =
    \frac{\DiscountedCount(w_1^n) + \gamma(w_1^{n-1}) \ProbMKN*{w_n}{w_2^{n-1}}}
         {\Count(w_1^{n-1} \Skp)} \\
\end{equation}
With the \emph{discounted} count
${\DiscountedCount(w_1^n) = \max\Set{\Count(w_1^n) - \Discount(\Count(w_1^n)), 0}}$.

The exact definitions of $D(\DummyArg)$ and $\gamma(w_1^{n-1})$ are not
important in our context, however for the sake of completeness they are given
in \cref{sec:discounts-interpolation-weights}.

We call the word $w_n$ for which we calculate the probability the
\emph{argument}, and the conditional sequence $w_1^{n-1}$ the \emph{history}.

\Cref{eq:mkn-high} is only defined if the denominator
${\Count(w_1^{n-1} \Skp) > 0}$, that is if the history $w_1^{n-1} \Skp$ was seen
in training.
If $w_1^{n-1} \Skp$ was not seen, we perform a so called \emph{``back-off''}
and set:
\begin{align}
  \label{eq:mkn-backoff}
  \ProbMKN{w_n}{w_1^{n-1}} = \ProbMKN{w_n}{w_2^{n-1}}
      & & \Condition{\Count(w_1^{n-1} \Skp) = 0}
\end{align}
It is common to perform multiple back-offs until the history was actually seen.

Thus, interpolation with lower order models correspond to leaving out the first
word of the history, on which our probabilities are conditioned.
Lower order models are computed differently, and in turn interpolated with
further lower orders:
\begin{equation}
  \label{eq:mkn-low}
  \ProbMKN*{w_n}{w_1^{n-1}} =
    \frac{\DiscountedCount*(\WSkp w_1^n) + \gamma(w_1^{n-1}) \ProbMKN*{w_n}{w_2^{n-1}}}
         {\ContCountIp(\WSkp w_1^{n-1} \WSkp)} \\
\end{equation}
With
${\DiscountedCount*(\WSkp w_1^n) = \max\Set{ \ContCountIp(\WSkp w_1^n) - \Discount(\Count(w_1^n)), 0 }}$.
the \emph{discounted} continuation count.

Backing-off to compute lower orders is never necessary, because
if history $w_1^{n-1}$ was seen, $w_2^{n-1}$ was seen as well.
\todo{This should actually not be true, because we increase n-gram length for
first lower order. How do we handle this in current implementation?}

The lowest order (the probability of a single word) can not be further
interpolated and we use the relative continuation frequency of that word.
Similarly if we only want to compute the probability of a single word on the
highest order, we compute it as the relative frequency of that word:
\begin{subequations}
  \label{eq:mkn-lowest}
  \begin{align}
    \label{eq:mkn-lowest-abs}
    \ProbMKN {w} &= \frac{\Count(w)}{\Count(\Skp)} \\
    \label{eq:mkn-lowest-cont}
    \ProbMKN*{w} &= \frac{\ContCountIp(\WSkp w)}{\ContCountIp(\WSkp \WSkp)}
  \end{align}
\end{subequations}

Note that this definition of lowest order probabilities deviates from
\textcite{ChenGoodman1999}.
They instead smooth the lowest order with the uniform distribution of
$\frac{1}{\Cardinality{\Vocab}}$.
\todo{Why don't we do this?}


% ------------------------------------------------------------------------------
\section{Generalized Language Model}
\label{sec:review-lm-glm}

The equations of this section match the ones from \textcite{Pickhardt2014},
but are given in a slightly modified own notation that enables easier proceeding
in \cref{ch:weightedsum}.

The Generalized Language Model is a natural extension to Modified Kneser-Ney
Smoothing.
It is again based on interpolation, though it differs in the way lower order
models are interpolated.
The highest order is computed as:
\begin{equation}
  \label{eq:glm-high}
  \ProbGLM{w_n}{w_1^{n-1}} =
    \frac{\DiscountedCount(w_1^n) + \frac{\gamma(w_1^{n-1})}{\NumSkpOp{w_1^{n-1}}}
                                    \sum_{j=1}^{\NumSkpOp{w_1^{n-1}}} \ProbGLM*{w_n}{\SkpOp[j]{w_1^{n-1}}}}
         {\Count(w_1^{n-1} \Skp)}
\end{equation}
Auxiliary definitions $\DiscountedCount$, $\DiscountedCount*$, $\gamma$ are the
same as for Modified Kneser-Ney Smoothing and given in
\cref{sec:discounts-interpolation-weights}.

The skip operator $\SkpOp[j]{w_1^{n-1}}$ can be any mapping that relates
$w_1^{n-1}$ to exactly $\NumSkpOp{w_1^{n-1}}$ many derived sequences.
It is easy to see that for $\NumSkpOp{w_1^{n-1}} = 1$ and
$\SkpOp[1]{w_1^{n-1}} = w_2^{n-1}$ \cref{eq:glm-high} is an
instantiation of \cref{eq:mkn-high} (Modified Kneser-Ney Smoothing), and thus
a true generalization.

In practice only one definition for $\SkpOp$ is used though.
That is $\NumSkpOp{w_1^{n-1}}$ is set to the number of non-skip words in
$w_1^{n-1}$ and $\SkpOp[j]{w_1^{n-1}}$ replaces the $j$th non-skip word with
a skip.
For instance: $\SkpOp[2]{w_1^3} = w_1 \Skp w_3$.

Again as with Modified Kneser-Ney Smoothing, \cref{eq:glm-high} is not
defined if $w_1^{n-1}$ is unseen and thus our denominator would be zero.
The back-off step is then performed as:
\begin{equation}
  \label{eq:glm-backoff}
  \begin{split}
    \ProbGLM{w_n}{w_1^{n-1}} = \frac{1}{\NumSkpOp{w_1^{n-1}}}
                               \sum_{j=1}^{\NumSkpOp{w_1^{n-1}}} \ProbGLM{w_n}{\SkpOp[j]{w_1^{n-1}}} \\
      \hfill \Condition{\Count(w_1^{n-1} \Skp) = 0}
  \end{split}
\end{equation}

Subsequently lower order models are computed as:
\todo{Replace \Skp with \WSkp in $\ContCountIp$ in GLM?}
\todo{\WSkp at end of $\ContCountIp$ correct?}
\begin{equation}
  \label{eq:glm-low}
  \ProbGLM*{w_n}{w_1^{n-1}} =
    \frac{\DiscountedCount*(\WSkp w_1^n) + \frac{\gamma(w_1^{n-1})}{\NumSkpOp{w_1^{n-1}}}
                                     \sum_{j=1}^{\NumSkpOp{w_1^{n-1}}} \ProbGLM*{w_n}{\SkpOp[j]{w_1^{n-1}}}}
         {\ContCountIp(\WSkp w_1^{n-1} \WSkp)}
\end{equation}

The case of the lowest order occurs when $\NumSkpOp{w_1^{n-1}} = 0$ and is
handled similarly to Modified Kneser-Ney Smoothing.
In practice this is the case when $w_1^{n-1}$ only consists of skips.
\begin{subequations}
  \label{eq:glm-lowest}
  \begin{align}
    \ProbGLM {w_n}{w_1^{n-1}} &= \frac{\Count(w_1^n)}{\Count(w_1^{n-1} \Skp)}
      & \Condition{\NumSkpOp{w_1^{n-1}} = 0} \\
    \ProbMKN*{w_n}{w_1^{n-1}} &= \frac{\ContCountIp(\WSkp w_1^n)}{\ContCountIp(\WSkp w_1^{n-1} \WSkp)}
      & \Condition{\NumSkpOp{w_1^{n-1}} = 0}
  \end{align}
\end{subequations}

% ------------------------------------------------------------------------------
\section{Discounting and Interpolation Weights}
\label{sec:discounts-interpolation-weights}

The purpose of the discounted count functions $\DiscountedCount(\DummyArg)$
and $\DiscountedCount*(\DummyArg)$ is to reduce the absolute values of
higher order probability of the language model.
This is done in order to have \enquote{left over probability mass} to assign
to lower orders \parencite{NeyEssen1991,NeyKneser1994}.

These discounted counts are defined as:
\begin{subequations}
  \begin{align}
    \DiscountedCount(w_1^n)        &= \max \Set{\Count(w_1^n) - \Discount(\Count(w_1^n)), \, 0} \\
    \DiscountedCount*(\WSkp w_1^n) &= \max \Set{ \ContCountIp(\WSkp w_1^n) - \Discount(\Count(w_1^n)), \, 0 }
  \end{align}
\end{subequations}

Discounting is done by subtracting a value $\Discount(c)$ from each count.
The higher the count value $c$ the higher it is discounted:
\todo{Citations missing?}
\begin{equation}
  \Discount(c) =
    \begin{dcases*}
      0      & if $c = 0$ \\
      D_1    & if $c = 1$ \\
      D_2    & if $c = 2$ \\
      D_{3+} & if $c \geq 3$
    \end{dcases*}
\end{equation}
With the the discounting values $D_\DummyIndex$ defined as:
\begin{subequations}
  \begin{align}
    D_1    &= 1 - 2 Y \frac{n_2}{n_1} \\
    D_2    &= 2 - 3 Y \frac{n_3}{n_2} \\
    D_{3+} &= 3 - 4 Y \frac{n_4}{n_3}
  \end{align}
\end{subequations}
Where $Y = \frac{n_1}{n_1 + n_2}$, and where $n_i$ denotes the total number of
of \mbox{$n$-grams} which appear exactly $i$ times \parencite{Pickhardt2014}.
\todo{This doesn't really fully explain how $n_i$ works does it?}

Based on this we can define the interpolation weight $\gamma(\DummyArg)$ that
determines how much lower order probabilities factor into those of higher
orders \parencite{Pickhardt2014}:
\begin{equation}
  \gamma(w_1^{n-1}) =   D_1    \ContCountI    (w_1^{n-1} \WSkp)
                      + D_2    \ContCountII   (w_1^{n-1} \WSkp)
                      + D_{3+} \ContCountIIIp (w_1^{n-1} \WSkp)
\end{equation}
For the given discounted count functions $\DiscountedCount$ and
$\DiscountedCount*$ it has to be chosen like this to ensure true probabilities.
