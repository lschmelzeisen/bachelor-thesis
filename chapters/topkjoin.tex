\chapter{Fast Prefix Queries using Top-\emph{k} Joins}
\label{ch:topkjoin}

\todo[inline]{Update math notation once it has been fixed in
\cref{ch:weightedsum}.}

As motivated in the introduction, we want to solve prefix queries based
on statistical language models.
Our objective is to reduce the execution time of a high number of next word
prediction queries for arbitrary histories and prefixes.
\Cref{eq:prefixquery} shows that next word prediction queries $\NWP[p][k](h)$
are an argmax of probabilities.
To optimize query time we follow a two-pronged approach:
First we identify calculations shared by multiple probabilities, and perform
them only once up front.
Secondly we try to avoid enumerating the whole argument space of the argmax.
Instead our goal is to always compute the probability for the next most
promising argument, and to terminate as soon as we are sure to have found the
$k$ words that maximize the probability.

As specified in \cref{ch:review} each language model probability  is calculated
from occurrence counts of sequences in a corpus.
In order to avoid having to analyze the corpus on each query for its relevant
sequences, the number of occurrence for all contained sequences in a corpus is
counted up front in a learning phase.
We store those counts in a data structure that is optimized for necessary
retrieval.
We will explain that data structure and define what necessary retrieval means
in \cref{sec:completiontrie}.
Subsequently we may persist that data, so that for each requested corpus the
necessary analyzation only has to be performed once.

To find the answer for a query $\NWP[p][k]{h}$ probabilities $\Prob{w}{h}$ have
to be calculated for a fixed history $h$ but many different $w$.
We have shown in \cref{ch:weightedsum} that we can represent each probability
as a weighted sum that resembles \cref{eq:weightedsum}.
The weights $\SumWeight^h_i$ of that sum are composed of exactly these counts
that are independent of the argument $w$.
This enables us to calculate the $\SumWeight_i^h$ once beforehand and reuse them
for every probability calculation of a query.
The remaining factors $\SumArg^h_i(w)$ depend on $w$.
This means we can find a function $s$ that expresses our
probabilities\footnote{The definition for $s$ is just \cref{eq:weightedsum}.
This new representation is only introduced to make it clear, that given the
$\SumWeight^h_i$ the probability can be calculated from the $\SumArg^h_i(w)$.}:
\todo{It is not visible from the notation that $s$ depends on the
  $\SumWeight^h_i$.}
\begin{equation}
  \label{eq:scoringfunc}
  \Prob{w}{h} = s\left(\SumArg^h_1(w), \, \ldots, \, \SumArg^h_N(w)\right)
\end{equation}

% ------------------------------------------------------------------------------
\section{Top-\emph{k} Joining Techniques}
\label{sec:topkjoin}

The problem we are trying to solve, namely finding the $k$ words $w$ that
maximize our probability $\Prob{w}{h}$, is actually well researched under the
topic of \emph{top-$k$ joining techniques}.
The result of a join operation is the set of all combinations of tuples from
multiple data sources, where each tuple combination satisfies the \emph{join
condition}.
Top-$k$ joins are then these that only return the $k$ highest scoring
according to some \emph{scoring function}.

We can easily see that our prefix queries fit the scheme of top-$k$ join
queries.
To calculate the probability of any word $w$, we have to look up all
$\SumArg^h_i(w)$ that belong to that word.
Finding all combinations
$\left(\SumArg^h_1(w), \, ..., \, \SumArg^h_N(w)\right)$
for each word $w$ is actually a join operation with the join condition that
the $w$ is constant over the $\SumArg^h_i(w)$ of a single combination.
As defined, for a query $\NWP[p][k]{w}$ we only want to return the $k$ best
predictions.
The $k$ best predictions are those that yield the $k$ highest values according
to a scoring function, which in our case is the probability.
Each join combination gives the arguments for the scoring function, as shown
in \cref{eq:scoringfunc}.

Almost all top-$k$ joining techniques that try to be more efficient than
enumerating all join combinations, require \emph{monotone} scoring functions.
Monotone in our context means that higher $\SumArg^h_i(w)$ values correspond
to higher probabilities.
We have shown in \cref{sec:monotony} that our probability definitions are
monotone on the $\SumArg^h_i(w)$.
This fact can be utilized to find the top-$k$ predictions by always calculating
the probability for the next promising $w$ with the highest $\SumArg^h_i(w)$.

Finding the $w$ with the highest $\SumArg^h_i(w)$ requires sorting.
Since join combinations
$\left(\SumArg^h_1(w), \, ..., \, \SumArg^h_N(w)\right)$ are multi-dimensional
there is no meaningful order defined on them, besides sorting by the scoring
function.
However that requires calculating the probability for each join
combination, which is precisely what we are trying to avoid.
For this reason we instead maintain $N$ data relations, one for each
$\SumArg^h_i(w)$, and sort them independently in the learning phase.
During query time, we can then perform \emph{sorted access} on these relations,
that is for a given history $h$ we want to retrieve words $w$ paired with and
sorted by their $\SumArg^h_i(w)$.
Some top-$k$ joining techniques additionally require \emph{random access},
that is for a given history $h$ and a given word $w$ we want to find
$\SumArg^h_i(w)$.
The data structure we use for this will be explained in
\cref{sec:completiontrie}.

Among other things, top-$k$ joining techniques can be classified by the type
of join operation they support \parencite{Ilyas2008}.
The join operation for our problem is whether for two values $\SumArg^h_i(w_x)$
and $\SumArg^h_j(w_y)$ the $w_x$ and $w_y$ are equal.
This is actually the simplest and most common type of join operation and is
called an \emph{equi-join}.
Those have the nice property that for each $\SumArg^h_i(w)$ there is exactly one
corresponding $\SumArg^h_j(w)$ for all $j$ that are different from $i$, which
allows easy indexing in a hash table by the key $w$.
Top-$k$ joining techniques that operate on equi-joins are also called
\emph{top-$k$ selections} \parencite{Ilyas2008}.

Among others, two fundamental algorithms were established by
\textcite{Fagin2001}: the \emph{threshold algorithm}, which requires sorted and
random access, and the \emph{no random access algorithm}, which only requires
sorted access.
We will apply these two algorithms to the problem of next word prediction and
compare them against each other and the simple argmax approach.
\textcite{Fagin2001} prove that both these algorithms are \emph{instance
optimal} solutions to top-$k$ join queries: there is no possible algorithm
that provides a better solution over all possible database contents.

However algorithms which outsource some computation into a precompution step, and thus
achieve a faster query response are still possible:
there exists a plethora of such more sophisticated top-$k$ joining techniques,
of which \textcite{Ilyas2008} surveys a wide number.
For example \emph{onion indices} \parencite{Chang2000} and
\emph{rank join indices} \parencite{Tsaparas2003} feature an extended learning
phase, in which optimized data representation are created over all arguments to
the scoring function.
These allow faster top-$k$ joins for arbitrary scoring functions over the same
arguments.
However optimizations like these are not applicable to our prefix query problem,
as the history $h$ changes with each query $\NWP[p][k](h)$.
Because of this the whole argument space $\SumArg^h_i(w)$ changes, which in turn
means we would have to built indices with each query, obviously defeating the
purpose.
The \emph{Rank Join operator} \parencite{Ilyas2004} is another more recent
algorithm, designed for fast solution in the presence of arbitrary join
conditions.
However in the precense of an equi-join it practically reduces to the threshold
algorithm.

% ------------------------------------------------------------------------------
\section{Corpus Data Structure: Completion Trie}
\label{sec:completiontrie}

For our needs we require a data structure that can return sorted pairs
$\left(w, \SumArg^h_i(w)\right)$ for each $i$ for a given history $h$.
Additionally as we are solving prefix queries, it has to be possible to specify
a prefix $p$ with which all $w$ have to start.

On top of that the threshold algorithm requires random access to the data
structure, that is for a given $w$ we want to find $\SumArg^h_i(w)$.
Random access might yield additional information to an algorithm and thus allow
it to terminate earlier.
Also our definitions to calculate the sum weights $\SumWeight^h_i$ given in
\cref{ch:weightedsum} require this form of random access, so it is desirable for
our data structure to support it.
The alternative would be to maintain a separate data store that is optimized for
random access --- for example a hash map --- which would necessitate a huge
memory usage.

Another desirable characteristic of data storage is data compression.
Since the analyzation of larger corpora yields to more accurate language models
\noref, a higher degree of data compression allows to perform more accurate
word prediction on the same hardware.
Obviously a balance between data compression and access time has to be found,
since fast prefix queries are the goal of this thesis.

The data structure which satisfies all these requirements is the
\emph{completion trie} described by \textcite{HsuOttaviano2013}.
A completion trie is a compact prefix tree that allows storage of arbitrary
pairs of strings and integer-scores.
A prefix trie is an ordered tree data structure where each node stores a
character sequence.
Retrieving a full string from the trie is done by concatenating all character
sequences along a path from the root to a leaf node.
In a completion trie each node also stores the highest score of all its leaf
nodes.
This enables very fast retrieval of all string-score-pairs sorted by score,
where the string start with a requested prefix, hence the name
\emph{completion} trie.

\textcite{HsuOttaviano2013} devise a method for storing only a minimal number of
tree edges, while maintaining data locality.
Further employing a variable-byte encoding they report an average compression
ratio of their data structure of 51\% compared to raw uncompressed text files.
In the same publication \textcite{HsuOttaviano2013} present two alternative
data structures for the same task that achieve compression ratios of 29\%
respectively 30\%.
However they report access times which are at least twice as high as those
of the completion trie.
Since speed is the primary concern of this thesis these alternatives
were not further considered.

For $1 \leq i \leq N$ we maintain $N$ many completion trie instances and store
pairs $\left(h \NGramConcat w, \SumArg^h_i(w)\right)$ in each trie for all possible
histories $h$ and words $w$, where $h \NGramConcat w$ is the concatenation of
$h$ and $w$.
This enables all our desired accessing schemes:
\begin{itemize}
  \item Sorted access for some history $h$ is done by querying for completions
    of $h$.
    \todo{Actually $\hat h$.}
    Results are then pairs where the string starts with the requested
    history.
    The remaining part of the string is the word $w$.
    Results are sorted on the score $\SumArg^h_i(w)$.
    \todo{Querying $i$-th trie is only said implicitly.}
  \item A prefix $p$ with which $w$ has to start is easily included.
    Querying for completions of $h \NGramConcat p$ returns the same pairs as above, but only
    those where $p$ is a prefix in $w$.
  \item Random access for a fixed history $h$ and fixed word $w$ is achieved by
    walking the path along the trie that is described by $h \NGramConcat w$.
    If that path ends in a leaf node, this node's score is $\SumArg^h_i(w)$.
    If that path does not exists, or the found node is not a leaf node,
    $\SumArg^h_i(w)$ is zero.
\end{itemize}

% ------------------------------------------------------------------------------
\section{Threshold Algorithm}
\label{sec:thresholdalgorithm}

The \emph{Threshold Algorithm}, applied to our problem $\NWP[p][k](w)$, roughly
works as follows \parencite{Fagin2001}:
\begin{enumerate}
  \item We perform sorted access to data sources (in our case completion tries
    are queried with prefix $h \NGramConcat p$) in any order.
    One strategy might be a round robin approach, that is each source is
    accessed once in turn.
  \item Every time a pair $(w, \SumArg^h_i(w))$ is a encountered with a new word
    $w$, we look up all other $\SumArg^h_j(w)$ for $j \neq i$ with random
    access.
    With these the word probability
    $\Prob{w}{h} = s(\SumArg^h_1, \ldots, \SumArg^h_N)$ can be calculated.
    We keep track of the $k$ pairs $(w, \: \Prob{w}{h})$ with the highest
    probabilities.
  \item Let $\overline{\SumArg}_i$ denote the last count $\SumArg^h_i(w)$ that
    was encountered during sorted access to the $i$th data source.
    Then the \emph{threshold}
    $t = s(\overline{\SumArg}_1, \ldots, \overline{\SumArg}_N)$
    is an upper bound for the probability of all words that have not yet been
    seen.
  \item Once we have encountered $k$ probabilities greater than the current
    threshold $t$ the algorithm terminates and the $k$ best pairs
    $(w, \: \Prob{w}{h})$ are returned.
\end{enumerate}

\begin{algorithm}
  \caption{\emph{Threshold Algorithm} to solve $\NWP[p][k]{h}$}
  \label{alg:thresholdalgorithm}
  \begin{algorithmic}[1]
    \Require $C_1, \: \ldots, \: C_N$
      \Comment{corpus data stored in Completion Tries}
    \Require $history$, $prefix$, $limit$
    \Statex \Comment{history $h$, prefix $p$ and limit $k$ for prefix query $\NWP[p][k]{h}$}
    \Ensure $predictions$
      \Comment{list of word-probability-pairs}

    \State $\SumWeight_1, \: \ldots, \: \SumWeight_N \gets$ \Call{calcSumWeights}{history}
      \Comment{see \cref{ch:weightedsum}}
      \label{ln:ta-sumweights}
    \For{$i$ \textbf{from} $1$ \textbf{to} $N$}
      \label{ln:ta-bara-for}
      \State $\overline{\SumArg}_i \gets C_i$.\Call{peekCount}{history $\NGramConcat$ prefix}
        \Comment{threshold factors}
        \label{ln:ta-bara-peek}
    \EndFor
    \State seen $\gets$ \textbf{new} \Call{Set}{$ $}
      \Comment{set of seen words}
      \label{ln:ta-seen}
    \State queue $\gets$ \textbf{new} \Call{PriorityQueue}{$ $}
      \Comment{prediction candidates}
      \label{ln:ta-queue}

    \vspace{0.7em}
    \While{$i \gets$ \Call{nextTrie}{$ $}}
      \LineComment{perform sorted access, track seen words}
      \label{ln:ta-while}
      \State word$,$ count $\gets C_i$.\Call{next}{history $\NGramConcat$ prefix}
        \label{ln:ta-next}
      \If{seen.\Call{contains}{word}}
        \label{ln:ta-isseen-if}
        \State \textbf{continue}
          \Comment{skip words already seen in previous iterations}
          \label{ln:ta-isseen-continue}
      \EndIf
      \vspace{-1.2em} % TODO: how is the gap created we are hotfixing here?
      \State seen.\Call{add}{word}
        \label{ln:ta-seen-add}

      \vspace{0.7em}
      \LineComment{compute word probability}
      \For{$j$ \textbf{from} $1$ \textbf{to} $N$}
        \If{j == i}
          \State $\SumArg_j \gets$ count
            \label{ln:ta-a-sorted}
        \Else
          \State $\SumArg_j \gets C_j$.\Call{get}{history $+$ word}
            \Comment{random access}
            \label{ln:ta-a-random}
        \EndIf
      \EndFor
      \State probability $\gets \sum_{j = 0}^N \SumWeight_j \SumArg_j$
        \label{ln:ta-prob}

      \vspace{0.7em}
      \LineComment{keep limit-many best predictions}
      \State queue.\Call{push}{(word, probability)}
        \label{ln:ta-queue-add}
      \If{queue.\Call{size}{$ $} $>$ limit}
        \label{ln:ta-queue-full}
        \State queue.\Call{removeLowestProbabilityEntry}{$ $}
          \label{ln:ta-queue-remove}
      \EndIf

      \vspace{0.7em}
      \LineComment{compute threshold}
      \State $\overline{\SumArg}_i \gets \SumArg_i$
        \label{ln:ta-bara-update}
      \State threshold $\gets \sum_{j = 0}^N \SumWeight_j \overline{\SumArg}_j$
        \label{ln:ta-threshold}

      \vspace{0.7em}
      \LineComment{stop if better predictions are impossible}
      \If{queue.\Call{size}{$ $} $=$ limit \textbf{and}
        \label{ln:ta-done-if-1}
        \\\hspace{2.5em}threshold $<$ queue.\Call{getLowestProbability}{$ $}}
        \label{ln:ta-done-if-2}
        \State \textbf{break}
          \label{ln:ta-done-break}
      \EndIf
    \EndWhile

    \vspace{0.7em}
    \State predictions $\gets$ queue.\Call{toList}{$ $}
      \label{ln:ta-tolist}
  \end{algorithmic}
\end{algorithm}

Our implementation of the \emph{Threshold Algorithm} is given in
\cref{alg:thresholdalgorithm}.

As input we require $N$ completion tries from which we can extract
$(w, \: \SumArg^h_i(w))$ pairs in sorted or random order as described in
\cref{sec:completiontrie}.
Further, as we are finding answers to an $\NWP[p][k]{h}$ query, those parameters
are necessary.
Both \emph{history} and \emph{prefix} are strings that may be empty;
\emph{limit} has to be an integer greater zero.
The output \emph{predictions} will be a list composed of at most
\emph{limit} $(w, \: \Prob{w}{h})$ pairs, sorted descendingly by probability.

The calculation of probabilities depends on sum weights
$\SumWeight_1, \: \ldots, \: \SumWeight_N$.
They only depend on the fixed \emph{history} and can thus be computed once at
the start (\cref{ln:ta-sumweights}).
To calculate the \emph{threshold}, the upper bound for all of remaining
probabilities, the factors
$\overline{\SumArg}_1, \: \ldots, \: \overline{\SumArg}_N$ are required.
Those are the last accessed counts $\SumArg_i(w)$ of each trie.
Before performing any trie-access we initialize them to the highest count in the
trie (\cref{ln:ta-bara-for,ln:ta-bara-peek}).
\emph{Peeking} a data source is the operation that returns the next entry in it,
without advancing its iterator.

\emph{seen} is a set that keeps track of all processed words thus far
(\cref{ln:ta-seen}).
It's purpose is to avoid random access for the calculation of probabilities
that have been performed before.
It is an optimization that enables a faster execution time at the cost of
a higher space complexity.\todo{HashSet}

\emph{queue} keeps track of the word-probability-pairs with the highest
probabilities that are candidates for prediction (\cref{ln:ta-queue}).
At any point in time it contains at most $\text{\emph{limit}} + 1$ entries.
It is implemented as a priority queue, which we expect to be ordered on
the probabilities of each entry.
Thus efficient querying (\cref{ln:ta-done-if-2}) or removal
(\cref{ln:ta-queue-remove}) of the entry with the lowest probability is
possible.

After initialization is done we query words until we have found the $limit$
best predictions.
The \Call{nextTrie}{$ $} function in \cref{ln:ta-while} returns the number $i$
of the trie to be queried next.
It is thus stateful and needs to keep track of previous return values.
In our implementation we perform a round-robin approach, e.g.\ tries are
returned in the order $1, 2, \ldots, N, 1, 2, \ldots$
Once a trie is depleted it is omitted.

For each iteration we perform sorted access and retrieve the next
word-count-pair from the current trie, that starts with the requested
\emph{history} and \emph{prefix} (\cref{ln:ta-next}).
The \Call{next}{$\DummyArg$} method of a trie returns the current pair under
the iterator and advances it in sorted order.
If the word has been encountered in a previous iteration from a different trie
we continue with the next iteration step
(\cref{ln:ta-isseen-if,ln:ta-isseen-continue}).
If the word is new, we add it to the list of seen words (\cref{ln:ta-seen-add}).

In the following the probability $\Prob{w}{h}$ for the word is calculated
(\cref{ln:ta-prob}).
The factor $\SumArg_i$ is known from sorted access (\cref{ln:ta-a-sorted}), all
other $\SumArg_j$ with $j \neq i$ are retrieved by random access
(\cref{ln:ta-a-random}).
With the word's probability we update our record of the \emph{limit} best
predictions thus far
(\cref{ln:ta-queue-add,ln:ta-queue-full,ln:ta-queue-remove}).
The sorting of the priority queue happens implicit with the
\Call{push}{$\DummyArg$} operation.

Afterwards the last access sorted count of the current trie is update
(\cref{ln:ta-bara-update}) and with it the \emph{threshold} calculated
(\cref{ln:ta-threshold}).
If we already found $limi$ predictions and the probability of all those
predictions is higher than our upper bound of all remaining ones, the algorithm
terminates (\cref{ln:ta-done-if-1,ln:ta-done-if-2,ln:ta-done-break}).
The resulting \emph{predictions} are formed by the conversion of the
\emph{queue} to a list (\cref{ln:ta-tolist}), where the sorting of the
\emph{queue} is respected.

% ------------------------------------------------------------------------------
\section{No Random Access Algorithm}
\label{sec:norandomaccessalgorithm}

The \emph{No Random Access Algorithm}, applied to our problem $\NWP[p][k](w)$,
roughly works as follows \parencite{Fagin2001}:
\begin{enumerate}
  \item We perform sorted access to data sources (in our case completion tries
    are queried with prefix $h \NGramConcat p$) in any order.
    One strategy might be a round robin approach, that is each source is
    accessed once in turn.
  \item We maintain a mapping that initially maps all words to \emph{unkown}
    counts $\SumArg^h_i(w)$.
    For every word-count pair queried, we set that count in the mapping to its
    now know value.
  \item Using this mapping we calculate upper and lower bounds for all seen
    words.
    Let $\overline{\SumArg}_i$ denote the last count $\SumArg^h_i(w)$ that
    was encountered during sorted access to the $i$th data source.
    The upper bound for a word probability is calculated by replacing unkown
    counts with their highest possible value $\overline{\SumArg}_i$.
    Lower bounds are calculated by replaced unkown counts with zeros.
  \item Once $k$ words have been found, whose lower bound of probability is
    higher than all other upper bounds, the algorithm terminates and returns
    these $k$ best predictions.
\end{enumerate}

\begin{algorithm}
  \caption{\emph{No Random Access Algorithm} to solve $\NWP[p][k]{h}$}
  \label{alg:norandomaccessalgorithm}
  \begin{algorithmic}[1]
    \Require $C_1, \: \ldots, \: C_N$
      \Comment{corpus data stored in Completion Tries}
    \Require $history$, $prefix$, $limit$
    \Statex \Comment{history $h$, prefix $p$ and limit $k$ for prefix query $\NWP[p][k]{h}$}
    \Ensure $predictions$
      \Comment{list of words}

    \State $\SumWeight_1, \: \ldots, \: \SumWeight_N \gets$ \Call{calcSumWeights}{history}
      \Comment{see \cref{ch:weightedsum}}
      \label{ln:nra-sumweights}
    \For{$i$ \textbf{from} $1$ \textbf{to} $N$}
      \label{ln:nra-bara-for}
      \State $\overline{\SumArg}_i \gets C_i$.\Call{peekCount}{history $\NGramConcat$ prefix}
        \label{ln:nra-bara-peek}
    \EndFor
    \State wordCounts $\gets$ \textbf{new} \Call{Map}{$ $}
      \Comment{map from words to their counts}
      \label{ln:nra-wordcounts}
    \State queue $\gets$ \textbf{new} \Call{PriorityQueue}{$ $}
      \Comment{all predictions}
      \label{ln:nra-queue}
    \State predictions $\gets$ \textbf{new} \Call{List}{$ $}
      \label{ln:nra-predictions}

    \vspace{0.7em}
    \While{$i \gets$ \Call{nextTrie}{$ $}}
      \label{ln:nra-while}
      \LineComment{perform sorted access, track known counts per word}
      \State word$,$ count $\gets C_i$.\Call{next}{history $\NGramConcat$ prefix}
        \label{ln:nra-next}
      \If{\textbf{not} wordCounts.\Call{contains}{word}}
        \label{ln:nra-contains}
        \State $\SumArg_1, \: \ldots, \: \SumArg_N \gets $ \textbf{unkown}$, \: \ldots, \:$\textbf{unkown}
          \label{ln:nra-set-unkown}
      \Else
        \State $\SumArg_1, \: \ldots, \: \SumArg_N \gets $ wordCounts.\Call{get}{word}
          \label{ln:nra-get-wordcounts}
        \State queue.\Call{remove}{word}
          \label{ln:nra-queue-remove}
      \EndIf
      \State $\SumArg_i \gets$ count
        \label{ln:nra-a-set}
      \State wordCounts.\Call{set}{word, $(\SumArg_1, \: \ldots, \: \SumArg_N)$}
        \label{ln:nra-update-mapping}

      \vspace{0.7em}
      \LineComment{compute word probability upper- and lower-bound}
      \State $\overline{\SumArg}_i \gets \SumArg_i$
        \label{ln:nra-bara-update}
      \State upperBound $\gets 0$
        \label{ln:nra-upperbound-init}
      \State lowerBound $\gets 0$
        \label{ln:nra-lowerbound-init}
      \For{$j$ \textbf{from} $1$ \textbf{to} $N$}
        \If{$\SumArg_j =$ \textbf{unknown}}
          \State upperBround $\gets$ upperBound $+ \SumWeight_j \overline{\SumArg}_j$
            \label{ln:nra-upperbound-unknown}
        \Else
          \State upperBound $\gets$ upperBound $+ \SumWeight_j \SumArg_j$
            \label{ln:nra-upperbound-known}
          \State lowerBound $\gets$ lowerBound $+ \SumWeight_j \SumArg_j$
            \label{ln:nra-lowerbound-known}
        \EndIf
      \EndFor

      \vspace{0.7em}
      \LineComment{predict words with lower bound $\geq$ all other upper bounds}
      \State queue.\Call{push}{(word, upperBound, lowerBound)}
        \label{ln:nra-queue-push}
      \While{\textbf{true}}
        \label{ln:nra-while-true}
        \State word, upperBound, lowerBound $\gets$ queue.\Call{pop}{$ $}
          \label{ln:nra-queue-pop}
        \If{lowerBound $\geq$ queue.\Call{getLargestUpperBound}{$ $}}
          \label{ln:nra-lowerbound-greater}
          \State predictions.\Call{add}{word}
            \label{ln:nra-certain-predict}
        \Else
          \State queue.\Call{push}{(word, upperBound, lowerBound)}
            \label{ln:nra-queue-readd}
          \State \textbf{break}
            \label{ln:nra-predict-break}
        \EndIf
        \If{predictions.\Call{size}{$ $} $=$ limit}
          \label{ln:nra-isdone}
          \State \textbf{exit}
            \label{ln:nra-exit}
        \EndIf
      \EndWhile
    \EndWhile

    \vspace{0.7em}
    \For{$i$ \textbf{from} predictions.\Call{size}{$ $} \textbf{to} limit}
      \label{ln:nra-for-limit}
      \State word, upperBound, lowerBound $\gets$ queue.\Call{pop}{$ $}
        \label{ln:nra-pop-2}
      \If{upperBound $= 0$}
        \label{ln:nra-upperbound-zero-if}
        \State \textbf{break}
          \label{ln:nra-upperbound-zero-break}
      \EndIf
      \State predictions.\Call{add}{word}
        \label{ln:nra-limit-predict}
    \EndFor
  \end{algorithmic}
\end{algorithm}

\Cref{alg:norandomaccessalgorithm} shows our implementation of the
\emph{No Random Access Algorithm}.

The input requirements are the same as for the \emph{Threshold Algorithm}
described in \cref{sec:thresholdalgorithm}.
The output is similar, however we only return a list of predicted words without
their probabilities, as the algorithm doesn't compute the exact probabilities.
Also the initialization of sum weights
$\SumWeight_1, \: \ldots, \: \SumWeight_N$ and last seen counts
$\overline{\SumArg}_1, \: \ldots, \: \overline{\SumArg}_N$ is the same as
before (\cref{ln:nra-sumweights,ln:nra-bara-for,ln:nra-bara-peek}).

\emph{wordCounts} is the mapping from words $w$ to $N$-tuples that contain the
counts $\SumArg^h_i(w)$ (\cref{ln:nra-wordcounts}).
Unset values are interpreted as having \emph{unkown} counts.

\emph{queue} is a priority queue that stores $($word$,$ upper
bound$,$ lower bound$)$ tuples (\cref{ln:nra-queue}).
As opposed to the \emph{Threshold Algorithm} the queue needs to keep track of
bounds for all encountered words, not just \emph{limit} many.
Entries in the queue are sorted by lower bounds and for objects with equal
lower bounds sorted by their upper bounds.

\emph{predictions} is the list of all possible word predictions
(\cref{ln:nra-predictions}).
Words in the list are sorted by probability descending without actually
calculating the probability.
It contains at most \emph{limit} items.

The selection of the next trie with \Call{nextTrie}{$ $} (\cref{ln:nra-while})
and the following sorted access to that trie (\cref{ln:nra-next}) are equal
to those of the \emph{Threshold Algorithm}.
Each new word-count-pair is stored in \emph{wordCounts}.
If the word has not been seen before (\cref{ln:nra-contains}) we initialize all
counts $\SumArg_1, \: \ldots, \: \SumArg_N$ as \emph{unkown} values.
Otherwise we load previously stored counts from the mapping
(\cref{ln:nra-get-wordcounts}).
Additionally as we know the word to receive updated bounds its entry from the
\emph{queue} (\cref{ln:nra-queue-remove}).
Afterwards we update the one $\SumArg_i$ we have retrieved the value for
(\cref{ln:nra-a-set}) and write the resulting count-tuple back into the map
(\cref{ln:nra-update-mapping}).

With the update counts we can compute a better upper and lower bound for the
word probability.
First we update the last accessed count of the current trie
(\cref{ln:nra-bara-update}).
Bounds are then calculated as the weighted sums of $\SumArg_j$
(\cref{ln:nra-upperbound-known,ln:nra-lowerbound-known}).
In case the the value of any $\SumArg_j$ is \emph{unkown}, it is  substituted
with the highest possible value $\overline{\SumArg}_j$ for the calculation of
the upper bound (\cref{ln:nra-upperbound-unknown}) or in the case of the lower
bound it is not factored in at all.

The newly calculated bounds of the word are then added to the \emph{queue}
(\cref{ln:nra-queue-push}).
Afterwards all words that have a higher lower bound than the largest
upper bound of all other words in the queue are added as final
\emph{predictions}.
First the next word with the highest lower bound is retrieved from the queue
with the \Call{pop}{$ $} operation (\cref{ln:nra-queue-pop}).
If that word's lower bound is greater or equal to the largest remaining upper
bound in the queue (\cref{ln:nra-lowerbound-greater}) we can add it to the list
of \emph{predictions} (\cref{ln:nra-certain-predict}) and repeat the process.
Otherwise we add the word back into the queue and continue with new sorted
retrieval.
The algorithm terminates once \emph{limit} predictions have been found
(\cref{ln:nra-isdone,ln:nra-exit}).
This procedure ensures that predictions are added in order of decreasing
probabilities.

A case than can occur for small corpora, is that all tries may be depleted
before \emph{limit} predictions have been established.
Because of this we fill the \emph{predictions} with words from the \emph{queue}
in order until \emph{limit} many have been reached
(\cref{ln:nra-for-limit,ln:nra-pop-2,ln:nra-upperbound-zero-if,ln:nra-upperbound-zero-break,ln:nra-limit-predict}).
However if remaining words have an probability upper bound of zero
(\cref{ln:nra-upperbound-zero-if}) we prefer to have less than \emph{limit}
predictions, as the remaining ones are guaranteed not to match.

\todo[inline]{NRA implementation buggy because upper bounds for words are not
updated until they are found encountered during sorted access? Maybe read
\textcite{Fagin2001}.}

% ------------------------------------------------------------------------------
\section{Implementation}

\begin{draft}
Not actually $h * p$ but $\hat{h} * p$.

Can't actually store $\SumArg(\DummyArg)$ but only counts $\Count(\DummyArg)$.
Discounts fuck everything up and sometimes require random access even when only
doing sorted.

Faster threshold calucation.

Can't just do $C_i$.\Call{next}{$\DummyArg$}, iterators needed instead.

Optimized \Call{nextTrie} implementation possible with blah...
\end{draft}

% ------------------------------------------------------------------------------
\section{Discussion}

\begin{draft}
Space requirements of algorithms.

NRA can give predictions.

NRA sorting may be bad for small corpora.

Random access may be far more expensive on distributed systems.

Corpus needs to be preanalyzed.
\end{draft}

\begin{draft}
Access strategy:
Mention \textcite{Guentzer2000} and \textcite{Guentzer2001} improvements to TA
and NRA.
\end{draft}

% ------------------------------------------------------------------------------
\section{Alternatives}

\begin{draft}
Why beam search is impossible.

Beam search is a path finding algorithm that always expands the most promising
states, and terminates when a solution was found.

This however is not directly applicable to our problem, as finding a solution
is typically very easy (get top node from sorted pattern counts and take it as
solution).

How to interpret: expanding the most promising state?
\begin{enumerate}
  \item Peek sorted counts, list with highest count is most promising.
  \item Now how to determine if we want to do random access to missing pattern
    counts or rather fetch next sorted count.
  \item Could do beam width many sorted accesses, and complete missing patterns
    with random acccess and call it a day.
\end{enumerate}

For random access the cost of an edge is only known, once the edge is walked.
For sorted access we can peek.
\end{draft}

\begin{lstlisting}[
  label = {lst:topksql},
  float,
  caption = {\todo[inline]{Example SQL-Request Caption}},
  language = SQL,
  %basicstyle = \ttfamily,
  emph = {history1, historyN},
  emphstyle = \textit,
  deletekeywords = {count} % Used as column name, don't want to escape it though
]
  SELECT
    word,
    s(Trie1.alpha, ..., TrieN.alpha) AS probability
  FROM Vocabulary
  LEFT OUTER JOIN Trie1
    ON history1 + word = Trie1.sequence
  ...
  LEFT OUTER JOIN TrieN
    ON historyN + word = TrieN.sequence
  ORDER BY probability DESC
  LIMIT k;
\end{lstlisting}
