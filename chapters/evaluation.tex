\chapter{Evaluation}
\label{ch:evaluation}

We will now evaluate the benefit of the two independent optimizations we
proposed to next word prediction: representing language model probabilities as
weighted sums (\cref{ch:weightedsum}) and calculating the word with the highest
probability via top-$k$ joining (\cref{ch:topkjoin}).
To this end we empirically compare their performance in a variety of usage
scenarios.

This chapter is organized as follows:
first the experimental setup used over all experiments is described in
\cref{sec:experimental-setup}, then our form of data visualization is explained
in \cref{sec:boxplot}, and finally weighted sums are evaluated in
\cref{sec:evaluation-weightedsum} and top-$k$ joining in
\cref{sec:evaluation-topkjoin}.


% ------------------------------------------------------------------------------
\section{Experimental Setup}
\label{sec:experimental-setup}

As explained in \cref{ch:review}, the calculations of the considered language
models' probabilities depend on occurrence counts obtained through statistical
analysis of text corpora.
For this evaluation the \emph{Open American National Corpus (OANC)} by
\textcite{OANC} was used.
It is a free collection of written text and transcripts of spoken data,
featuring both historical and contemporary American English of all genres.
It contains around 600 thousand sentences which amount to 14 million words in
total.

In our experiments only the written part of that corpus was used, because next
word prediction has no application to speaking.
The corpus comes tagged with semantic sentence boundaries which were used to
split the data into sentences.
No begin or end of sentence tags were inserted, as to not falsify the actual
occurrence counts.
Tokenization was performed by assuming word boundaries to occur at each space.
To avoid punctuation marks directly following words being counted as only one
word, all special characters were surrounded by spaces.
This means for our analysis, punctuation marks count as regular words.
This has the downside of text like \texttt{you're} becoming \texttt{you ' re}.
Other than that, the data was left as is.
In particular no case normalization, stemming or removing of names was
performed.

That text was then randomly split into 80\% training and 20\% heldout data.
To measure how the algorithms under examination behaved for different sizes
of training data, we successively discarded 50\% of sentences from training.
Through this we obtained five datasets equaling 100\%/50\%/25\%/12.5\%/6.25\%
of the original training corpus size.
\Cref{tab:evaluation-trainings} shows the different datasets.
Distinct language models were then learned on each of these training sets.

\begin{table}
  \hspace{-0.35cm}
  \begin{tabular}{lrrr}
    \toprule
    Corpus & No.\ Sentences & No.\ Unique Words & Raw File Size \\
    \midrule
    \texttt{OANC}            & \num{608277} & \num{196345} & \SI{71.0}{\mebi\byte} \\
    \texttt{Training-100\%}  & \num{486409} & \num{178671} & \SI{57.0}{\mebi\byte} \\
    \texttt{Training-50\%}   & \num{243204} & \num{133038} & \SI{29.0}{\mebi\byte} \\
    \texttt{Training-25\%}   & \num{121602} & \num{ 98209} & \SI{15.0}{\mebi\byte} \\
    \texttt{Training-12.5\%} & \num{ 60801} & \num{ 70988} & \SI{ 7.1}{\mebi\byte} \\
    \texttt{Training-6.25\%} & \num{ 30400} & \num{ 50372} & \SI{ 3.6}{\mebi\byte} \\
    \bottomrule
  \end{tabular}
  \caption{Overview of the different datasets used for training.}
  \label{tab:evaluation-trainings}
\end{table}

From the heldout data \num{40000} non-overlapping sequences of ten words were
selected.
For our experiments all but the last words of that sequence were considered to
form the history, and the last word then is the word for which either a
probability is calculated, or which is expected to be predicted by next word prediction.
For cases were shorter histories of words were needed, words were removed from
the front of the sequences, so the words whose probability are calculated
respectively which are predicted remain the same.
To avoid the problem of how to handle unknown words in the evaluation, these
sequences were chosen as to not contain any words, that do not occur in
the smallest 6.25\% training corpus.
Because of this no pruning of words had to be performed, and an \texttt{UNK}
token was not assigned an artificially occurrence count.

The evaluation was performed using the \emph{Generalized Language Modeling
Toolkit (GLMTK)}\footnote{The \emph{Generalized Language Modeling Toolkit
(GLMTK)} is freely available under
\mbox{\url{https://github.com/renepickhardt/generalized-language-modeling-toolkit/}}.},
a freely available toolkit in Java to compare different language modeling
techniques, that I wrote as part of my work as a student research assistant.
Initially it was able to perform the traditional, recursive calculations for
both Modified Kneser-Ney Smoothing and the Generalized Language Model.
For this thesis I implemented next word prediction using all optimizations
described in \cref{ch:weightedsum,ch:topkjoin} as well as all experiments from
this chapter.
\todo{Tag bachelor commit and mention in footnote.}

All experiments were run on a virtual machine that was assigned eight
\SI{3.0}{\giga\hertz} processor cores, each with a cache size of
\SI{4096}{\kibi\byte}.
Although multiple cores were available, all experiments were run in a single
thread, one after another, to avoid thread communication influencing the
benchmark.
We used Linux (3.2.0-75-virtual) as the operating system, and
the OpenJDK with java version 1.7.0\_79 as the implementation of the
Java Platform.
The Java Virtual Machine (JVM) was assigned \SI{16}{\gibi\byte} of main memory,
a limit that none of the experiments hit.

Java Virtual Machines are know to exhibit \enquote{warm-up} behavior \noref.
That is it takes some time while running a program to find hotspot sections
in the code and JIT compile these.
Traditionally experimenters want to avoid that benchmarks are distorted by
this warm-up behavior \noref.
To circumvent this, the code under benchmark is usually executed multiple times,
to ensure JIT compilation of the relevant sections, and the actual benchmark
is only performed afterwards.
To this end, we decided to run each experiment twice:
the first execution is for warming-up the Java Virtual Machine, while actual
tracking of performance metrics only occurs in the second run.
As each experiment consists of executing \num{40000} test sequences, we are
confident that the warm-up is completed before the second execution.


% ------------------------------------------------------------------------------
\section{Data Visualization: Box Plots}
\label{sec:boxplot}

Because of the large sample size of \num{40000} testing sequences, showing
all measured data points in diagrams is unfeasible.
Instead statistical characteristics of the measured distributions of data
have to be displayed.
In our experiments, it turned out that finding fitting statistical
characteristics is non-trivial.
Natural choices for this are the mean and the standard deviation of
distributions.
However these are only suited to accurately describe measurements that are
normal distributed, and are not suitable for our needs because of reasons:
\begin{itemize}
  \item Some measured distributions exhibited a non-negligible amount of
    \emph{right-skewness}.
    In other words the mass of the distribution is concentrated on the left.
    This case occurred when measuring the runtime of algorithms, and can be
    explained by increased communication of the operation system during the
    calculation of a tiny part of the test sequences.
    Such outliers should not unduly affect the statistical characteristics,
    which is not true for the mean and the standard deviation.
    On top of this the standard deviation is only capable of showcasing
    symmetric dispersion, so its use would be misleading for skewed
    distributions.
  \item Another observation is, that a few distributions are \emph{multimodal}.
    That is multiple distinct peaks are visible in a density histogram of that
    distribution.
    Again this mostly occurred when measuring the runtime of algorithms,
    and can be explained by the fact that the \num{40000} test sequences can
    be partitioned into several classes of different computational costs.
    For example calculating probabilities is vastly different depending on
    how many words of the history were seen in the training corpus.
    Both the mean and the standard deviation do not accurately describe
    multimodal distributions.
\end{itemize}

Instead, more \emph{robust}, \emph{non-parametric} statistical characteristics
have to be used, that do not impose assumptions about the underlying
measured distributions.
One common form of visualizing data that fits these requirements is the use of
\emph{box plots}.
Box plots visually describe the measured distribution through the combination of
five characteristics.
We follow the convention of \textcite{Tukey1977}:
\begin{itemize}
  \item As the name suggests the defining feature of box plots is their use of
    boxes.
    The lower/upper bar of the box gives the location of the first
    respectively third \emph{quartile} of the data.
    So 50\% of data measuring points fall into the plot's box.
  \item The band inside the box gives the location of the \emph{median}.
    By its position relative to the quartile it is possible to visually estimate
    the skewness of the distribution.
  \item The box is surrounded by whiskers, who reach to the smallest/largest
    datum in the sample.
    But their length is limited to at most $1.5$ \emph{inter quartile ranges}.
    Every measuring points outside of that range is considered to be an outlier.
    Outliers are not shown in the plot.
\end{itemize}

% ------------------------------------------------------------------------------
\section{Probability Calculation through Weighted Sums}
\label{sec:evaluation-weightedsum}

We now want to evaluate how our method of calculating language model
probabilities through weighted sums
\mbox{$\Prob{w}{h} = \sum_{i = 1}^{N} \SumWeight^h_i \cdot \SumArg_i^h(w)$},
described in \cref{ch:weightedsum}, fares against the traditional, recursive
approach.
The metric we will measure is \emph{calculation time}.
Comparison from a space complexity point of view is unnecessary, as both
approaches only require a data structure that allows obtaining the occurrence
counts of arbitrary sequences\footnote{One might argue that the weighted sum
method also needs to store the sum weights~$\SumWeight^h_i$. But as we only
calculate these values at query time for a specific query, only $N$ weights have
to be stored per query. As explained in \cref{ch:weightedsum} $N$ is usually a
very small number. For example calculating probabilities under a 5th order
Markov assumption --- that is the length of the history $h$ is capped to 4 words
--- requires storing at most 5 (MKN, \cref{eq:weightedsum-mkn-num}) respectively
32 (GLM, \cref{eq:weightedsum-glm-num}) floating point numbers.
Such a small requirement of memory can be neglected}.

Our first experiment aims to compare the calculation time of one single
probability for both approaches and both considered language models, independent
of any application.
To this end we tracks the calculation time that is necessary to calculate
the conditional probability $\Prob{w}{h}$ of test sequences.
The last word of a test sequence gives the word~$w$ for which the probability is
calculated, while the first to the second to last words of the sequence form
the conditional history~$h$.
Probability calculation time depends mostly on the order of the employed Markov
assumption, as this directly determines the length of the considered history
and thus the recursion depth of the calculation.
In the experiment we therefore track the calculation time for different
$n$-gram lengths of the the sequence under testing.
Lower length sequences are derived from higher length ones by removing the
word at the beginning, so that the words $s$ under prediction remains the same
over all $n$-gram lengths.
The results are shown in \cref{fig:evaluation-probtime}.

\begin{figure}
  \begin{center}
    Calculation Time per Probability $\Prob{w}{h}$
  \end{center}

  \hspace{-0.21\textwidth}
  \makebox[1.42\textwidth][c]{
    \begin{subfigure}{.7\textwidth}
      \raggedleft
      \input{figures/thesis-time-mkn}
      \subcaption{}
      \label{fig:evaluation-probtime-mkn}
    \end{subfigure}
    \hspace{.02\textwidth}
    \begin{subfigure}{.7\textwidth}
      \raggedright
      \input{figures/thesis-time-glm}
      \subcaption{}
      \label{fig:evaluation-probtime-glm}
    \end{subfigure}
  }
  \caption{Time to calculate one probability for different $n$-gram lengths
    comparing the traditional, recursive approach (\cref{ch:review}) as well as
    our novel weighted-sum approach (\cref{ch:weightedsum}) to calculate both
    Modified Kneser-Ney Smoothing (MKN) and the Generalized Language Model
    (GLM).}
  \label{fig:evaluation-probtime}
\end{figure}

In \cref{fig:evaluation-probtime-mkn} for Modified Kneser-Ney Smoothing we
observe a linear or better relation between $n$-gram-length and calculation
time.
In the range of $1$- to $4$-grams the calculation time of the traditional,
recursive approach is slightly lower, while from $5$-grams onwards the
weighted-sum method comes of a bit faster.
Without measuring any kind of significance, our conclusion from this figure is
that both approaches perform probability calculation in approximately the same
time.
This can be explained by the fact that both techniques calculate results in
exactly the same manner.
The weighted-sum approach does not lead to any reduction in complexity, as it
moves the recursive calculations into the generation of weights.

\Cref{fig:evaluation-probtime-glm} gives the same experiment for the Generalized
Language Model.
Because of the log-scale on the y-axis we conclude a clear exponential relation
between $n$-grams-length and calculation time for both approaches,
Again for short $n$-grams lengths ($1$- to $3$-grams) the traditional, recursive
approach is slightly faster, while for longer $n$-grams ($4$-grams and onwards)
the weighted-sum method gets increasingly better.
A reduction in algorithmic complexity did not take place, as the relation is
still exponential, but the weighted-sum method seems to be performing its
calculations in a much more efficient manner.
For $5$-grams, the most widely used value in practice
\parencite{JurafskyMartin2009,Goodman2001,Stolcke2000}, we observe that the
weighted-sum approach is about double as fast.

But the main advantage of the weighted sum approach lies not in the fact
that the calculation for a single, arbitrary probability is faster.
Instead in the application of next word prediction, it allows to calculate the
sum weights $\SumWeight^h_i$ only once in a pre-computation step of each query.
This is possible because for a next word prediction query, all required
probabilities depend on a constant history $h$, as explain in
\cref{ch:weightedsum}.

\begin{figure}
  \centering
  Relative Weight $\SumWeight^h_i$ Calculation Time per Probability
  \\[1.5ex]
  \begin{minipage}{0.7\textwidth}
    \centering
    \input{figures/thesis-weight-times}
  \end{minipage}
  \caption{Proportion of the calculation time per probability that is used
    for the calculation of the weights $\SumWeight^h_i$.
    Given for the weighted-sum approach of both Modified Kneser-Ney Smoothing
    (MKN) and the Generalized Language Model (GLM).
    A value of \num{0.6} means that 60\% of time
    was used to calculate the weights and the remaining 40\% of time was used
    to calculate the final probability using these weights.
    Shown is the mean of the measured values with the errors bars displaying the
    standard deviation.}
  \label{fig:evaluation-weight-times}
\end{figure}

An interesting question is now: how much time of each probability calculation
is actually used to calculate the weights, and can therefore be saved by
off-loading its calculation to a pre-computation step?
\Cref{fig:evaluation-weight-times} shows this proportion for the previous
experiment.
We can see that the amount of saved work increases with $n$-gram length.
For the interesting value of $5$-grams we note a work load reduction from
about 55\% for Modified Kneser-Ney Smoothing to 70\% for the Generalized
Language Model with an application like next word prediction.

We conclude that there is no downside to the weighted-sum approach.
For Modified Kneser-Ney Smoothing it is able to calculate probabilities in
approximately the same time, while it is about twice as fast for the
Generalized Language Model.
Additionally, in the application of next word prediction,  it to further reduce
the calculation time by around 55\% respectively 70\% depending on choice
of language model.

\begin{draft}
For different percentages of seen sequences?
Number of weights per model.
\end{draft}

% ------------------------------------------------------------------------------
\section{Next Word Prediction}
\label{sec:evaluation-topkjoin}

% - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -
\subsection{Prediction Quality}
\label{subsec:evaluation-topkjoin-quality}

First we will compare the achieved prediction quality of the employed
language models.
Note that for this experiment the choice of underlying algorithm does not
matter.
The Simple Argmax, the Threshold Algorithm, and the No-Random Access Algorithm
always predict the same words, they just compute these results in different
ways.

A standard metric to evaluate the quality of a word prediction system is the
degree of \emph{normalized keystroke savings}
\parencite{Swiffin1987,Bickel2005,Trnka2011}.
They measure the percentage of keystrokes that can be omitted by using next
word prediction compared to letter-by-letter typing.
Normalized keystroke savings (NKSS) are calculate as \parencite{Trnka2011}:
\begin{equation}
  \text{NKSS} = \frac{\text{keystrokes}_\text{normal} - \text{keystrokes}_\text{with prediction}}
                     {\text{keystrokes}_\text{normal}}
\end{equation}

In the context of next word prediction we are only concerned with the keystroke
savings of entering single, independent words.
For our purpose $\text{keystrokes}_\text{normal}$ thus is the number of
characters of a word, while $\text{keystrokes}_\text{with prediction}$ is
the number of characters of a word one has to type until the system suggests
the indented word as a completion.
This however is not a clear definition: most next word prediction systems do not
only offer a single prediction after each keystroke, but a list of $k$
predictions from which the user can choose.
Measurements of normalized keystroke savings are thus inherently depend on the
choice of $k$.

There also is no clear consensus on how keystrokes are counted
\parencite{Trnka2011}.
For this thesis, we count each character as a single keystroke, independent of
case or accessibility from a normal keyboard.
Word separating characters like spaces or newlines are not considered in our
analysis, as they do not matter for next word prediction.
On top of that we did not count the process of selecting the indented word from
the list of predictions as a keystroke to enable keystroke savings to lie in the
complete range from zero to one.

\begin{table}
  \centering
  \begin{tabular}{ccc}
    \toprule
    & \multicolumn{2}{c}{Normalized Keystroke Savings} \\
    \cmidrule{2-3}
    n-gram length & \hspace{0.8cm}MKN\hspace{0.8cm} & \hspace{0.8cm}GLM\hspace{0.8cm} \\
    \midrule
    2 & 0.44 & 0.44 \\
    3 & 0.50 & 0.50 \\
    4 & 0.51 & 0.51 \\
    5 & 0.51 & 0.52 \\
    \bottomrule
  \end{tabular}
  \caption{Comparison of normalized keystroke savings for Modified Kneser-Ney
    Smoothing (MKN) and the Generalized Language Model (GLM) over varying
    $n$-gram lengths.
    A prediction was only considered valid if the intended word occurred as
    the top-$1$ prediction.}
  \label{tab:evaluation-nkss}
\end{table}

\Cref{tab:evaluation-nkss} shows our measurement of normalized keystroke
savings with the testing sequences for both considered language models.
As is apparent, the choice between Modified Kneser-Ney Smoothing and the
Generalized Language Model is largely irrelevant from a keystroke savings point
of view, at least for our experimental setup: for two significant  digits, all
values match, with the exception of the last row, which only differs slightly.
The Generalized Language Model however comes with severely higher costs in both
required space and runtime as we will see in
\cref{subsec:evaluation-topkjoin-space,subsec:evaluation-topkjoin-time}.

% - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -
\clearpage
\subsection{Space Requirements}
\label{subsec:evaluation-topkjoin-space}

Next we will measure the memory requirements of our next word prediction
system.
Again the choice of algorithm to calculate next word prediction is irrelevant.
For all algorithms we use the same data structure to store occurrence counts of
text corpora: the completion trie described in \cref{sec:completiontrie}.
This data structure is optimized for compact storage and supports all required
access types.
However the amount of necessary data varies with the employed language model.
As the Generalized Language Model also needs to keep track of the occurrence
of skipped $n$-grams, we expect it to always require more memory.

\begin{figure}
  \centering
  Completion Trie Size for $5$-gram Prediction
  \\[1.5ex]
  \begin{minipage}{0.7\textwidth}
    \centering
    \input{figures/thesis-trie-size}
  \end{minipage}
  \caption{Memory sizes of the completion tries used to store the occurrence
    counts that are required to compute $5$-gram next word prediction.
    Distinguished for Modified Kneser-Ney Smoothing (MKN) and the Generalized
    Language Model (GLM).}
  \label{fig:evaluation-trie-size}
\end{figure}

\Cref{fig:evaluation-trie-size} shows the necessary memory for each different
corpus size and each language model.
In the plot all measurement points of one language model technique lie on one
pretty clear, straight line.
Note that both axis feature logarithmic scales.
Thus the relation between the number of sentences in a corpus and the required
memory space is monomial in practice.
As the slope of the observed line is lesser than one, the power of the monomial
also has to be lass than one.
We thus conclude that the relation between increasing corpus size to the
required memory is better than linear.

For each measured corpus size, the Generalized Language Model takes roughly
six times the amount of memory.
\todo{Explainable by six times as many patterns?}
Together with the fact that its prediction quality is only very slightly better,
there is no reason for its use over Modified Kneser-Ney Smoothing.
This argument will be reinforced by comparing their runtimes.

% - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -
\clearpage
\subsection{Runtime Analysis}
\label{subsec:evaluation-topkjoin-time}

Finally we want to compare the presented next word prediction algorithms by
their runtime, as fast next word prediction algorithms were the main objective
of this thesis.
In this thesis we described three different solutions:
\begin{inparaenum}[(1)]
  \item the Simple Argmax algorithm, which calculates the probability of all
    words in a vocabulary and then selects the ones with the highest
    probabilities,
  \item the Threshold Algorithm, known from the field of top-$k$ joining,
    which utilizes sorted and random access to a sorted list of occurrence
    counts to find predictions faster, and
  \item the No-Random-Access Algorithm which works similarly but only requires
    sorted access.
\end{inparaenum}

We studied the behavior of these algorithms under two different language models
in a wide variety of settings.
We measured the runtime of each algorithm
\begin{inparaenum}[(1)]
  \item applied to Modified Kneser-Ney Smoothing and the Generalized Language
    Model,
  \item with occurrence counts from five differently sized training corpora,
  \item for $n$-gram lengths from two to five,
  \item predicting one to five words.
\end{inparaenum}
\Cref{fig:evaluation-time-nwp-mkn-corpora,fig:evaluation-time-nwp-mkn-ngram,%
  fig:evaluation-time-nwp-mkn-predictions,fig:evaluation-time-nwp-glm-corpora,%
  fig:evaluation-time-nwp-glm-ngram,fig:evaluation-time-nwp-glm-predictions}
show selected results from these experiments.

\begin{figure}[p!]
  \vspace{-0.4cm}
  \captionsetup[subfigure]{aboveskip=0cm, belowskip=-0.4cm}

  \small
  \begin{center}
    Calculation Time per top-$1$ Prediction using $5$-grams
  \end{center}

  \vspace{-0.2cm}
  \hspace{-0.16\textwidth}
  \makebox[1.32\textwidth][c]{
    \begin{subfigure}{.65\textwidth}
      \vspace{-0.25cm}
      \raggedleft
      \input{figures/thesis-time-nwp-mkn-corpora-smpl}
      \subcaption{}
      \label{fig:evaluation-time-nwp-mkn-corpora-smpl}
    \end{subfigure}
    \hspace{.02\textwidth}
    \begin{subfigure}{.65\textwidth}
      \raggedright
      \input{figures/thesis-time-nwp-mkn-corpora-ta-nra}
      \subcaption{}
      \label{fig:evaluation-time-nwp-mkn-corpora-ta-nra}
    \end{subfigure}
  }
  \normalsize
  \caption{Time to calculate one top-$1$ word prediction query using
    $5$-grams over varying training corpus sizes for Modified Kneser-Ney
    Smoothing.}
  \label{fig:evaluation-time-nwp-mkn-corpora}

  \small
  \vspace{-3ex}
  \begin{center}
    \mbox{Calculation Time per top-$1$ Prediction on Corpus with \num{179000} Unique Words}
  \end{center}

  \vspace{-0.2cm}
  \hspace{-0.16\textwidth}
  \makebox[1.32\textwidth][c]{
    \begin{subfigure}{.65\textwidth}
      \raggedleft
      \input{figures/thesis-time-nwp-mkn-ngram-smpl}
      \subcaption{}
      \label{fig:evaluation-time-nwp-mkn-ngram-smpl}
    \end{subfigure}
    \hspace{.02\textwidth}
    \begin{subfigure}{.65\textwidth}
      \raggedright
      \input{figures/thesis-time-nwp-mkn-ngram-ta-nra}
      \subcaption{}
      \label{fig:evaluation-time-nwp-mkn-ngram-ta-nra}
    \end{subfigure}
  }
  \normalsize
  \caption{Time to calculate one top-$1$ word prediction query with language
    model $n$-gram length varied from two to five for Modified Kneser-Ney
    Smoothing. Calculated on the \texttt{Training-100\%} corpus.}
  \label{fig:evaluation-time-nwp-mkn-ngram}

  \small
  \vspace{-3ex}
  \begin{center}
    \mbox{\hspace{-0.8cm}Calculation Time per Prediction using $5$-grams on Corpus with \num{179000} Unique Words}
  \end{center}

  \vspace{-0.2cm}
  \hspace{-0.16\textwidth}
  \makebox[1.32\textwidth][c]{
    \begin{subfigure}{.65\textwidth}
      \vspace{-0.25cm}
      \raggedleft
      \input{figures/thesis-time-nwp-mkn-predictions-smpl}
      \subcaption{}
      \label{fig:evaluation-time-nwp-mkn-predictions-smpl}
    \end{subfigure}
    \hspace{.02\textwidth}
    \begin{subfigure}{.65\textwidth}
      \raggedright
      \input{figures/thesis-time-nwp-mkn-predictions-ta-nra}
      \subcaption{}
      \label{fig:evaluation-time-nwp-mkn-predictions-ta-nra}
    \end{subfigure}
  }
  \normalsize
  \caption{Time to calculate one top-$k$ word prediction query using $5$-grams
    for Modified Kneser-Ney Smoothing. Calculated on the
    \texttt{Training-100\%} corpus}
  \label{fig:evaluation-time-nwp-mkn-predictions}
\end{figure}

\begin{figure}[p!]
  \vspace{-0.4cm}
  \captionsetup[subfigure]{aboveskip=0cm, belowskip=-0.4cm}

  \small
  \begin{center}
    Calculation Time per top-$1$ Prediction using $5$-grams
  \end{center}

  \vspace{-0.2cm}
  \hspace{-0.16\textwidth}
  \makebox[1.32\textwidth][c]{
    \begin{subfigure}{.65\textwidth}
      \raggedleft
      \input{figures/thesis-time-nwp-glm-corpora-smpl}
      \subcaption{}
      \label{fig:evaluation-time-nwp-glm-corpora-smpl}
    \end{subfigure}
    \hspace{.02\textwidth}
    \begin{subfigure}{.65\textwidth}
      \raggedright
      \input{figures/thesis-time-nwp-glm-corpora-ta-nra}
      \subcaption{}
      \label{fig:evaluation-time-nwp-glm-corpora-ta-nra}
    \end{subfigure}
  }
  \normalsize
  \caption{Time to calculate one top-$1$ word prediction query using
    $5$-grams over varying training corpus sizes for the Generalized Language
    Model.}
  \label{fig:evaluation-time-nwp-glm-corpora}

  \small
  \vspace{-3ex}
  \begin{center}
    \mbox{Calculation Time per top-$1$ Prediction on Corpus with \num{179000} Unique Words}
  \end{center}

  \vspace{-0.2cm}
  \hspace{-0.16\textwidth}
  \makebox[1.32\textwidth][c]{
    \begin{subfigure}{.65\textwidth}
      \raggedleft
      \input{figures/thesis-time-nwp-glm-ngram-smpl}
      \subcaption{}
      \label{fig:evaluation-time-nwp-glm-ngram-smpl}
    \end{subfigure}
    \hspace{.02\textwidth}
    \begin{subfigure}{.65\textwidth}
      \raggedright
      \input{figures/thesis-time-nwp-glm-ngram-ta-nra}
      \subcaption{}
      \label{fig:evaluation-time-nwp-glm-ngram-ta-nra}
    \end{subfigure}
  }
  \normalsize
  \caption{Time to calculate one top-$1$ word prediction query with language
    model $n$-gram length varied from two to five for the Generalized Language
    Model. Calculated on the \texttt{Training-100\%} corpus.}
  \label{fig:evaluation-time-nwp-glm-ngram}

  \small
  \vspace{-3ex}
  \begin{center}
    \mbox{\hspace{-0.8cm}Calculation Time per Prediction using $5$-grams on Corpus with \num{179000} Unique Words}
  \end{center}

  \vspace{-0.2cm}
  \hspace{-0.16\textwidth}
  \makebox[1.32\textwidth][c]{
    \begin{subfigure}{.65\textwidth}
      \raggedleft
      \input{figures/thesis-time-nwp-glm-predictions-smpl}
      \subcaption{}
      \label{fig:evaluation-time-nwp-glm-predictions-smpl}
    \end{subfigure}
    \hspace{.02\textwidth}
    \begin{subfigure}{.65\textwidth}
      \vspace{-0.25cm}
      \raggedright
      \input{figures/thesis-time-nwp-glm-predictions-ta-nra}
      \subcaption{}
      \label{fig:evaluation-time-nwp-glm-predictions-ta-nra}
    \end{subfigure}
  }
  \normalsize
  \caption{Time to calculate one top-$k$ word prediction query using $5$-grams
    for the Generalized Language Model. Calculated on the
    \texttt{Training-100\%} corpus}
  \label{fig:evaluation-time-nwp-glm-predictions}
\end{figure}

In \cref{fig:evaluation-time-nwp-mkn-corpora,fig:evaluation-time-nwp-glm-corpora}
the relation between the number of unique words in a corpus and the runtime of
each algorithms is shown.
A linear relation can be observed for measurements of the Simple Argmax
algorithm.
This can easily be explained by the fact, that it calculate one probability for
each unique word.
The top-$k$ joining algorithms seem to grow with the logarithm of the number of
unique words.
However no clear statement can be made from these measurements, the relation
could also be linear in practice, and the seemingly logarithmic relation could
only be the result of variance.
On the other hand a logarithmic relation would be well explainable, because
top-$k$ algorithms precisely try to avoid enumerating all words.
More importantly though, the top-$k$ joining algorithms are able to calculate
the results multiple orders of magnitude faster for both language models.
Calculations that ranged in the order of seconds were speed up to
sub-millisecond performance.

\Cref{fig:evaluation-time-nwp-mkn-ngram,fig:evaluation-time-nwp-glm-ngram}
attest similar runtime-improvements.
They show the connection between employed $n$-gram length and calculation time.
For Modified Kneser-Ney Smoothing a linear relation is observed, while that for
the Generalized Language Model is exponential.
It is clear that the relevant factor is calculation time of probabilities, as
\cref{fig:evaluation-probtime} has shown identical correlations between $n$-gram
lengths and calculation time.
Together with \cref{tab:evaluation-nkss} this figure gives a good overview of
possible runtime / prediction quality trade-offs.
\todo{Does it make sense to take measurement of unique words vs trie depth?}

Lastly \cref{fig:evaluation-time-nwp-mkn-predictions,%
fig:evaluation-time-nwp-glm-predictions} show how these algorithms behave
when more than one word is to predicted.
Obviously for the Simple Argmax Algorithm the time remains constant, as it
calculates probabilities for all words anyhow.
Keeping track of the $k$ highest probabilities can be considered a no-op versus
actually calculating the probabilities.
On the other hand, the top-$k$ joining algorithms correlate linearly with
the number of words to predict.
The No Random Access seems to be performing significantly worse; the linear
relation is almost impossible to make out in the plot for the Threshold
Algorithm.

Over all experiments a clear improvement in runtime through top-$k$ join
algorithms is observable.
At large the calculations using Modified Kneser-Ney Smoothing were around ten
times faster than those of the Generalized Language Model.
Additionally the Threshold Algorithm was able to outperform the No Random Access
Algorithm in all but the most niche measurement\footnote{The only measuring
point for which the No Random Access Algorithm was faster were for $2$-grams.
However $2$-grams are not used in any application in practice because they yield
comparably bad prediction quality (\cref{tab:evaluation-nkss}).}.

To quantify that last point we will perform a comparison of both top-$k$ join
algorithms.
To this end we summarize the previous experiments by dividing the
No-Random-Access-Algorithm-runtimes through the Threshold-Algorithm-runtimes
for each performed query.
\Cref{fig:evaluation-comp-nra-ta} shows a histogram of these ratios.
Note the logarithmic x-axis and the thus increasing bin sizes.
By the mean we could say that the Threshold Algorithm is fifteen times faster
for an average query.
However the mean might not be a good measure because the data is not normal
distributed.
But even for a robust measure like the median the Threshold Algorithm is about
one and a half times faster.
We thus conclude that for our experiment setup and data structure access costs
the Threshold Algorithm significantly outperforms.
One should only use the No-Random-Access Algorithm because of structural
inabilities to support random access.
Note that this result is not surprising and directly follows from data theory.

\begin{figure}[p!]
  \centering
  Comparison of NWP with TA and NRA over varying\\
  $n$-grams lengths, top-$k$ predictions and corpus sizes
  \\[1.5ex]
  \begin{minipage}{1.4\textwidth}
    \centering
    \hspace{-0.2\textwidth}
    \input{figures/thesis-comp-nra-ta}
  \end{minipage}
  \caption{Histogram showing the ratio of runtime of the No Random Access
    Algorithm over the Threshold Algorithms for all performed experiments.
    These experiments were next word prediction queries for Modified Kneser-Ney
    Smoothing with $n$-gram length from two to five, number of predictions $k$
    from one to five and five increasing corpus sizes.
    The area in red gives the cases were the No-Random-Access algorithm was
    faster, while the blue area shows the ones were it was slower.}
  \label{fig:evaluation-comp-nra-ta}
\end{figure}

\todo[inline]{Missing: space requirements of algorithms}
\todo[inline]{Missing: experiments with prefix}
