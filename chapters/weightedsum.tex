\chapter{Formulating Language Models as Weighted Sums}
\label{ch:weightedsum}

\tikzset{
  state/.style = {draw, circle, align = center, text centered, text width = 4.2em},
  invis/.style = {text width = 4.2em},
  order/.style = {align = center, text centered, text width = 3em, text = Maroon},
}

This chapter will show how to represent  $\ProbMKN{w_n}{w_1^{n-1}}$ and
$\ProbGLM{w_n}{w_1^{n-1}}$ as weighted sums of terms depending on $w_n$ for a
fixed history $h = w_1^{n-1}$ and an  arbitrary argument $w = w_n$.
In other words we will express the formulas given in \cref{ch:review-lm} as
equations of the following form:
\begin{equation}
  \Prob{w}{h} = \sum_{i = 1}^{N} \SumWeight_i^h \cdot \SumArg_i^h(w_n)
\end{equation}

Using this it is possible to calculate queries of type ``Noisy Channel Argmax''
(\cref{eq:noisychannel}) much more efficiently, as it is possible to perform the
(potentially expensive) calculation of $\SumWeight_i^h$ in advance.
To compute every $\Prob{w}{h}$ we now just have to calculate the
$\SumArg_i^h(w)$.

Another benefit of this representation is that it is a prerequisite of Top-$k$
joining to supply a monotone scoring function, which only depends on the
joined arguments.
Weighted sums are trivially monotone \todo{really?} and we will be able to
select our join arguments to exactly match $\SumArg_i^h$.
In addition further optimized top-$k$ joining algorithms exists, that require
weighted sum scoring functions.
However we will not use these algorithms in this work.
For a detailed discussion see \cref{ch:top-k-joining}.

In the case of Generalized Language Models our solution will also have a
considerably better computational complexity to calculate probabilities than the
na{\"\i}ve approach of just computing the formulas given in
\cref{sec:review-lm-glm}.
\begin{draft}
This will counter biggest disadvantage of GLM.
\end{draft}

We will now present the basic idea of how to transform $\Prob{w_n}{w_1^{n-1}}$
into weighted sums, that applies to both Modified Kneser-Ney Smoothing and
the Generalized Language Model as well:

Note that in all \cref{eq:mkn-high,eq:mkn-low,eq:glm-high,eq:glm-low} the
probability event $w_n$ only appears in the argument terms of
$\DiscountedCount$ or $\DiscountedCount*$.
Additionally $w_n$ occurs as arguments for $\Count$ and $\ContCountIp$ in the
lowest order \cref{eq:mkn-lowest,eq:glm-lowest}.

Our general idea is thus, to first expand recursive calls to
$\Prob(\DummyArg)$ or $\Prob*(\DummyArg)$, and second to factor
out those terms that do not depend on $w_n$ by the distributive property.
Examples of this idea are given in \cref{app:expansion}.
However they assume the history to be seen in training.
Forfeiting this assumption will add considerable complexity in the case
of Generalized Language Models.

% ------------------------------------------------------------------------------
\clearpage %TODO
\section{Modified Kneser-Ney Smoothing}

Modified Kneser-Ney Smoothed probabilities $\ProbMKN{w}{h}$ are calculated as
follows:
\begin{enumerate}
  \item Backing-off: Leave out words at the beginning of the history $h$, until
    $h \Skp$ is seen for the first time (\cref{eq:mkn-backoff}).
  \item Highest order: Take frequency counts $\DiscountedCount(h \: w)$ and
    $\Count(h \Skp)$ (\cref{eq:mkn-high}) and shorten the history by one word.
  \item Lower orders: Take continuation counts $\DiscountedCount*(\WSkp h \: w)$
    and $\ContCountIp(\WSkp h \WSkp)$ (\cref{eq:mkn-low}) and shorten the
    history further until it is empty.
  \item Lowest order: Take continuation counts $\ContCountIp(\WSkp w)$ and
    $\ContCountIp(\WSkp \WSkp)$ (\cref{eq:mkn-lowest}).
\end{enumerate}
All counts except those of the lowest order are discounted using
$\DiscountedCount$ and $\DiscountedCount*$ to interpolate that order with the
following by the weight $\gamma(h)$.
This process of interpolation different orders is visualized in
\cref{fig:history-mkn}.

\begin{figure}
  \centering
  \begin{tikzpicture}
    \begin{scope}[node distance = 7em]
      \node [state] (highest)                    {$w_1 \: w_2 \: w_3$};
      \node [state] (lower1)  [below of=highest] {$w_2 \: w_3$};
      \node [state] (lower2)  [below of=lower1]  {$w_3$};
      \node [state] (lowest)  [below of=lower2]  {$\varnothing$};
    \end{scope}

    \begin{scope}[node distance = 5.5em]
      \node [order] [right of=highest] {$\ProbMKN {w_4}{w_1 w_2 w_3}$};
      \node [order] [right of=lower1]  {$\ProbMKN*{w_4}{w_2 w_3}$};
      \node [order] [right of=lower2]  {$\ProbMKN*{w_4}{w_3}$};
      \node [order] [right of=lowest]  {$\ProbMKN*{w_4}$};
    \end{scope}

    \begin{scope}[node distance = 6em]
      \node [order] [left of=highest] {Highest order};
      \node [order] [left of=lower1]  {1st Lower order};
      \node [order] [left of=lower2]  {2nd Lower order};
      \node [order] [left of=lowest]  {Lowest order};
    \end{scope}

    \path[->, >=latex, thick]
      (highest) edge node [right, order] {$\textstyle{\gamma(w_1 w_2 w_3)}$} (lower1)
      (lower1)  edge node [right, order] {$\textstyle{\gamma(w_2 w_3)}$}     (lower2)
      (lower2)  edge node [right, order] {$\textstyle{\gamma(w_3)}$}         (lowest);
  \end{tikzpicture}
  \caption{
    Interpolation of different orders during computation of
    $\ProbMKN{w_4}{w_1 w_2 w_3}$.
    Centered are the histories for each probability.
    It is assumed that the history $w_1 w_2 w_3$ was seen during training, so
    that no backing-off is necessary.
    Orders are combined with with interpolation weights $\gamma(\DummyArg)$.
  }
  \label{fig:history-mkn}
\end{figure}

The first step in expressing $\ProbMKN{w}{h}$ as a weighted sum is finding the
number $N$ of sum weights.
In the case of Modified Kneser-Ney Smoothing this is exactly the number of
interpolation orders.
Let $\SeenHistory$ be the first seen history that may be the result of backing
off given history $h$ a number of times.
The number $N$ of sum weights is then the number of words in the first seen
history $\SeenHistory$ plus one.
\begin{equation}
  N = \StringLength{\SeenHistory} + 1
\end{equation}

The next step is to find the actual terms $\SumArg_i^h(w)$ that shall be
weighted added.
When looking at the equations that define Modified Kneser-Ney we note the fact
that only the count terms in the numerators depend on the probability event $w$.
Exactly these terms compose our sum arguments:
\begin{equation}
  \SumArg_i^h(w) =
    \begin{dcases*}
      \Count(w)                                              & if $N = 1$ \\
      \DiscountedCount(\SeenHistory \: w)                    & if $N \neq 1 \land i = 1$ \\
      \DiscountedCount*(\WSkp \History_i(\SeenHistory) \: w) & if $N \neq 1 \land 1 < i < N$ \\
      \ContCountIp(\WSkp w)                                  & if $N \neq 1 \land i = N$
    \end{dcases*}
\end{equation}
Where $\History_i(w_1^n) = w_i^n$ is a helper mapping that gives the history
of the $i$th interpolation order.

Lastly we need to define the actual sum weights $\SumWeight_i^h$.
We note that --- because each order is interpolated with the weight $\gamma$ of
the previous order --- each order is in total weighted by the product of the
$\gamma$s of all previous orders.
Additionally lower order models occur in the numerators of fractions, because of
that they are weighted by all previous denominator counts as well.
Building on this we can define:
\begin{equation}
  \SumWeight_i^h = \frac{\prod_{j = 1}^{i - 1} \gamma(\History_j(\SeenHistory))}
                        {\Count(\SeenHistory \Skp) \prod_{k = 2}^{i} \ContCountIp(\WSkp \History_k(\SeenHistory) \WSkp)}
\end{equation}
\todo[inline]{Or better recursively? Or alternatively?}
\begin{equation}
  \SumWeight_i^h = \frac{1}{\Count(\SeenHistory \Skp)} \prod_{j = 2}^i \frac{\gamma(\History_{j-1}(\SeenHistory))}{\ContCountIp(\WSkp \History_j(\SeenHistory) \WSkp)}
\end{equation}

\todo[inline]{Do we prove that this is actually MKN?}

Specifying an algorithm to compute interpolation weights $\SumWeight_i^h$ that
minimizes frequency count lookup (using the iterative definition of
$\SumWeight_i^h$) is straightforward and given in \cref{alg:weightedsum-mkn}.
\todo{Do we actually show this algorithm, as it is rather simple?}

\begin{algorithm}
  \caption{Computing Modified Kneser-Ney sum weights}
  \label{alg:weightedsum-mkn}
  \begin{algorithmic}[1]
    \Require $h$
      \Comment{History for which to determinate interpolation weights}
    \Ensure $\SumWeight_i^h$
      \Comment{List of interpolation weights}

    \State $\SeenHistory \gets h$
      \Comment{Find first seen history $\SeenHistory$}
    \While{$\Count(\SeenHistory \Skp) = 0$}
      \State $\SeenHistory \gets \SeenHistory\text{.backoff()}$
        \todo[inline]{Better notation for backoff}
    \EndWhile
    \State $N = \StringLength{\SeenHistory} + 1$
    \State $\SumWeight^h \gets$ new List of size $N$
    \State $\SumWeight_1^h \gets \frac{1}{\Count(\SeenHistory \Skp)}$
      \Comment{Set highest order weight}
    \For{$i \gets 2 \mathinner{\ldotp \ldotp} N$}
      \State $\SumWeight_i^h \gets \SumWeight_{i-1}^h \frac{\gamma(\History_{i-1}(\SeenHistory))}{\ContCountIp(\WSkp \History_i(\SeenHistory) \WSkp)}$
        \Comment{Compute lower orders using previous order}
        \todo[inline]{Replace fraction with $x^{-1}$ for better visual?}
    \EndFor
  \end{algorithmic}
\end{algorithm}

% ------------------------------------------------------------------------------
\section{Generalized Language Model}

The difference between a Modified Kneser-Ney language model and a
Generalized Language Model is the way in which orders are interpolated.
Instead of just one probability --- that of shortening the history by one ---
being factored in, in the Generalized Language Model the average of multiple
probabilities of the next lower order is factored into the higher order
probability.

This section will not consider the general case were one probability
$\ProbGLM{w}{h}$ incorporates exactly $\#_\partial(h)$ probabilities
$\ProbGLM*{w}{\partial_j h}$ of the next lower order.
Instead only the special case described by \textcite{Pickhardt2014}, that is
actually used in practice, is examined.
That is the number of lower order probabilities $\#_\partial(h)$ incorporated
is set to the number of non-skip words in $h$, and $\partial_j(h)$ is defined as
replacing the $j$th non-skip word in $h$ with a skip $\Skp$.

%   -    -    -    -    -    -    -    -    -    -    -    -    -    -    -    -
\subsection{Binomial Diamond}

The directed graph of \cref{fig:history-glm} visualizes the lower order
probabilities that factor into those of higher orders, using that definition of
$\partial_j(h)$.
The nodes represent the histories $h$ of a probability $\ProbGLM{w_4}{h}$.
The directed edges represent an immediate dependency, for example the
probabilities that occur in the definition of
$\ProbGLM*{w_4}{w_1 w_2 \Skp}$ are $\ProbGLM*{w_4}{w_1 \Skp \Skp}$ and
$\ProbGLM*{w_4}{\Skp w_2 \Skp}$.

\begin{figure}
  \centering
  \begin{tikzpicture}
    \begin{scope}[node distance = 6.5em]
      \node [state] (000)                  {$w_1 \: w_2 \: w_3$};
      \node [invis] (000l) [left  of=000]  {};
      \node [invis] (000r) [right of=000]  {};

      \node [state] (001)  [below of=000l] {$w_1 \: w_2 \: \Skp$};
      \node [state] (010)  [below of=000]  {$w_1 \: \Skp \: w_3$};
      \node [state] (100)  [below of=000r] {$\Skp \: w_2 \: w_3$};

      \node [state] (011)  [below of=001] {$w_1 \: \Skp \: \Skp$};
      \node [state] (101)  [below of=010] {$\Skp \: w_2 \: \Skp$};
      \node [state] (110)  [below of=100] {$\Skp \: \Skp \: w_3$};

      \node [state] (111)  [below of=101] {$\Skp \: \Skp \: \Skp$};
      \node [invis] (111l) [below of=011] {};
    \end{scope}

    \begin{scope}[node distance = 6em]
      \node [order] [left of=000l]  {Highest order};
      \node [order] [left of=001]   {1st Lower order};
      \node [order] [left of=011]   {2nd Lower order};
      \node [order] [left of=111l]  {Lowest order};

      % to get align to center right
      \node [order] [right of=000r] {};
    \end{scope}

    \path[->, >=latex, thick]
      (000) edge (001)
      (000) edge (010)
      (000) edge (100)

      (001) edge (011)
      (001) edge (101)
      (010) edge (011)
      (010) edge (110)
      (100) edge (101)
      (100) edge (110)

      (011) edge (111)
      (101) edge (111)
      (110) edge (111);
  \end{tikzpicture}
  \caption{
    Binomial diamond of order 3 for history $h = w_1 \: w_2 \: w_3$.
  }
  \label{fig:history-glm}
\end{figure}

We call a graphs that represents the inter-dependencies of probabilities during
the calculation of $\ProbGLM{w}{h}$ the \emph{binomial diamond} of order
$n = \StringLength{h}$, because of its diamond-like shape, and a number of
properties:

\begin{enumerate}
  \item \label{itm:num-layers} If we partition the graph into layers, where
    layer $k$ contains the histories that have exactly $k$ skip words, the graph
    has $n + 1$ layers.
  \item \label{itm:num-childs}  Layer $k$ contains exactly $\binom{n}{k}$ nodes.
  \item \label{itm:num-edges}   Nodes in layer $k$ have exactly $n - k$ outgoing
    and $k$ incoming edges.
  \item \label{itm:num-nodes}  There are $2^n$ nodes in the graph.
\end{enumerate}

\Cref{itm:num-layers} is due to the fact that layer $k$ includes exactly those
histories that are part of the $k$th highest of probability.

To explain \cref{itm:num-childs}, it is helpful to imagine replacing words in
a history as laying a mask of either a skip or a non-skip on top of each word.
Then the number of nodes in each layer can be understood as permuting the
skip and non-skip masks on top of the history.
It is a well known fact that there are exactly $\binom{n}{k}$ permutations of
$k$ tokens of type A (skips), $l$ tokens of type B (non-skips) and $n = k + l$.

\Cref{itm:num-edges} directly follows from the fact, that each non-skip word
in a history can be replaced by a skip, and that each skip is the result
of those replacements.

Lastly, \cref{itm:num-nodes} is a corollary of \cref{itm:num-childs} because
$\sum_{k=0}^n \binom{n}{k} = 2^n$. This equation be inferred from the binomial
theorem $(x+ y)^n = \sum_{k=0}^n \binom{n}{k} x^{n-k} y^k$ by setting
${x = y = 1}$.

Following the model of the binomial diamond will be a major aid in understanding
the weighted sum representation of the Generalized Language Model.

% - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -
\subsection{Weighted sum}

% - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -
\subsection{Bitmagic}

% - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -
\subsection{Computational Complexity}
