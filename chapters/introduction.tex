\chapter{Introduction}
\label{ch:introduction}

\emph{Word prediction} or autocompletion is the art of inferring the next
word (or words) a user intents to type from the previous words they have
entered.
The user can then either select a completion, or in the case that it did not
match, type the next indented character, with which more accurate completions
can be given.

In the context of \emph{natural language processing} the prediction requires
understanding of the user's language.
A successful general purpose word prediction will have knowledge about a
language's grammar, word frequencies, idioms, and much more.
This is in contrast to simpler solutions, like selecting the best matching word
from a list of previously entered ones in a word processing software.

\todo{For more benefits and citations read \url{http://trace.wisc.edu/docs/wordprediction2001/}.}
Word prediction has many benefits.
Most obviously it may speed up text entry \parencite{Trnka2007}.
Furthermore it may be of assistance to users with spelling disorders
\parencite{Newell1992}.
But it may also be of used as a method to start computing answers to a user
query that they have not even finished typing \noref.

Today word prediction is a part of various software systems.
Although word prediction has been primarily used to augment the communication of
users with a difficulty to express themselves verbally
\parencite{Swiffin1987,Trnka2011}, the benefits of a strong word prediction
should be obvious to anyone typing large amounts of text on a computer:
be it the list of possible queries offered by popular search engines such as
Google, or the next word suggestions that many modern mobile phones provide, to
speed up the slow two-thumbs typing.

This thesis will address the problem of \emph{next} word prediction.
Our viewpoint is that we predict only one following word, although the
techniques described in this thesis can also be applied to the prediction
of whole sequences.
Good interfaces usually suggest a list of next word predictions and allow the
user to select from these.
This is done to mitigate the error should the top prediction turn out to not
match the user's intend.

We formalize the task of next word prediction as follows:
\begin{equation}
  \label{eq:prefixquery}
  \NWP[p][k]{h} =
    \Argmax*[k]{\mathclap{\substack{w \in \Vocab,\\p \: \text{prefix of} \: w}}}
      \Prob{w}{h}
\end{equation}
Our task is such: given a history $h$ of entered words and an already entered
prefix $p$ of the intended word, we predict $k$ completions.
Of course $p$ may also be empty which corresponds to the case that the user has
not yet typed any prefix.
We subsume next word prediction with or without a prefix under the title of
\emph{prefix queries}, with a possibly empty prefix.
We solve this task by finding the $k$ words $w$ in the vocabulary $\Vocab$, the
set of all words, that maximize the probability $\Prob{w}{h}$.
That probability models the likelihood that the word $w$ follows the history
$h$.
$\Argmax*[k]$ is a helper used to model the $k$ words that maximize the
probability and an extension to the conventional $\Argmax$ function:
\begin{subequations}
  \begin{align}
    \Argmax [1]{x} f(x) &= \big\{ x \,\Given\, \forall y : f(y) \leq f(x) \big\} \\
    \Argmax [k]{x} f(x) &= \big\{ x \,\Given\, \forall y \notin \Argmax*[k-1]{z} f(z) : f(y) \leq f(x) \big\} \\
    \Argmax*[k]{x} f(x) &= \bigcup_{{1 \leq i \leq k}} \Argmax[i]{x} f(x)
  \end{align}
\end{subequations}

The quality of predictions directly correlates with the aptitude of the chosen
probabilities $\Prob{w}{h}$.
Finding good ways to estimate these probabilities is the art of
\emph{statistical language models} (\cref{sec:introduction-langmodels}).

Computing next word predictions following \cref{eq:prefixquery} is a very time
expensive operation.
Depending on the chosen probability estimate more or less complex calculations
have to be performed, which usually require access to backing data structures.
Furthermore that probability is calculated for all words in the vocabulary
$\Vocab$, or at least for the set of all words one wants to use for prediction.

Multi-word prediction is typically implemented using \emph{Viterbi-} or
\emph{beam search}-algorithms \parencite{Bickel2005,JurafskyMartin2009}.
These algorithms are based on following most promising prediction paths and
pruning the search space with some heuristic.
They however are not applicable to the problem of predicting only the next
word, and at least \textcite{Bickel2005} still employ the na\"{\i}ve method of
querying all known words to find the next word in a multi-word prediction.

This thesis concerns itself with improving the query time of prefix queries.
To this end two independent optimizations will be presented:
\begin{enumerate}
  \item We will show a novel way to rewrite the recursive definitions for
    $\Prob{w}{h}$ of the two state-of-the-art language modeling techniques
    \emph{Modified Kneser-Ney Smoothing} and the \emph{Generalized Language
    Model} as weighted sums.
    In this representation the com\-pu\-ta\-tion-costly weights will be
    independent of $w$ and can be calculated once in a precomputation step for
    each query.
    This will significantly reduce the computation time of $\Prob{w}{h}$ for
    varying words~$w$ but a fixed history~$h$.
  \item We will apply well known \emph{top-$k$ joining techniques}
    (\cref{sec:introduction-topkjoin}) to the problem of next word prediction.
    By utilizing a sophisticated data structure that allows retrieval of
    sequence-count-lists sorted by counts for arbitrary sequence prefixes,
    we will be able to considerably reduce execution time of next word
    prediction by avoiding to enumerate all words in the vocabulary.
\end{enumerate}

These optimizations will allow us to give fast, \emph{non-approximative}
predictions.
That is the predictions will be as good as the underlying language model allows:
the system will always predict exactly these words that maximize the
probability.
No pruning of the word distribution's long tail is necessary.

The remainder of this chapter will introduce the two main subjects of this
thesis: statistical language models (\cref{sec:introduction-langmodels}) and
top-$k$ join queries (\cref{sec:introduction-topkjoin}).



% ------------------------------------------------------------------------------
\section{Statistical Language Models}
\label{sec:introduction-langmodels}

\emph{Statistical language models} assign a probability $\Prob[\text{LM}]{s}$ to
a sequence of words $s = w_1^l = w_1 \ldots w_l$ by means of a probability
distribution, created through statistical analysis of text corpora.
Through their use we model knowledge of the user's language in the word
prediction application.

A major problem in creating statistical language models from text corpora, is
that even for very large corpora, most possible word sequences in a language
will not be observed in training \noref.
A common approximation is to make a $n$th-order \emph{markov assumption},
i.e.\ to assume that the probability of the word to be predicted $w_l$ only
depends on the $n\!-\!1$~previous words $w_{l-n+1}^{l-1}$ rather than the full
sequence $w_1^{l-1}$.
Thus only sequences of length $n$ have to be considered.
The resulting language models are called \emph{$n$-gram models}.
State-of-the-art language modeling uses $n$-grams with $n$ as large as $5$
\parencite{JurafskyMartin2009,Goodman2001,Stolcke2000}.

Most language model probability formulas are given in recursive form, that only
implicitly depend on the occurrence count of sequences in the corpus \noref.
If these probabilities can be expressed as monotone scoring functions of
occurrence counts, we can apply top-$k$ joining techniques in order to solve
noisy channel type queries.


% ------------------------------------------------------------------------------
\section{Top-\emph{k} Join Queries}
\label{sec:introduction-topkjoin}

Top-$k$ queries are queries whose results are limited to only the $k$ most
important ones.
These top-$k$ results are identified by scoring all answers with a
\emph{scoring function}, and selecting the $k$ highest scoring.
\emph{Top-$k$ join queries} are these top-$k$ queries that produce
their results trough combination of multiple data sources \parencite{Ilyas2008}.

One example might be the query to a web search engine.
Search results are ranked by relevance, as computed by several criteria
(matching keywords, links from other sites, popularity, \ldots).
Users generally require more than just the one highest ranking search result, as
it may not be exactly what they are they were looking for.
On the other hand it would be infeasible to relevance-rank all sites in the
search engine's database.
Many search engines solve this by only displaying the top 10 sites, deemed most
relevant to the user's query, and querying more if required.

\begin{draft}
Top-$k$ joining techniques have widespread use and are of importance in
many applications \noref.
\end{draft}

As can be seen by the example, the na\"{\i}ve way to solve top-$k$ join
queries, i.e.\ to aggregate all possible join results, sort all of them by
score, and then to discard non top-$k$ results, is in most cases far to
slow to be of any use.
There are of plethora of different top-$k$ processing techniques, but most
of them achieve efficiency by requiring \emph{monotone scoring functions} and
then sorting data sources on each predicate of that function
\parencite{Ilyas2008}.


% ------------------------------------------------------------------------------
\section{Thesis Overview}


% ==============================================================================
% ==============================================================================
% ==============================================================================
\clearpage

\textbf{Old stuff only kept here for reference. Continue with next chapter.}

\begin{draft}
\textcite{Bickel2005} actually does trivial argmax and calculates probabilities
for all words in vocabulary repeatedly.
\end{draft}

The \emph{noisy channel model} is a commonly applied concept in natural language
processing.
As first described by \textcite{Shannon1948}, it consists of a transmitter that
tries to pass a message over a channel to a receiver.
The channel is called noisy, because it may distort the message in any way.
The task is then to reconstruct the original message, from the recieved,
distorted one.

This metaphor has many applications, such as spelling correction
\parencite{JurafskyMartin2009,Manning2008,Kernighan1990,Mays1991},
%          \------------- non-word-errors ------------/    \- real-word-erros
word prediction \parencite{Bickel2005}, part-of-speech tagging
\parencite{Church1988}, machine translation \parencite{Brown1990}, speech
recognition or text compression.
\todo{Newer citations!}
% JurafskyMartin2009: Chapter 5: Spelling Correction
%                     Chapter 9: Speech Recognition!

This thesis will present a novel approach to solve noisy channel type queries.
The basic idea is to formulate the query as the argmax of a monotone scoring
function, and then to find the solution using \emph{top-$k$ joining techniques}.
This approach will exemplary be showcased by applying it to the problem of
\emph{next word prediction}.

% ------------------------------------------------------------------------------
\section{Noisy Channel Model}

\Rene[noline]{Probability space unclear. Most of the time not exactly stated
when talking about Noisy Channel Theorem. Maybe not bring it up in bachelor
thesis.}
Formalized, the task of the noisy channel model is to find the dispatched
message~$m$, which in our context is the sequence of words $s$ in a
langauge~$\Language$ that is most likely to be the source of the received,
distorted observation~$o$:
\begin{align}
  \label{eq:noisychannel}
  m &= \Argmax{s \in \Language} \Prob{s}{o} \\
  \intertext{Commonly this noisy channel equation is rewritten using Bayes'
    theorem to:}
  \label{eq:noisychannelbayes}
  \begin{split}
    m &= \Argmax{s \in \Language} \frac{\Prob{o}{s} \cdot \Prob{s}}{\Prob{o}}
       = \Argmax{s \in \Language} \Prob{o}{s} \cdot \Prob{s}
  \end{split}
\end{align}
With the observation likelihood~$\Prob{o}{s}$ and the prior
probability~$\Prob{s}$.
Omitting the denominator~$\Prob{o}$ is valid because it is a constant that does
not depend on the argument~$s$.
Most commonly the observation likelihood is some domain specific probability
distribution $\Prob[\text{Domain}]{o}{s}$.
For the prior probability, \emph{statistical language model} distributions
$\Prob[\text{LM}]{s}$ are used.
%They assign a probability $\Prob[\text{LM}]{s}$ to a sequence of words
%$s$ by means of a probability distribution, created through statistical analysis
%of text corpora.
Thus the noisy channel model is a natural combintion of domain knowledge with
language information.

For example, to translate French into English, \textcite{Brown1990} defined a
probability distribution $\Prob[\text{Translate}]{e}{f}$, that would give the
probability for each English sentence $e$ to be the result of translating
the French sentence $f$ into English.
To obtain the most likely translation, the $e$ that maximizes that probability
has to be found.
With applying Bayes' theorem they combine their distribution with language
models and arrive at
$\Argmax{e} \Prob[\text{Translate}]{f}{e} \cdot \Prob[\text{LM}]{e}$, which is
just \cref{eq:noisychannelbayes}.

% ------------------------------------------------------------------------------
\section{Next Word Prediction}

In this thesis the problem of \emph{next word prediction} will be researched.
For a sequence of words $w_1 \ldots w_n = w_1^{n}$, the task is to find the last word
$w_n$, given only the first $n-1$ words $w_1^{n-1}$.
This is done by finding the $w$ that maximizes the probability to
occur after $w_1^{n-1}$ \parencite{Bickel2005}:
\begin{equation}
  \label{eq:nwp}
  w_n = \Argmax{w \in \Vocab} \Prob{w}{w_1^{n-1}}
\end{equation}
With vocabulary $\Vocab$, the set of all words.

We can see that next word prediction is an instance of the noisy channel model,
as specified in \cref{eq:noisychannel}.

As there is no domain knowledge to apply, one way to solve next word prediction
is to directly calculate probabilities $\Prob{w}{w_1^{n-1}}$ by using
\emph{statistical language models} \parencite{Bickel2005}.

Next word prediction techniques are often evaluated by measuring \emph{next
keystroke savings} \parencite{Bickel2005}\todo{More citations!}.
\begin{draft}Next Keystroke savings are: What actually?\end{draft}
As such, we would like our implementation to also support the computation of
next keystroke savings.
%A metric closely related to next word prediction is \emph{next keystroke
%savings}.
%\Lukas[inline]{How do we define NKSS? How do I justify it's use in this thesis?
%I need NKSS to justify my choice of completion-tries as data structure :).}

% ------------------------------------------------------------------------------
\section{Top-\emph{k} Join Queries}



% ------------------------------------------------------------------------------
\section{Statistical Language Models}



% ------------------------------------------------------------------------------
\section{Thesis' contribution}

\todo[noline]{Wouldn't this be better as an abstract? How do I introduce here
instead?}
The concern of this thesis is solving noisy channel type queries that are based
on statistical language model probabilities.
Our novel approach is to formulate language models as weighted sum functions on
occurrence counts.
As these weighted sum functions are monotone, we can then employ various
top-$k$ joining techniques, to efficiently find those arguments that maximize
the noisy channel's probability.
This approach can be used for all noisy channel type queries, that can be
expressed as monotone scoring functions.
We will present our method by implementing next word prediction with it.

In \cref{ch:relatedwork} an overview of related work will be given.
\Cref{ch:review} will review the two types of language models, that are used in
this thesis: \emph{Modified Kneser-Ney Smoothing} and the \emph{Generalized
Language Model}.
We will then express these language models as weighted sums in
\cref{ch:weightedsum}.
Using this representation as monotone scoring functions, top-$k$ joining
techniques and their implementations will be discussed in \cref{ch:topkjoin}.
In \cref{ch:evaluation} the given approach will be evaluated using empirical
results.
\Cref{ch:conclusion} concludes.

\begin{draft}
\begin{displayquote}[\textcite{JurafskyMartin2009}]
Because in practice most implementations of Viterbi use beam
search, some of the literature uses the term beam search or time-synchronous beam
search instead of Viterbi.
\end{displayquote}
\end{draft}

The established way to solve noisy channel type queries is to use
\emph{Viterbi-} or \emph{beam-search-algorithms}
\parencite{JurafskyMartin2009,Bickel2005}.
These algorithms are based on following most promising looking paths and
pruning the search space with some heuristic.
Trough this pruning, remarkable runtime performance can be achieved, however it
has other undesirable effects, for instance these algorithms are not guaranteed
to always find (optimal) solutions \parencite{Bickel2005}.

This thesis' approach using top-$k$ joining does not have these drawbacks, it
will always find the best solution(s).
