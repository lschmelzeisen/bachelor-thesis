\documentclass[m,bachelor,binding]{WeSTthesis}

\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage[ngerman,english]{babel}
\usepackage{csquotes}

\usepackage{xparse}

\usepackage{amsmath}
\usepackage{mathtools}
\usepackage{nicefrac}
\usepackage{IEEEtrantools}

\usepackage[parfill]{parskip}

\usepackage[toc]{appendix}

% In-paragrap enumerations
\usepackage{paralist}

% Pseudocode
\usepackage{algpseudocode}
\usepackage[chapter]{algorithm}  % Have algorithm numbering per chapter
\renewcommand{\algorithmicrequire}{\textbf{Input:}}
\renewcommand{\algorithmicensure}{\textbf{Output:}}

% Code listings
\usepackage{listings}

% Drawing figures
\usepackage{tikz}

% Courier as monospace font.
\usepackage{courier}

\usepackage{pdflscape}

\usepackage[
  backend=biber,
  style=authoryear-comp,
  %style=alphabetic,
  maxbibnames=10,
  maxcitenames=2,
  backref=true,
  backrefstyle=three+,
  doi=false,
  isbn=false,
  url=false,
]{biblatex} 
\addbibresource{bibliography.bib}
\setlength{\bibitemsep}{0.5em}  % vertical space between bibliography entries
\setlength{\bibhang}{1em}       % indent after first line on bibliography entries
% http://tex.stackexchange.com/questions/67153/bibliography-not-in-toc-when-using-biblatex-biber
\DefineBibliographyStrings{english}{%
  bibliography = {References},
}

% For text colorization
%\usepackage{color}
%\usepackage[usenames,dvipsnames]{xcolor}

% Should be included last, to redefine commands
\usepackage{hyperref}

% Has to be included even after hyperref
\usepackage[nameinlink,noabbrev]{cleveref}
% http://tex.stackexchange.com/questions/119513/cleveref-and-appendix-packages-appendix-referenced-as-section
\crefname{appsec}{appendix}{appendices}
% Capitalize all references, regardless of command used.
% We use different commands, to allow easy disabling of this.
\renewcommand\cref{\Cref}

\usepackage{proba}

% -- Math ----------------------------------------------------------------------

% Scale
% see http://tex.stackexchange.com/questions/60453/reducing-font-size-in-equation
\newcommand*{\Scale}[2][4]{\scalebox{#1}{$#2$}}%

% For conditions after equations
\newcommand{\Condition}[1]{\Scale[0.85]{(\text{if} \: #1)}}

% Given
% see http://tex.stackexchange.com/questions/141570/sizing-for-given-that-symbol-vertical-bar
\newcommand\Givenbase[1][]{\,#1\lvert\,}
\let\Given\Givenbase
\newcommand\Sgiven{\Givenbase[\delimsize]}
\DeclarePairedDelimiterX\Basics[1](){\let\Given\Sgiven #1}

% Set
% see http://tex.stackexchange.com/questions/13634/define-pretty-sets-in-latex-esp-how-to-do-the-condition-separator
\newcommand\SetSymbol[1][]{
  \nonscript\,#1\vert\nonscript\,\mathopen{}\allowbreak
}
\DeclarePairedDelimiterX\Set[1]{\{}{\}}%
  { \renewcommand\Given{\SetSymbol[\delimsize]} #1 }
  
\newcommand{\DummyArg}{\cdot}
\newcommand{\DummyIndex}{\bullet}

% Argmax
\DeclareMathOperator*{\Argmax}{arg\,max}

% Cardinality
\newcommand{\Cardinality}[1]{\left|#1\right|}

% String Length
\newcommand{\StringLength}[1]{\left|#1\right|}

% Probability
\NewDocumentCommand{\Prob}{s o g g}{
  \IfBooleanTF{#1}{\operatorname{\hat P}}{\operatorname{P}}_{\IfNoValueTF{#2}{\,}{#2}}
  \IfNoValueTF{#3}{}{(\IfNoValueTF{#4}{#3}{#3 \Given #4})}
}
\NewDocumentCommand{\ProbMKN}{s g g}{
  \IfBooleanTF{#1}{\Prob*[\text{MKN}]{#2}{#3}}
                  {\Prob [\text{MKN}]{#2}{#3}}
}
\NewDocumentCommand{\ProbGLM}{s g g}{
  \IfBooleanTF{#1}{\Prob*[\text{GLM}]{#2}{#3}}
                  {\Prob [\text{GLM}]{#2}{#3}}
}

% Counts
\DeclareMathOperator{\Count}{c}
\DeclareMathOperator{\ContCount}{N}
% roman numbering because numbers are not allowed in commands
\newcommand{\ContCountIp}  {\ContCount_{\Scale[0.5]{1+}}} 
\newcommand{\ContCountI}   {\ContCount_{\Scale[0.5]{1 }}}
\newcommand{\ContCountII}  {\ContCount_{\Scale[0.5]{2 }}}
\newcommand{\ContCountIIIp}{\ContCount_{\Scale[0.5]{3+}}}
\DeclareMathOperator{\Discount}{D}
\NewDocumentCommand{\DiscountedCount}{s}{
  \operatorname{\IfBooleanTF{#1}{\ContCountIp}{\Count}^d}
}

% Absolute Skip
\newcommand{\Skp}{\ensuremath{\,\Scale[0.7]{\square}\,}}
% Continuation Skip
\newcommand{\WSkp}{\ensuremath{\,\bullet\,}}

% Language
\newcommand{\Language}{\mathcal{L}}

% Next Word Prediction
\NewDocumentCommand{\NWP}{g}{
  \operatorname{NWP}\IfNoValueTF{#1}{}{(#1)}
}

% Next Keystroke Savings
\NewDocumentCommand{\NKSS}{g}{
  \operatorname{NKSS}\IfNoValueTF{#1}{}{(#1)}
}

% Weighted Sum
\DeclareMathOperator{\SumWeight}{\lambda}
\DeclareMathOperator{\SumArg}{\alpha}

\newcommand{\SeenHistory}{h^\prime}

\DeclareMathOperator{\History}{\eta}

% -- Code ----------------------------------------------------------------------

\newcommand{\inlinecode}[1]{\lstinline[columns=fixed]{#1}}

% -- Editiorial Commands -------------------------------------------------------

\definecolor{draftcolor}{rgb}{.5, .5, .5}
\newenvironment{draft}{\color{draftcolor}}{}
\newcommand{\todo}[1]{\textcolor{red}{{\footnotesize\textbf{\texttt{TODO:}}} #1}}
\newcommand{\lukas}[1]{\textcolor{cyan}{{\footnotesize\textbf{Lukas:}} #1}}
\newcommand{\rene}[1]{\textcolor{blue}{{\footnotesize\textbf{Rene:}} #1}}

\newcommand{\noref}{\textcolor{blue}{\footnotesize[Citation needed]}}
\newcommand{\mbref}[1]{\textcolor{blue}{\footnotesize[Citation: #1?]}} 

% ------------------------------------------------------------------------------

\newcommand{\mytitle}{Computing Language Model ArgMax-Queries Using Top-\emph{k}
  Joining Techniques}
%\newcommand{\mytitle}{Applying Top-\emph{k} Joining Techniques to
%  Modified-Kneser-Ney Language Model's and Generalized Language Model's Argmax
%  Queries}
\newcommand{\myname}{Lukas Schmelzeisen}
\newcommand{\mymail}{lukas@uni-koblenz.de}

% ------------------------------------------------------------------------------

\title{\mytitle}

\author{
  \myname \\
  \small{\texttt{\href{mailto:\mymail}{\nolinkurl{\mymail}}}}
}

\degreecourse{Informatik}

\firstreviewer{Prof.~Dr.~Steffen Staab}
\firstreviewerinfo{Institute for Web Science and Technologies}

\secondreviewer{Ren{\'e} Pickhardt}
\secondreviewerinfo{Institute for Web Science and Technologies}

\date{\today}
\hypersetup{
  unicode = true,
  pdfauthor = \myname,
  pdftitle = \mytitle,
}

\begin{document}

% ==============================================================================
\pagenumbering{roman}
\selectlanguage{ngerman}
\maketitle
\selectlanguage{english}

% ==============================================================================
\vspace*{\fill}

\begin{minipage}{\linewidth} % To have both abstracts on one page
  \begin{abstract}
    \todo{Abstract.}
  \end{abstract}

  \vspace{5em}

  \selectlanguage{ngerman}
  \begin{abstract}
    \todo{Ãœbersetzen nach deutsch.}
  \end{abstract}
  \selectlanguage{english}
\end{minipage}

\vspace*{\fill}
\todo{Page numbering on abstract page.}
\clearpage

% ==============================================================================
\todo{License!}

\todo{Discussion with Ren{\'e}:}
\begin{itemize}
  \item Is ``we'' ok, when single author?
  \item Are chapters ok?
  \item Capitalize in document reference?
  \item Citation style?
  \item Conditions after Equations ok as in \cref{ch:review-lm}?
  \item Scope ok? Not too long or short?
  \item Terminology: Probability event? Interpolation weight/argument?
\end{itemize}

\clearpage

% ==============================================================================
\tableofcontents
\clearpage
\pagenumbering{arabic}

% ==============================================================================
\chapter{Introduction}

\begin{draft}
Introduction pargraph: We solve noisy channel argmax with Top-$k$ joining
techniques.
\end{draft}

The \emph{noisy channel model} is a commonly applied concept in natural language
processing.
As first described by \cite{Shannon1948}, it consists of a transmitter, who
tries to pass a message over a channel to a receiver.
The channel is called noisy, because it may distort the message in any way.
The task is then to reconstruct the original message, from the recieved
distorted one.

This metaphor has many applications, such as spelling correction
\parencite{JurafskyMartin2009,Manning2008,Kernighan1990,Mays1991},
%          \------------- non-word-errors ------------/    \- real-word-erros
part-of-speech-tagging \parencite{Church1988}, machine translation
\parencite{Brown1990}, speech recognition or text compression.
\todo{Find newer citations. Maybe from \cite{Bickel2005}}
% JurafskyMartin2009: Chapter 5: Spelling Correction
%                     Chapter 9: Speech Recognition!

Formalized, this task of the noisy channel model is to find the dispatched
message~$m$, which is the sequence of words $s = w_1^n = w_1 \ldots w_n$ in a
langauge~$\Language$ that is most likely to be the source of the received
distorted observation~$o$:
\begin{align}
  \label{eq:noisychannel}
  m &= \Argmax_{s \in \Language} \Prob{s}{o} \\
  \intertext{Using Bayes' theorem one can rewrite this to:}
  \label{eq:noisychannelbayes}
  \begin{split}
    m &= \Argmax_{s \in \Language} \frac{\Prob{o}{s} \cdot \Prob{s}}{\Prob{o}} \\
      &= \Argmax_{s \in \Language} \Prob{o}{s} \cdot \Prob{s}
  \end{split}
\end{align}
With the observation likelihood~$\Prob{o}{s}$ and the prior
probability~$\Prob{s}$.
Omitting the denominator~$\Prob{o}$ is valid because it is a constant that does
not depend on the argument~$s$.
Most commonly the observation likelihood is some domain specific probability
distribution $\Prob[\text{Domain}]{o}{s}$, for the prior probability
\emph{statistical language models} are used.
They assign a probability $\Prob[\text{LM}]{s}$ to a sequence of words
$s = w_1^n = w_1 \ldots w_n$ by means of a probability distribution, created
through statistical analysis of text corpora.
Thus the noisy channel model is a natural combintion of domain knowledge with
language information.

For example, to translate French into English, \cite{Brown1990} defined a
probability distribution $\Prob[\text{Translate}]{e}{f}$, that would give the
probability for each English sentence $e$ to be the result of translating
the French sentence $f$ into English.
To obtain the most likely translation, the $e$ that maximizes that probability
has to be found.
With applying Bayes' theorem they combine their distribution with language
models and arrive at
$\Argmax_{e} \Prob[\text{Translate}]{f}{e} \cdot \Prob[\text{LM}]{e}$, which is
just \cref{eq:noisychannelbayes}.
\\ \lukas{It is possible to omit this paragraph, but I think it provides
clarification.}

A major problem in creating statistical language models from text corpora, is
that even for very large corpora, most possible word sequences in a language
will not be observed in training \noref.
A common approximation are \emph{$n$-gram models}, in which it is assumed that
the probability of a word only depdens on the $n\!-\!1$~previous words, thus
only sequences of length $n$ have to be considered.

State-of-the-art language models use $n$-grams with $n$ as large as $5$
\mbref{JurafskyMartin2009,Goodman2001}.
\begin{draft}
\Cref{eq:noisychannel} is hard to calculate for such large $n$.
So we want to optimize that.
\end{draft}

\begin{draft}
We try to use top-$k$ joining.

We are interested in \emph{next word prediction} $\NWP$ and
\emph{next keystroke savings} $\NKSS$.

So we have to map our problem on the realm of top-$k$ joining.

We apply that technique.

May be good because we do not have to prune, and pruning might be bad
\parencite{Stolcke2000,Chelba2013,Chelba2010,Siivola2007}.

Most non-trivial top-\emph{k} joining algorithms require a monotone scoring
function, some further improvements can be made if using a linear scoring
function~\parencite{Ilyas2008}.
\end{draft}

\todo{Read \cite{Bickel2005} for introduction to word prediction.}

\todo{Overview of following chapters.}

% ==============================================================================
\chapter{Related Work}

\begin{draft}
Commonly to solve the described problem Viterbi or beam search algorithms are
used \mbref{JurafskyMartin2009}.
\end{draft}

\todo{Is there some work on expressing language models as weighted sums?}

% ------------------------------------------------------------------------------
\section{State-of-the-art Language Models}

\begin{draft}
This section summarizes the state-of-the-art language models considered in this
work.
And their origins?
\end{draft}

The currently most commonly used \parencite{JurafskyMartin2009,Chelba2013}
technique for estimating language models is \emph{Modified Kneser-Ney Smoothing}
by \cite{ChenGoodman1996,ChenGoodman1998,ChenGoodman1999}, based on
\emph{Kneser-Ney Smoothing} by \cite{KneserNey1995}.
Very recently \cite{Pickhardt2014} provided a generalization of this technique,
the \emph{Generalized Language Model}.

\begin{draft}
Justify selection of MKN and GLM over recently popular LMs from
\parencite{Chelba2013}.

Mention \emph{Nerual Network Language Models} \parencite{Bengio2003,Mikolov2012}
(do we not use them because they can not be represented as weighted sums).
\end{draft}

An in-depth summary of Modified Kneser-Ney Smoothing and the Generalized
Language Model is given in \cref{ch:review-lm}.

% ------------------------------------------------------------------------------
\section{Top-\emph{k} Joining Techniques}

\lukas{I would much rather summarize top-$k$ joining techniques in
\cref{ch:top-k-joining} as the task is clearly defined there, and we can have
more in-depth discussion about it.}

\todo{Reference to \cref{ch:top-k-joining}.}

% ==============================================================================
\chapter{Review of Considered Language Modeling Techniques}
\label{ch:review-lm}

This chapter will recall language modeling techniques considered in this work,
and introduce the used notation (\cref{sec:review-notation}).
Namely, \emph{Modified Kneser-Ney Smoothing} (\cref{sec:review-lm-mkn})
and the \emph{Generalized Language Model} (\cref{sec:review-lm-glm})
are described.

\begin{itemize}
  \item \todo{How do I declare citations for this section?}
  \item \todo{Do I cite Rene or ChenGoodman?}
  \item \todo{Do I cite once or on every definition?}
  \item \todo{Some stuff is not published anywhere else yet. (For example new
    skip notation or new glm notation)}
\end{itemize}

% ------------------------------------------------------------------------------
\section{Notation: Counts and Skips}
\label{sec:review-notation}

Let $\Count(w_1^n)$ denote the frequency count of a sequence's
$w_1^n = w_1 \ldots w_n$ occurence in training data.

Let $\Skp$ be a wildcard (called \emph{``skip''}), that can take the place
of any word at its location.
Thus, for example $\Count(\Skp w_1^n)$ is the frequency count of sequences
in training data who start with any word $x \in \Sigma$ and were the remaining
words are $w_1^n$:
\begin{equation}
  \Count(\Skp w_1^n) = \sum_{x \in \Sigma} \Count(x \: w_1^n)
\end{equation}
Where $\Sigma$ is the set of all words in a language
$\Language \subseteq \Sigma^{*}$.
It is easy to extend this definition to sequences with multiple skips:
\begin{equation}
  \Count(w_1 \Skp \Skp w_2) = \sum_{x, y \in \Sigma} \Count(w_1 \: x \: y \: w_2)
\end{equation}

\todo{Do we mention the difference of $\Count(\Skp w_1^n)$ to $\Count(w_1^N)$
in absence of Beginning of Sentence tags?}

Additionally we define \emph{continuation} counts $\ContCount_k(\DummyArg)$ as
the number of sequences from the training data that occur exactly $k$ times and
can be constructed by replacing $\WSkp$ (called \emph{``continuation skip''})
with any word $x \in \Sigma$.
For example $\ContCountII(\WSkp w_1^n)$ denotes the number of words that can
precede $w_1^n$ in training data, and occur exactly $2$ times:
\begin{equation}
  \ContCountII(\WSkp w_1^n) =
    \Cardinality{\left\{ x \in \Sigma \,\middle|\, \Count(x \: w_1^n) = 2 \right\}}
\end{equation}
Further let $\ContCount_{k+}(\DummyArg)$ count sequences that occur $k$ or more
times:
\begin{equation}
  \ContCountIp(\WSkp w_1^n) =
    \Cardinality{\left\{ x \in \Sigma \,\middle|\, \Count(x \: w_1^n) \geq 1 \right\}}
\end{equation}

In continuation counts regular skips and continuation skips can be mixed, for
example:
\todo{Check this equation!}
\begin{equation}
  \ContCountIp(\WSkp w \Skp \WSkp) =
    \Cardinality{\left\{ x, y \in \Sigma \,\middle|\, c(x \: w \Skp y) \geq 1 \right\}}
\end{equation}

To make a clear distinction between ``normal'' counts $\Count(\DummyArg)$ and
continuation counts $\ContCount_{\DummyIndex}(\DummyArg)$, we
sometimes call the former \emph{absolute} counts.

% ------------------------------------------------------------------------------
\section{Modified Kneser-Ney Smoothing}
\label{sec:review-lm-mkn}

\begin{draft}
Modified Kneser-Ney smoothed language models are computed by interpolating
between higher and lower order $n$-gram language models.
The highest order distribution is interpolated with lower order distirbutions
as follows:
\todo{(Copied from Rene)}
\end{draft}
\begin{equation}
  \label{eq:mkn-high}
  \ProbMKN{w_n}{w_1^{n-1}} =
    \frac{\DiscountedCount(w_1^n) + \gamma(w_1^{n-1}) \ProbMKN*{w_n}{w_1^{n-2}}}
         {\Count(w_1^{n-1} \Skp)} \\
\end{equation}

With the \emph{discounted} count
$\DiscountedCount(w_1^n) = \max\Set{\Count(w_1^n) - \Discount(\Count(w_1^n)), 0}$.
%Where the \emph{discounted} count $\DiscountedCount(w_1^n)$ is:
%\begin{equation}
%  \DiscountedCount(w_1^n) = \max\Set{\Count(w_1^n) - \Discount(\Count(w_1^n)), 0} \\
%\end{equation}

The exact definitions of $D(\DummyArg)$ and $\gamma(w_1^{n-1})$ are not
important in our context, however for the sake of completeness they are given 
in \cref{sec:discounts-interpolation-weights}.

\begin{draft}
Essentially, interpolation with a lower order model corresponds to leaving out
the first word in the considered sequence.
\todo{(Copied from Rene)}
\end{draft}
Lower order models are computed differently, and in turn interpolated with
further lower orders:
\begin{equation}
  \label{eq:mkn-low}
  \ProbMKN*{w_n}{w_1^{n-1}} =
    \frac{\DiscountedCount*(w_1^n) + \gamma(w_1^{n-1}) \ProbMKN*{w_n}{w_1^{n-2}}}
         {\ContCountIp(\WSkp w_1^{n-1} \WSkp)} \\
\end{equation}

With the \emph{discounted} continuation count
$\DiscountedCount*(w_1^n) = \max\Set{ \ContCountIp(\WSkp w_1^n) - \Discount(\Count(w_1^n)), 0 }$.
%\begin{equation}
%  \DiscountedCount*(w_1^n) = \max\Set{ \ContCountIp(\WSkp w_1^n) - \Discount(\Count(w_1^n)), 0 }
%\end{equation}

Of course \cref{eq:mkn-high} is only defined if the denominator
$\Count(w_1^{n-1} \Skp) > 0$, that is if the history $w_1^{n-1} \Skp$ was seen
in training.
If $w_1^{n-1} \Skp$ was not seen, we perform a so called \emph{``backoff''}
and set:
\begin{align}
  \label{eq:mkn-backoff}
  \ProbMKN{w_n}{w_1^{n-1}} = \ProbMKN{w_n}{w_1^{n-2}}
      & & \Condition{\Count(w_1^{n-1} \Skp) = 0}
\end{align}
It is common to perform multiple backoffs until the history was actually seen.
Backing off for computing lower orders is never necessary, because
if history $w_1^n$ was seen, $w_1^{n-1}$ was seen as well.

The lowest order (the probability of a single word) can not be further
interpolated and we use the relative continuation frequency of that word.
Similarly if we only want to compute the probability of a single word on the
highest order, we compute it as the relative frequency of that word:
\begin{subequations}
  \label{eq:mkn-lowest}
  \begin{align}
    \ProbMKN {w} &= \frac{\Count(w)}{\Count(\Skp)} \\
    \ProbMKN*{w} &= \frac{\ContCountIp(\WSkp w)}{\ContCountIp(\WSkp \WSkp)}
  \end{align}
\end{subequations}

% ------------------------------------------------------------------------------
\section{Generalized Language Model}
\label{sec:review-lm-glm}

The Generalized Language Model is a natural extension to Modified Kneser-Ney
Smoothing.
It is again based on interpolation, though it differs in the way lower order
models are interpolated.
The highest order is computed as:
\begin{equation}
  \label{eq:glm-high}
  \ProbGLM{w_n}{w_1^{n-1}} =
    \frac{\DiscountedCount(w_1^n) + \frac{\gamma(w_1^{n-1})}{\#_\partial(w_1^{n-1})}
                                    \sum_{j=1}^{\#_\partial(w_1^{n-1})} \ProbGLM*{w_n}{\partial_j w_1^{n-1}}}
         {\Count(w_1^{n-1} \Skp)}
\end{equation}
Auxiliary definitions $\DiscountedCount, \DiscountedCount*, \gamma$ are the same
as for Modified Kneser-Ney Smoothing and given in \cref{sec:review-lm-mkn}.

The skip operator $\partial_j w_1^{n-1}$ can be any mapping that relates
$w_1^{n-1}$ to exactly $\#_\partial(w_1^{n-1})$ many derived sequences.
It is easy to see that for $\#_\partial(w_1^{n-1}) = 1$ and
$\partial_1 w_1^{n-1} = w_2^{n-1}$ \cref{eq:glm-high} is an
instantiation of \cref{eq:mkn-high} (Modified Kneser-Ney Smoothing), and thus
a true generalization.

In practice only one definition for $\partial$ is used though.
That is $\#_\partial(w_1^{n-1})$ is set to the number of non-skip words in
$w_1^{n-1}$ and $\partial_j(w_1^{n-1})$ replaces the $j$th non-skip word with
a skip.
For instance: $\partial_2(w_1^3) = w_1 \Skp w_3$.

Again as with Modified Kneser-Ney Smoothing, \cref{eq:glm-high} is not
defined if $w_1^{n-1}$ is unseen and thus our denominator would be zero.
The backoff step is then performed as:
\begin{equation}
  \label{eq:glm-backoff}
  \begin{split}
    \ProbGLM{w_n}{w_1^{n-1}} = \frac{1}{\#_\partial(w_1^{n-1})}
                               \sum_{j=1}^{\#_\partial(w_1^{n-1})} \ProbGLM{w_n}{\partial_j w_1^{n-1}} \\
      \hfill \Condition{\Count(w_1^{n-1} \Skp) = 0}
  \end{split}
\end{equation}

Subsequently lower order models are computed as:
\begin{equation}
  \label{eq:glm-low}
  \ProbGLM*{w_n}{w_1^{n-1}} =
    \frac{\DiscountedCount*(w_1^n) + \frac{\gamma(w_1^{n-1})}{\#_\partial(w_1^{n-1})}
                                     \sum_{j=1}^{\#_\partial(w_1^{n-1})} \ProbGLM*{w_n}{\partial_j w_1^{n-1}}}
         {\ContCountIp(\WSkp w_1^{n-1} \WSkp)}
\end{equation}

The case of the lowest order occurs when $\#_\partial(w_1^{n-1}) = 0$ and is
handled similarly to Modified Kneser-Ney Smoothing.
In practice this is the case when $w_1^{n-1}$ only consists of skips.
\begin{subequations}
  \label{eq:glm-lowest}
  \begin{align}
    \ProbGLM {w_n}{w_1^{n-1}} &= \frac{\Count(w_1^n)}{\Count(w_1^{n-1} \Skp)}
      & \Condition{\#_\partial(w_1^{n-1}) = 0} \\
    \ProbMKN*{w_n}{w_1^{n-1}} &= \frac{\ContCountIp(\WSkp w_1^n)}{\ContCountIp(\WSkp w_1^{n-1} \WSkp)}
      & \Condition{\#_\partial(w_1^{n-1}) = 0}
  \end{align}
\end{subequations}

\todo{Replace \Skp with \WSkp in $\ContCountIp$ in GLM?}

\todo{\WSkp at end of $\ContCountIp$ correct?}

% ------------------------------------------------------------------------------
\section{Discounting and Interpolation Weights}
\label{sec:discounts-interpolation-weights}

\todo{Explain Discounting and Smoothing.}

\begin{draft}
\begin{equation}
  \Discount(c) =
    \begin{dcases*}
      0      & if $c = 0$ \\
      D_1    & if $c = 1$ \\
      D_2    & if $c = 2$ \\
      D_{3+} & if $c \geq 3$
    \end{dcases*}
\end{equation}

\begin{subequations}
  \begin{align}
    D_1    &= 1 - 2 Y \frac{n_2}{n_1} \\
    D_2    &= 2 - 3 Y \frac{n_3}{n_2} \\
    D_{3+} &= 3 - 4 Y \frac{n_4}{n_3}
  \end{align}
\end{subequations}

\begin{equation}
  Y = \frac{n_1}{n_1 + n_2}
\end{equation}

\begin{equation}
  \gamma(w_1^{n-1}) =   D_1    \ContCountI    (w_1^{n-1} \WSkp)
                      + D_2    \ContCountII   (w_1^{n-1} \WSkp)
                      + D_{3+} \ContCountIIIp (w_1^{n-1} \WSkp)
\end{equation}
\end{draft}

% ==============================================================================
\chapter{Formulating Language Models as Weighted Sums}
\label{ch:weightedsum}

This chapter will show how to represent  $\ProbMKN{w_n}{w_1^{n-1}}$ and
$\ProbGLM{w_n}{w_1^{n-1}}$ as weighted sums of terms depending on $w_n$ for a
fixed history $h = w_1^{n-1}$ and an  arbitrary argument $w = w_n$.
In other words we will express the formulas given in \cref{ch:review-lm} as
equations of the following form:
\begin{equation}
  \Prob{w}{h} = \sum_{i = 1}^{N} \SumWeight_i^h \cdot \SumArg_i^h(w_n)
\end{equation}

Using this it is possible to calculate queries of type ``Noisy Channel Argmax''
(\cref{eq:noisychannel}) much more efficiently, as it is possible to perform the
(potentially expensive) calculation of $\SumWeight_i^h$ in advance.
To compute every $\Prob{w}{h}$ we now just have to calculate the
$\SumArg_i^h(w)$.

Another benefit of this representation is that it is a prerequisite of Top-$k$
joining to supply a monotone scoring function, which only depends on the
joined arguments.
Weighted sums are trivially monotone \todo{really?} and we will be able to
select our join arguments to exactly match $\SumArg_i^h$.
In addition further optimized top-$k$ joining algorithms exists, that require
weighted sum scoring functions.
However we will not use these algorithms in this work.
For a detailed discussion see \cref{ch:top-k-joining}.

In the case of Generalized Language Models our solution will also have a
considerably better computational complexity to calculate probabilities than the
na{\"\i}ve approach of just computing the formulas given in
\cref{sec:review-lm-glm}.
\begin{draft}
This will counter biggest disadvantage of GLM.
\end{draft}

We will now present the basic idea of how to transform $\Prob{w_n}{w_1^{n-1}}$
into weighted sums, that applies to both Modified Kneser-Ney Smoothing and
the Generalized Language Model as well:

Note that in all \cref{eq:mkn-high,eq:mkn-low,eq:glm-high,eq:glm-low} the
probability event $w_n$ only appears in the argument terms of
$\DiscountedCount$ or $\DiscountedCount*$.
Additionally $w_n$ occurs as arguments for $\Count$ and $\ContCountIp$ in the
lowest order \cref{eq:mkn-lowest,eq:glm-lowest}.

Our general idea is thus, to first expand recursive calls to
$\Prob(\DummyArg)$ or $\Prob*(\DummyArg)$, and second to factor
out those terms that do not depend on $w_n$ by the distributive property.
Examples of this idea are given in \cref{app:expansion}.
However they assume the history to be seen in training.
Forfeiting this assumption will add considerable complexity in the case
of Generalized Language Models.

% ------------------------------------------------------------------------------
\section{Modified Kneser-Ney Smoothing}

The first step in computing $\ProbMKN{w}{h}$ is backing off the history $h$
until $h \Skp$ is seen for the first time (\cref{eq:mkn-backoff}).
After this one pair of frequency counts $\Count(h \: w)$ and $\Count(h \Skp)$
are taken (\cref{eq:mkn-high}) and we shorten the
history by one word.
We continue this process with continuation counts $\ContCountIp(\WSkp h \: w)$
and $\ContCountIp(\WSkp h \WSkp)$ (\cref{eq:mkn-low}) until our history is
empty.
Lastly we take the frequency counts of the lowest order
(\cref{eq:mkn-lowest})
All but the lowest order are interpolated with the next lower order via the
interpolation weight $\gamma(h)$.

Let $\SeenHistory$ be the first seen history that may be the result of backing
off given history $h$ a number of times.
The count $N$ of interpolation weights is then the number of words in 
the first seen history $\SeenHistory$ plus one.
\begin{equation}
  N = \StringLength{\SeenHistory} + 1
\end{equation}

We define a helper function $\History_i(w_1^n)$ that gives the history for the
$i$th order model:
\begin{equation}
  \History_i(w_1^n) = w_i^n
\end{equation}

Building on this we can define:
\begin{align}
  \SumArg_i^h(w) &= 
    \begin{dcases*}
      \Count(w)                                              & if $N = 1$ \\
      \DiscountedCount(\SeenHistory \: w)                    & if $N \neq 1 \land i = 1$ \\
      \DiscountedCount*(\WSkp \History_i(\SeenHistory) \: w) & if $N \neq 1 \land 1 < i < N$ \\
      \ContCountIp(\WSkp w)                                  & if $N \neq 1 \land i = N$
    \end{dcases*} \\
  \SumWeight_i^h &= \frac{\prod_{j = 1}^{i - 1} \gamma(\History_j(\SeenHistory))}
                        {\Count(\SeenHistory \Skp) \prod_{k = 2}^{i} \ContCountIp(\WSkp \History_k(\SeenHistory) \WSkp)}
\end{align}

In other words, $\SumArg_i^h(w)$ is the respective frequency count that we take
on the $i$th order of interpolation.
The coefficient $\SumWeight_i^h$ weights each $\SumArg$ with the interpolation
weight $\gamma$ of all higher orders divided by the denominator frequencies of
all yet encountered orders.
\todo{Rene: wording ok?}

\todo{Do we prove that this is actually MKN?}

Specifying an algorithm to compute interpolation weights $\SumWeight_i^h$ that
minimizes frequency count lookup is straightforward and given in
\cref{alg:weightedsum-mkn}.

\begin{algorithm}
  \caption{\todo{Weighted Sum MKN Caption}}
  \label{alg:weightedsum-mkn}
  \begin{algorithmic}[1]
    \Require $h$
      \Comment{History for which to determinate interpolation weights}
    \Ensure $\SumWeight_i^h$
      \Comment{Array of interpolation weights}
    
    \State $\SeenHistory \gets h$
      \Comment{Find first seen history $\SeenHistory$}
    \While{$\Count(\SeenHistory \Skp) = 0$}
      \State $\SeenHistory \gets \SeenHistory\text{.backoff()}$
        \Comment{\todo{Better notation for backoff}}
    \EndWhile
    \State $N = \StringLength{\SeenHistory} + 1$
    \State $\SumWeight^h \gets$ new array of size $N$
    %\State
    \State $nominator \gets 1$
    \State $denominator \gets \Count(\SeenHistory \Skp)$
    \State $\SumWeight_1^h \gets \frac{1}{denominator}$
    %\State
    \State $\History \gets \SeenHistory$
    \For{$i \gets 2, N$}
      \State $nominator \gets \gamma(\History) \frac{nominator}{denominator}$
      \State $\History \gets \History\text{.backoff()}$
      \State $denominator \gets \ContCountIp(\WSkp \History \WSkp)$
      \State $\SumWeight_i^h \gets \frac{nominator}{denominator}$
    \EndFor
    \State \todo{This can probably be simplified considerably (better check with source)}
  \end{algorithmic}
\end{algorithm}

\todo{Do we specify algorithm for MKN weighted sum (as it is rather simple)?}

% ------------------------------------------------------------------------------
\section{Generalized Language Model}

\begin{draft}
Binomial Diamond.
\end{draft}

\begin{figure}
  \centering
  \begin{tikzpicture}[
    node distance = 6em,
    seq/.style = {draw, circle, align=center, text centered, text width=3.5em},
  ]
    \node [seq] (000)                {$w_1 \: w_2 \: w_3$};
    
    \node [seq] (010) [below of=000] {$w_1 \: \Skp \: w_3$};
    \node [seq] (001) [left  of=010] {$w_1 \: w_2 \: \Skp$};
    \node [seq] (100) [right of=010] {$\Skp \: w_2 \: w_3$};
    
    \node [seq] (101) [below of=010] {$\Skp \: w_2 \: \Skp$};
    \node [seq] (011) [left  of=101] {$w_1 \: \Skp \: \Skp$};
    \node [seq] (110) [right of=101] {$\Skp \: \Skp \: w_3$};
    
    \node [seq] (111) [below of=101] {$\Skp \: \Skp \: \Skp$};
    
    \path[->, >=latex, thick]
      (000) edge (001)
      (000) edge (010)
      (000) edge (100)
      
      (001) edge (011)
      (001) edge (101)
      (010) edge (011)
      (010) edge (110)
      (100) edge (101)
      (100) edge (110)
      
      (011) edge (111)
      (101) edge (111)
      (110) edge (111);
  \end{tikzpicture}
  \caption{\todo{Binomial Diamond Caption}}
\end{figure}

\clearpage


\subsection{Bitmagic}

\subsection{Computational Complexity}

% ==============================================================================
\chapter{Top-\emph{k} Joining Language Models}
\label{ch:top-k-joining}

\lstset{
  language=SQL,
  basicstyle=\ttfamily,
  emph={arg,Pattern,Pattern1,PatternN,P_LM},
  emphstyle=\textit,
  deletekeywords={count} % Used as column name, don't want to escape it though.
}

This chapter explains how to efficiently compute arg-max-queries of language
model $\Prob[\text{LM}]$ using top-$k$ joining techniques.

The task is to efficiently compute the following expression:
\begin{equation}
  \Argmax_{t \in T} \Prob[\text{LM}]{t}{h}
\end{equation}
Where $t$ is a token in the set of all tokens $T$, $\Prob[\text{LM}]$ is a
language model, that can be expressed as a weighted sum (as described in
\cref{ch:weightedsum}), and $h$ is the conditional history, a sequence
of tokens.

Given a language model $\Prob[\text{LM}]$ as a scoring function, if we
imagine our input data forming database relations, we can express our
arg-max-query as a SQL-\texttt{SELECT}-Request.

\begin{lstlisting}[mathescape]
  SELECT
    Token.token AS arg,
    $\Prob[\text{LM}]$(Pattern1.count, ..., PatternN.count) AS score
  FROM Token
  LEFT OUTER JOIN Pattern1
    ON history(Pattern1) + arg = Pattern1.sequence
  ...
  LEFT OUTER JOIN PatternN
    ON history(PatternN) + arg = PatternN.sequence
  ORDER BY score DESC
  LIMIT k;
\end{lstlisting}

\todo{Where \inlinecode{history} and plus is concat.}
\todo{Explain \inlinecode{LEFT OUTER JOIN}.}

Our data model is thus: One input relation \inlinecode{Token} that stores
all possible tokens \inlinecode{arg}, over which we want to compute:
\begin{equation}
  \Argmax_{\text{\inlinecode{arg}} \, \in \, \text{\inlinecode{Token}}}
    \Prob[\text{LM}]{\text{\inlinecode{arg}}}{\text{\inlinecode{history}}}
\end{equation}
Additionally we have $n$ input relations
\inlinecode{Pattern1} to \inlinecode{PatternN} that contain pairs
of \inlinecode{sequence}s and the sequence's occurrence \inlinecode{count} as
columns.
Each \inlinecode{Pattern} relation contains exactly these sequences that fit the
relations pattern.

For an example database state and fully expanded SQL-Request see
\cref{app:sql-example}.

A naÃ¯ve method to solve this query would
\begin{inparaenum}[(1)]
  \item produce all possible object combinations that satisfy the join
    condition,
  \item order results by scoring function $\Prob[\text{LM}]$, and
  \item discard non top-$k$ objects (those that do not have the $k$ largest
    scores).
\end{inparaenum}

Considering that in reality only small values of $k$ are queried, this approach
can be deemed as rather wasteful, as large numbers of objects would have to be
discarded.
Optimally we would like to cherry-pick the top-$k$ join results and only
score and order those.
Relying on the fact that we are dealing with a \emph{monotone} scoring function
we approximate this by sorting our input relations by their counts.
\cite{Ilyas2008} survey a wide number of techniques to produces top-$k$ results
with a monotone scoring function from sorted relations.

Among other things, top-$k$ joining techniques differ on the type of join
conditions they can be applied to.
One particularly optimizable join operation is the \emph{equi-join}:
the join that checks for equality over a unique key attribute present in all
relations.
Equi-joins have the advantage that it is possible to form equivalence classes
over an object's key attribute to determine which join results an object is
part of.
Using this, the \emph{hash join} algorithm can efficiently generate join
results.

It is possible to view our join operations as an instance of equi-join:
By filtering each input relation \inlinecode{Pattern} to only contain
objects whose sequences begin with the requested history, objects'
sequences only differ on the last token (our argument token \inlinecode{arg}).
Our join condition thus reduces to
\inlinecode{arg = lastToken(Pattern.sequence)}.

\todo{Read \textit{``2013 BlanasPatel Memory Footprint Matters Efficient
Equi-Join Algorithms fro Main Memory Data Processing.pdf''} for possibly better
join algorithm}.

The technique for obtaining these both sorted and filtered relations is
explained in \cref{sec:sorted-and-filtered-access}.

\todo{Do we want discussion here rather than explanation?}

The algorithm used to produce top-$k$ results is presented in
\cref{sec:algorithm}.

% ------------------------------------------------------------------------------
\section{Remarks}

% ------------------------------------------------------------------------------
\section{Sorted and Filtered Access}
\label{sec:sorted-and-filtered-access}

As explained we require a data structure for efficiently retrieving all
sequences whose first to second to last words match a requested string $h$.
Additionally we want to specify that the last word has to contain $p$ as a
prefix, in order to support efficient Next-Keystroke-Savings $\NKSS$
computation.
\mbox{$\Set{\, w_1^n \: \Given \: w_1^{n-1} = h \; \land \; b \:\text{is prefix of}\: w_n \,}$}
which is equivalent to
\mbox{$\Set{\, w_1^n \: \Given \: \,}$}
\begin{equation*}
  \underbracket{\enskip w_1 \qquad w_2 \qquad \ldots \qquad w_{n-1} \enskip}
    _\text{matches $h$}
  \quad
  \underbracket{\enskip w_n \enskip}_\text{starts with $b$}
\end{equation*}

NKSS requires Top-\emph{k} Completion.

Use Completion-Trie \parencite{HsuOttaviano2013}.

\todo{Explain Completion-Trie here?}
\todo{Can we save all patterns in one trie?}

% ------------------------------------------------------------------------------
\section{Algorithm}
\label{sec:algorithm}

\subsection{Requirements}

\begin{enumerate}
  \item Dataset remains the same.
  \item Scoring function changes on each query.
  \item Join condition changes on each query!
\end{enumerate}

\subsection{Algorithm}

There are several techniques for indexing {Rank Join Indices \mbref{Tsaparas et
al 2003}, Onion Indices \mbref{Chang et al 2000}) which we don't use as history
changes to often.

Ranked Join Indicies\mbref{Tsaparas et al 2003})

Algorithms that precompute stuff may be general speed up, but not in our case
since we never expect the same thing to be queried again.

TA \parencite{Fagin2001}.

Rank-Join \parencite{Ilyas2004}.

Evaluate following algorithms:

\begin{enumerate}
  \item Traditional Rank-Join that generates all possible join combinations
    after each input.
  \item Improved version that uses random access for early tuple completion.
\end{enumerate}

Use observation that if a pattern is seen, it's child patterns' will be seen as
well.

Clever: How to find T as max of $(p_1^max, p_2^last)$ and $(p_1^last, p_2^max)$.

Clever: Priority-Queue with constant contains and update.

% ==============================================================================
\chapter{Evaluation}

No comparision with existing auto completion engines, as the goal was to solve
language model argmax, not fast auto completion.

Comparision with viterbi algorithm (speech recognition)?
Most implementations of viterbi are beam search or time-synchonous beam search.

% ==============================================================================
\chapter{Conclusion}

% ==============================================================================
\printbibliography[heading=bibintoc]

\begin{appendices}
\crefalias{chapter}{appsec}

% ==============================================================================
\chapter{Expansion}
\label{app:expansion}

Examples for \cref{ch:weightedsum}.

\section{Modified-Kneser-Ney}

\newcommand{\ProbMKNcab}[1]
  {\frac{\DiscountedCount(\text{a b c}) + \gamma(\text{a b}) #1}{\Count(\text{a b \Skp})}}
\newcommand{\ProbMKNcb}[1]
  {\frac{\DiscountedCount*(\text{b c}) + \gamma(\text{b}) #1}{\ContCountIp(\text{\WSkp b \WSkp})}}
\newcommand{\ProbMKNc}
  {\frac{\ContCountIp(\text{\WSkp c})}{\ContCountIp(\text{\WSkp \WSkp})}}

Sample expansion for $\ProbMKN{\text{c}}{\text{a b}}$, assuming the sequence
``$\text{a b c}$'' was seen:
\begin{align}
  \ProbMKN {\text{c}}{\text{a b}} &= \ProbMKNcab{\ProbMKN*{\text{c}}{\text{b}}} \\
  \ProbMKN*{\text{c}}{\text{b}}   &= \ProbMKNcb{\ProbMKN*{\text{c}}} \\
  \ProbMKN*{\text{c}}             &= \ProbMKNc
\end{align}

Together:
\begin{equation}
  \ProbMKN {\text{c}}{\text{a b}} = \ProbMKNcab{\ProbMKNcb{\ProbMKNc}}
\end{equation}

By using the distributive property:
\begin{eqnarray}
  \ProbGLM {\text{c}}{\text{a b}} =
    &\phantom{+} \; \hspace{0.2em}   \DiscountedCount (\text{a b c}) & \hspace{-0.5em} \cdot\ \frac{1}{\Count(\text{a b \Skp})} \\
    &         +  \; \hspace{1em}     \DiscountedCount*(\text{b c})   & \hspace{-0.5em} \cdot\ \frac{\gamma(\text{a b})}{\Count(\text{a b \Skp}) \ContCountIp(\text{\WSkp b \WSkp})} \nonumber \\
    &         +  \; \ContCountIp(\text{\WSkp c})          & \hspace{-0.5em} \cdot\ \frac{\gamma(\text{a b}) \gamma(\text{b})}{\Count(\text{a b \Skp}) \ContCountIp(\text{\WSkp b \WSkp}) \ContCountIp(\text{\WSkp \WSkp})} \nonumber
\end{eqnarray}


\clearpage
\section{Generalized Language Model}

\newcommand{\ProbGLMcab}[2]
  {\frac{\DiscountedCount(\text{a b c}) + \frac{\gamma(\text{a b})}{2}\left(#1 + #2\right)}{\Count(\text{a b \Skp})}}
\newcommand{\ProbGLMcsb}[1]
  {\frac{\DiscountedCount*(\text{\Skp b c}) + \frac{\gamma(\text{\Skp b})}{1} #1}{\ContCountIp(\text{\WSkp \Skp b \WSkp})}}
\newcommand{\ProbGLMcas}[1]
  {\frac{\DiscountedCount*(\text{a \Skp c}) + \frac{\gamma(\text{a \Skp})}{1} #1}{\ContCountIp(\text{\WSkp a \Skp \WSkp})}}
\newcommand{\ProbGLMcss}
  {\frac{\ContCountIp(\text{\WSkp \Skp \Skp c})}{\ContCountIp(\text{\WSkp \Skp \Skp \WSkp})}}

Sample expansion for $\ProbGLM{\text{c}}{\text{a b}}$, assuming the sequence
``$\text{\text{a b c}}$'' was seen:
\begin{align}
  \ProbGLM {\text{c}}{\text{a b}}       &= \ProbGLMcab{\ProbGLM*{\text{c}}{\text{\Skp b}}}{\ProbGLM*{\text{c}}{\text{a \Skp}}} \\
  \ProbGLM*{\text{c}}{\text{\Skp b}}    &= \ProbGLMcsb{\ProbGLM*{\text{c}}{\text{\Skp \Skp}}} \\
  \ProbGLM*{\text{c}}{\text{a \Skp}}    &= \ProbGLMcas{\ProbGLM*{\text{c}}{\text{\Skp \Skp}}} \\
  \ProbGLM*{\text{c}}{\text{\Skp \Skp}} &= \ProbGLMcss
\end{align}

Together:
\begin{align}
  &\ProbGLM{\text{c}}{\text{a b}} =\\
  &\qquad \ProbGLMcab{\ProbGLMcsb{\ProbGLMcss}}
                     {\ProbGLMcas{\ProbGLMcss}} \nonumber
\end{align}

By using the distributive property:
\begin{eqnarray}
  \ProbGLM {\text{c}}{\text{a b}} =
    &\phantom{+} \; \hspace{2.0em} \DiscountedCount (\text{a b c})    & \hspace{-0.5em} \cdot\  \frac{1}{\Count(\text{a b \Skp})} \\
    &         +  \; \hspace{2.0em} \DiscountedCount*(\text{\Skp b c}) & \hspace{-0.5em} \cdot\  \frac{\gamma(\text{a b})}{2 \Count(\text{a b \Skp}) \ContCountIp(\text{\WSkp \Skp b \WSkp})} \nonumber \\
    &         +  \; \hspace{2.0em} \DiscountedCount*(\text{a \Skp c}) & \hspace{-0.5em} \cdot\  \frac{\gamma(\text{a b})}{2 \Count(\text{a b \Skp}) \ContCountIp(\text{\WSkp a \Skp \WSkp})} \nonumber \\
    &         +  \; \ContCountIp(\text{\WSkp\Skp \Skp c})      & \hspace{-0.5em} \cdot\  \frac{\gamma(\text{a b})}{2 \cdot 1 \Count(\text{a b \Skp})} \Big(\frac{\gamma(\text{\Skp b})}{\ContCountIp(\text{\WSkp \Skp b \WSkp}) \ContCountIp(\text{\WSkp \Skp \Skp \WSkp})} \: + \nonumber \\
    &                                                          & \hspace{6.8em}                                                                            \frac{\gamma(\text{a \Skp})}{\ContCountIp(\text{\WSkp a \Skp \WSkp}) \ContCountIp(\text{\WSkp \Skp \Skp \WSkp})} \ \Big) \nonumber
\end{eqnarray}

% ==============================================================================
\chapter{Example SQL-Request}
\label{app:sql-example}

\todo{With database examples, and fully expanded SQL code, probably MKN query.}

% ==============================================================================
\chapter{Notes}

\todo{Remove before releasing.}

% ------------------------------------------------------------------------------
\begin{landscape}
  \section{Binomial Diamond}
  \begin{figure}[H]
    \centering
    \begin{tikzpicture}[
      node distance = 8.2em,
      seq/.style = {draw, circle, align=center, text centered, text width=4.8em},
    ]
      \node [seq] (0000) {$w_1 \: w_2 \: w_3 \: w_4$};
    
      \node [seq] (0010) [below left  of=0000] {$w_1 \: w_2 \: \Skp \: w_4$};
      \node [seq] (0100) [below right of=0000] {$w_1 \: \Skp \: w_3 \: w_4$};
      \node [seq] (0001) [      left  of=0010] {$w_1 \: w_2 \: w_3 \: \Skp$};
      \node [seq] (1000) [      right of=0100] {$\Skp \: w_2 \: w_3 \: w_4$};
    
      \node [seq] (0101) [below       of=0001] {$w_1 \: \Skp \: w_2 \: \Skp$};
      \node [seq] (0110) [below       of=0010] {$w_1 \: \Skp \: \Skp \: w_4$};
      \node [seq] (1001) [below       of=0100] {$\Skp \: w_2 \: w_3 \: \Skp$};
      \node [seq] (1010) [below       of=1000] {$\Skp \: w_2 \: \Skp \: w_4$};
      \node [seq] (0011) [      left  of=0101] {$w_1 \: w_2 \: \Skp \: \Skp$};
      \node [seq] (1100) [      right of=1010] {$\Skp \: \Skp \: w_3 \: w_4$};
    
      \node [seq] (0111) [below       of=0101] {$w_1 \: \Skp \: \Skp \: \Skp$};
      \node [seq] (1011) [below       of=0110] {$\Skp \: w_2 \: \Skp \: \Skp$};
      \node [seq] (1101) [below       of=1001] {$\Skp \: \Skp \: w_3 \: \Skp$};
      \node [seq] (1110) [below       of=1010] {$\Skp \: \Skp \: \Skp \: w_4$};
    
      \node [seq] (1111) [below right of=1011] {$\Skp \: \Skp \: \Skp \: \Skp$};
    
      \path[->, >=latex, thick]
        (0000) edge (0001)
        (0000) edge (0010)
        (0000) edge (0100)
        (0000) edge (1000)
      
        (0001) edge (0011)
        (0001) edge (0101)
        (0001) edge (1001)
        (0010) edge (0011)
        (0010) edge (0110)
        (0010) edge (1010)
        (0100) edge (0101)
        (0100) edge (0110)
        (0100) edge (1100)
        (1000) edge (1001)
        (1000) edge (1010)
        (1000) edge (1100)
      
        (0011) edge (0111)
        (0011) edge (1011)
        (0101) edge (0111)
        (0101) edge (1101)
        (0110) edge (0111)
        (0110) edge (1110)
        (1001) edge (1011)
        (1001) edge (1101)
        (1010) edge (1011)
        (1010) edge (1110)
        (1100) edge (1101)
        (1100) edge (1110)
      
        (0111) edge (1111)
        (1011) edge (1111)
        (1101) edge (1111)
        (1110) edge (1111);
    \end{tikzpicture}
  \end{figure}
\end{landscape}

% ------------------------------------------------------------------------------
\section{SQL Query Expansion}

\lstset{
  language=SQL,
  emph={token,pattern,pattern1,patternN},
  emphstyle=\textit,
  deletekeywords={count} % Used as column name, don't want to escape it though.
}

\subsection{Modified Kneser Ney Language Model}

\begin{lstlisting}
  SELECT
    token.token AS arg,
    func( 1.count,  11.count, 111.count,
         x1.count, x11.count)
      AS score
  FROM  token
  LEFT OUTER JOIN    1
    ON history(  1) + token =   1.sequence
  LEFT OUTER JOIN   11
    ON history( 11) + token =  11.sequence
  LEFT OUTER JOIN  111
    ON history(111) + token = 111.sequence
  LEFT OUTER JOIN   x1
    ON history( x1) + token =  x1.sequence
  LEFT OUTER JOIN  x11
    ON history(x11) + token = x11.sequence
  ORDER BY score DESC;
  LIMIT k;
\end{lstlisting}

\clearpage
\subsection{Generalized Language Model}

\begin{lstlisting}
  SELECT
    token.token AS arg,
    func( 001.count,  011.count,  101.count, 111.count,
         x001.count, x011.count, x101.count)
      AS score
  SELECT token, score
  FROM token
  LEFT OUTER JOIN  001
    ON history( 001) + token =  001.sequence
  LEFT OUTER JOIN  011
    ON history( 011) + token =  011.sequence
  LEFT OUTER JOIN  101
    ON history( 101) + token =  101.sequence
  LEFT OUTER JOIN  111
    ON history( 111) + token =  111.sequence
  LEFT OUTER JOIN x001
    ON history(x001) + token = x001.sequence
  LEFT OUTER JOIN x011
    ON history(x011) + token = x011.sequence
  LEFT OUTER JOIN x101
    ON history(x101) + token = x101.sequence
  ORDER BY score DESC;
  LIMIT k;
\end{lstlisting}

\end{appendices}

\end{document}
